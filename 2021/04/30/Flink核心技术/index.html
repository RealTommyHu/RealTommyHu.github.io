<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Flink核心概念概述一、Flink简介Flink是分布式流处理框架，对有界和无界的数据流进行高效的处理。核心是流处理，也支持批处理，将批处理看成是流处理的一种特殊情况，即：数据流是有明确界限的。而Spark Streaming核心是批处理，将流处理看成是批处理的一种特殊情况，即：把数据流进行极小粒度的拆分，拆分成多个微批处理。">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink核心技术">
<meta property="og:url" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/index.html">
<meta property="og:site_name" content="TommyHu的技术小馆">
<meta property="og:description" content="Flink核心概念概述一、Flink简介Flink是分布式流处理框架，对有界和无界的数据流进行高效的处理。核心是流处理，也支持批处理，将批处理看成是流处理的一种特殊情况，即：数据流是有明确界限的。而Spark Streaming核心是批处理，将流处理看成是批处理的一种特殊情况，即：把数据流进行极小粒度的拆分，拆分成多个微批处理。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-bounded-unbounded.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/streaming-flow.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-stack.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-api-stack.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-application-submission.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-task-subtask.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-tasks-slots.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-subtask-slots.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-task-parallelism.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-process.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-RichParallelSourceFunction.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-kafka-datasource-producer.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-kafka-datasource-console.png">
<meta property="og:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-Rescaling.png">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/flink-kafka-producer-consumer.png">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/flink-richsink.png">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/flink-mysql-sink.png">
<meta property="article:published_time" content="2021-04-30T06:53:02.000Z">
<meta property="article:modified_time" content="2021-05-06T03:06:16.465Z">
<meta property="article:author" content="Tommy Hu">
<meta property="article:tag" content="Flink">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-bounded-unbounded.png">

<link rel="canonical" href="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Flink核心技术 | TommyHu的技术小馆</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TommyHu的技术小馆</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">微信公众号：TommyHu的技术小馆</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tommy Hu">
      <meta itemprop="description" content="写点东西">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TommyHu的技术小馆">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Flink核心技术
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-04-30 14:53:02" itemprop="dateCreated datePublished" datetime="2021-04-30T14:53:02+08:00">2021-04-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-06 11:06:16" itemprop="dateModified" datetime="2021-05-06T11:06:16+08:00">2021-05-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Flink核心概念概述"><a href="#Flink核心概念概述" class="headerlink" title="Flink核心概念概述"></a>Flink核心概念概述</h1><h2 id="一、Flink简介"><a href="#一、Flink简介" class="headerlink" title="一、Flink简介"></a>一、Flink简介</h2><p>Flink是分布式流处理框架，对有界和无界的数据流进行高效的处理。核心是流处理，也支持批处理，将批处理看成是流处理的一种特殊情况，即：数据流是有明确界限的。而Spark Streaming核心是批处理，将流处理看成是批处理的一种特殊情况，即：把数据流进行极小粒度的拆分，拆分成多个微批处理。</p>
<span id="more"></span>

<p>Flink有界数据流和无界数据流：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-bounded-unbounded.png" alt="flink-bounded-unbounded.png"></p>
<p>Spark Streaming数据流的拆分：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/streaming-flow.png" alt="streaming-flow.png"></p>
<h2 id="二、Flink核心架构"><a href="#二、Flink核心架构" class="headerlink" title="二、Flink核心架构"></a>二、Flink核心架构</h2><p>分层架构，保证各层在功能上和职责上的清晰：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-stack.png" alt="flink-stack.png"></p>
<ul>
<li>API &amp; Libraries 层<ul>
<li>编程API：用于流处理的DataStream API。用于批处理的DataSet API。</li>
<li>顶层类库：复杂事件处理的CEP库。结构化数据查询的SQL &amp; Table库。基于批处理的机器学习库FlinkML和图处理库Gelly。</li>
</ul>
</li>
<li>Runtime 核心层<ul>
<li>Flink分布式计算框架的核心实现，包括：作业转换、任务调度、资源分配、任务执行等功能。可以在流式引擎下同时运行流处理程序和批处理程序。</li>
</ul>
</li>
<li>物理部署层<ul>
<li>支持在不同平台上部署运行Flink应用。</li>
</ul>
</li>
</ul>
<h2 id="三、Flink分层API"><a href="#三、Flink分层API" class="headerlink" title="三、Flink分层API"></a>三、Flink分层API</h2><p>API &amp; Libraries这一层进行了更为具体的划分：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-api-stack.png" alt="flink-api-stack.png"></p>
<ul>
<li>SQL &amp; Table API：同时适用于批处理和流处理，可以对有界数据流和无界数据流以相同的语义进行查询，产生相同的结果。支持基本查询、自定义的标量函数、聚合函数、表值函数，满足多样化的查询需求。</li>
<li>DataStream &amp; DataSet API：Flink数据处理的核心API，支持Java和Scala调用，提供数据常用操作的封装：读取、转换、输出。</li>
<li>Stateful Stream Processing：最低级别的抽象，通过Process Function函数内嵌到DataStream API中，Process Function是Flink提供的最底层的API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。</li>
</ul>
<h2 id="四、Flink集群架构"><a href="#四、Flink集群架构" class="headerlink" title="四、Flink集群架构"></a>四、Flink集群架构</h2><h3 id="4-1-核心组件"><a href="#4-1-核心组件" class="headerlink" title="4.1 核心组件"></a>4.1 核心组件</h3><p>第二层，Runtime层。采用Master-Slave结构，Master部分包含三个核心组件：Dispatcher、ResourceManager和JobManager，Slave主要是TaskManager进程：</p>
<ul>
<li>Dispatcher：接收客户端提交的执行程序，并传递给JobManager，还提供了一个Web UI界面用于监控作业的执行情况。</li>
<li>JobManager（也称为masters）：每个作业至少有一个JobManager。接收由Dispatcher传递过来的执行程序，包含了作业图（JobGraph）、逻辑数据流图（logical dataflow graph）、及其所有的class文件、第三方类库等。接着将JobGraph转换为执行图（ExecutionGraph）。然后向ResourceManager申请资源来执行该任务，申请到资源就将执行图分发给对应的TaskManager。（高可用部署下可以有多个JobManagers，其中一个作为leader，其余的处于standby状态）。</li>
<li>ResourceManager：管理slots并协调集群资源。接收来自JobManager的资源请求，将存在空闲slots的TaskManager分配给JobManager执行任务。（Flink基于不同的部署平台，如YARN、Mesos、K8s等提供了不同的资源管理器，当TaskManager没有足够的slots来执行任务时，它会向第三方平台发起会话来请求额外的资源）。</li>
<li>TaskManagers（也称为workers）：负责实际的子任务（subtasks）的执行，每个TaskManager拥有一定数量的slots，Slot是一组固定大小的资源的合集（如计算能力、存储空间）。启动后会将其所拥有的slots注册到ResourceManager上，由ResourceManager进行统一的管理。</li>
</ul>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-application-submission.png" alt="flink-application-submission.png"></p>
<h3 id="4-2-Task-amp-SubTask"><a href="#4-2-Task-amp-SubTask" class="headerlink" title="4.2  Task &amp; SubTask"></a>4.2  Task &amp; SubTask</h3><p>上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别：</p>
<p>在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。  简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-task-subtask.png" alt="flink-task-subtask.png"></p>
<p>解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： <em>A subtask is one parallel slice of a task</em>，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。</p>
<h3 id="4-3-资源管理"><a href="#4-3-资源管理" class="headerlink" title="4.3  资源管理"></a>4.3  资源管理</h3><p>理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-tasks-slots.png" alt="flink-tasks-slots.png"></p>
<p>这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。</p>
<p>基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-subtask-slots.png" alt="flink-subtask-slots.png"></p>
<p>可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-task-parallelism.png" alt="flink-task-parallelism.png"></p>
<h3 id="4-4-组件通讯"><a href="#4-4-组件通讯" class="headerlink" title="4.4 组件通讯"></a>4.4 组件通讯</h3><p>Flink 的所有组件都基于 Actor System 来进行通讯。Actor system是多种角色的 actor 的容器，它提供调度，配置，日志记录等多种服务，并包含一个可以启动所有 actor 的线程池，如果 actor 是本地的，则消息通过共享内存进行共享，但如果 actor 是远程的，则通过 RPC 的调用来传递消息。</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-process.png" alt="flink-process.png"></p>
<h2 id="五、Flink-的优点"><a href="#五、Flink-的优点" class="headerlink" title="五、Flink 的优点"></a>五、Flink 的优点</h2><p>最后基于上面的介绍，来总结一下 Flink 的优点：</p>
<ul>
<li>Flink 是基于事件驱动 (Event-driven) 的应用，能够同时支持流处理和批处理；</li>
<li>基于内存的计算，能够保证高吞吐和低延迟，具有优越的性能表现；</li>
<li>支持精确一次 (Exactly-once) 语意，能够完美地保证一致性和正确性；</li>
<li>分层 API ，能够满足各个层次的开发需求；</li>
<li>支持高可用配置，支持保存点机制，能够提供安全性和稳定性上的保证；</li>
<li>多样化的部署方式，支持本地，远端，云端等多种部署方案；</li>
<li>具有横向扩展架构，能够按照用户的需求进行动态扩容；</li>
<li>活跃度极高的社区和完善的生态圈的支持。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/programming-model.html">Dataflow Programming Model</a></li>
<li><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/runtime.html">Distributed Runtime Environment</a></li>
<li> <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/components.html">Component Stack</a></li>
<li>Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》.  O’Reilly Media .  2019-4-30 </li>
</ul>
<h1 id="Flink-Data-Source"><a href="#Flink-Data-Source" class="headerlink" title="Flink Data Source"></a>Flink Data Source</h1><h2 id="一、内置-Data-Source"><a href="#一、内置-Data-Source" class="headerlink" title="一、内置 Data Source"></a>一、内置 Data Source</h2><p>Flink Data Source 用于定义 Flink 程序的数据来源，Flink 官方提供了多种数据获取方法，用于帮助开发者简单快速地构建输入流，具体如下：</p>
<h3 id="1-1-基于文件构建"><a href="#1-1-基于文件构建" class="headerlink" title="1.1 基于文件构建"></a>1.1 基于文件构建</h3><p>**1. readTextFile(path)**：按照 TextInputFormat 格式读取文本文件，并将其内容以字符串的形式返回。示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.readTextFile(filePath).print();</span><br></pre></td></tr></table></figure>

<p><strong>2. readFile(fileInputFormat, path)</strong> ：按照指定格式读取文件。</p>
<p>**3. readFile(inputFormat, filePath, watchType, interval, typeInformation)**：按照指定格式周期性的读取文件。其中各个参数的含义如下：</p>
<ul>
<li><strong>inputFormat</strong>：数据流的输入格式。</li>
<li><strong>filePath</strong>：文件路径，可以是本地文件系统上的路径，也可以是 HDFS 上的文件路径。</li>
<li><strong>watchType</strong>：读取方式，它有两个可选值，分别是 <code>FileProcessingMode.PROCESS_ONCE</code> 和 <code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>：前者表示对指定路径上的数据只读取一次，然后退出；后者表示对路径进行定期地扫描和读取。需要注意的是如果 watchType 被设置为 <code>PROCESS_CONTINUOUSLY</code>，那么当文件被修改时，其所有的内容 (包含原有的内容和新增的内容) 都将被重新处理，因此这会打破 Flink 的 <em>exactly-once</em> 语义。</li>
<li><strong>interval</strong>：定期扫描的时间间隔。</li>
<li><strong>typeInformation</strong>：输入流中元素的类型。</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> flinktest;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.io.<span class="type">TextInputFormat</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.<span class="type">DataStreamSource</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">StreamExecutionEnvironment</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">ExampleDemo</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">        <span class="comment">//1、创建环境对象</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//2、读取文件作为数据源</span></span><br><span class="line">        <span class="type">DataStreamSource</span>&lt;<span class="type">String</span>&gt; fileSource = env.readFile(<span class="keyword">new</span> <span class="type">TextInputFormat</span>(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;/tmp/test.txt&quot;</span>)), <span class="string">&quot;/tmp/test.txt&quot;</span>);</span><br><span class="line">        <span class="comment">//3、打印数据</span></span><br><span class="line">        fileSource.print();</span><br><span class="line">        <span class="comment">//4、启动任务执行</span></span><br><span class="line">        env.execute(<span class="string">&quot;test file source&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String filePath = <span class="string">&quot;D:\\log4j.properties&quot;</span>;</span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.readFile(<span class="keyword">new</span> TextInputFormat(<span class="keyword">new</span> Path(filePath)),</span><br><span class="line">             filePath,</span><br><span class="line">             FileProcessingMode.PROCESS_ONCE,</span><br><span class="line">             <span class="number">1</span>,</span><br><span class="line">             BasicTypeInfo.STRING_TYPE_INFO).print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="1-2-基于集合构建"><a href="#1-2-基于集合构建" class="headerlink" title="1.2 基于集合构建"></a>1.2 基于集合构建</h3><p>**1. fromCollection(Collection)**：基于集合构建，集合中的所有元素必须是同一类型。示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromCollection(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).print();</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">senv.fromCollection(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).print()</span><br><span class="line">senv.execute()</span><br></pre></td></tr></table></figure>

<p>**2. fromElements(T …)**： 基于元素构建，所有元素必须是同一类型。示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).print();</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">senv.fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).print()</span><br><span class="line">senv.execute()</span><br></pre></td></tr></table></figure>

<p>**3. generateSequence(from, to)**：基于给定的序列区间进行构建。示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.generateSequence(<span class="number">0</span>,<span class="number">100</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">senv.generateSequence(<span class="number">0</span>,<span class="number">100</span>).print()</span><br><span class="line">senv.execute()</span><br></pre></td></tr></table></figure>

<p>**4. fromCollection(Iterator, Class)**：基于迭代器进行构建。第一个参数用于定义迭代器，第二个参数用于定义输出元素的类型。使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromCollection(<span class="keyword">new</span> CustomIterator(), BasicTypeInfo.INT_TYPE_INFO).print();</span><br></pre></td></tr></table></figure>

<p>其中 CustomIterator 为自定义的迭代器，这里以产生 1 到 100 区间内的数据为例，源码如下。需要注意的是自定义迭代器除了要实现 Iterator 接口外，还必须要实现序列化接口 Serializable ，否则会抛出序列化失败的异常：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomIterator</span> <span class="keyword">implements</span> <span class="title">Iterator</span>&lt;<span class="title">Integer</span>&gt;, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> i &lt; <span class="number">100</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        i++;</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>**5. fromParallelCollection(SplittableIterator, Class)**：方法接收两个参数，第二个参数用于定义输出元素的类型，第一个参数 SplittableIterator 是迭代器的抽象基类，它用于将原始迭代器的值拆分到多个不相交的迭代器中。</p>
<h3 id="1-3-基于-Socket-构建"><a href="#1-3-基于-Socket-构建" class="headerlink" title="1.3  基于 Socket 构建"></a>1.3  基于 Socket 构建</h3><p>Flink 提供了 socketTextStream 方法用于构建基于 Socket 的数据流，socketTextStream 方法有以下四个主要参数：</p>
<ul>
<li><strong>hostname</strong>：主机名；</li>
<li><strong>port</strong>：端口号，设置为 0 时，表示端口号自动分配；</li>
<li><strong>delimiter</strong>：用于分隔每条记录的分隔符；</li>
<li><strong>maxRetry</strong>：当 Socket 临时关闭时，程序的最大重试间隔，单位为秒。设置为 0 时表示不进行重试；设置为负值则表示一直重试。示例如下：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.socketTextStream(&quot;192.168.0.229&quot;, 9999, &quot;\n&quot;, 3).print();</span><br></pre></td></tr></table></figure>

<p>通过网络工具netcat传输文本信息，以win10为例，输入命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -L -p 9999 </span><br></pre></td></tr></table></figure>

<h2 id="二、自定义-Data-Source"><a href="#二、自定义-Data-Source" class="headerlink" title="二、自定义 Data Source"></a>二、自定义 Data Source</h2><h3 id="2-1-SourceFunction"><a href="#2-1-SourceFunction" class="headerlink" title="2.1 SourceFunction"></a>2.1 SourceFunction</h3><p>除了内置的数据源外，用户还可以使用 <code>addSource</code> 方法来添加自定义的数据源。自定义的数据源必须要实现 SourceFunction 接口，这里以产生 [0 , 1000) 区间内的数据为例，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.addSource(<span class="keyword">new</span> SourceFunction&lt;Long&gt;() &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> count = <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning &amp;&amp; count &lt; <span class="number">1000</span>) &#123;</span><br><span class="line">            <span class="comment">// 通过collect将输入发送出去 </span></span><br><span class="line">            ctx.collect(count);</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;).print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="2-2-ParallelSourceFunction-和-RichParallelSourceFunction"><a href="#2-2-ParallelSourceFunction-和-RichParallelSourceFunction" class="headerlink" title="2.2 ParallelSourceFunction 和 RichParallelSourceFunction"></a>2.2 ParallelSourceFunction 和 RichParallelSourceFunction</h3><p>上面通过 SourceFunction 实现的数据源是不具有并行度的，即不支持在得到的 DataStream 上调用 <code>setParallelism(n)</code> 方法，此时会抛出如下的异常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Source: 1 is not a parallel source</span><br></pre></td></tr></table></figure>

<p>如果你想要实现具有并行度的输入流，则需要实现 ParallelSourceFunction 或 RichParallelSourceFunction 接口，其与 SourceFunction 的关系如下图： </p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-RichParallelSourceFunction.png" alt="flink-RichParallelSourceFunction.png"></p>
<p>ParallelSourceFunction 直接继承自 SourceFunction，具有并行度的功能。RichParallelSourceFunction 则继承自 AbstractRichFunction，同时实现了 ParallelSourceFunction 接口，所以其除了具有并行度的功能外，还提供了额外的与生命周期相关的方法，如 open() ，closen() 。</p>
<h2 id="三、Streaming-Connectors"><a href="#三、Streaming-Connectors" class="headerlink" title="三、Streaming Connectors"></a>三、Streaming Connectors</h2><h3 id="3-1-内置连接器"><a href="#3-1-内置连接器" class="headerlink" title="3.1 内置连接器"></a>3.1 内置连接器</h3><p>除了自定义数据源外， Flink 还内置了多种连接器，用于满足大多数的数据收集场景。当前内置连接器的支持情况如下：</p>
<ul>
<li>Apache Kafka (支持 source 和 sink)</li>
<li>Apache Cassandra (sink)</li>
<li>Amazon Kinesis Streams (source/sink)</li>
<li>Elasticsearch (sink)</li>
<li>Hadoop FileSystem (sink)</li>
<li>RabbitMQ (source/sink)</li>
<li>Apache NiFi (source/sink)</li>
<li>Twitter Streaming API (source)</li>
<li>Google PubSub (source/sink)</li>
</ul>
<p>除了上述的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink 相关的连接器如下：</p>
<ul>
<li>Apache ActiveMQ (source/sink)</li>
<li>Apache Flume (sink)</li>
<li>Redis (sink)</li>
<li>Akka (sink)</li>
<li>Netty (source)</li>
</ul>
<p>随着 Flink 的不断发展，可以预见到其会支持越来越多类型的连接器，关于连接器的后续发展情况，可以查看其官方文档：<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html">Streaming Connectors</a> 。在所有 DataSource 连接器中，使用的广泛的就是 Kafka，所以这里我们以其为例，来介绍 Connectors 的整合步骤。</p>
<h3 id="3-2-整合-Kakfa"><a href="#3-2-整合-Kakfa" class="headerlink" title="3.2 整合 Kakfa"></a>3.2 整合 Kakfa</h3><h4 id="1-导入依赖"><a href="#1-导入依赖" class="headerlink" title="1. 导入依赖"></a>1. 导入依赖</h4><p>整合 Kafka 时，一定要注意所使用的 Kafka 的版本，不同版本间所需的 Maven 依赖和开发时所调用的类均不相同，具体如下：</p>
<table>
<thead>
<tr>
<th align="left">Maven 依赖</th>
<th align="left">Flink 版本</th>
<th align="left">Consumer and Producer 类的名称</th>
<th align="left">Kafka 版本</th>
</tr>
</thead>
<tbody><tr>
<td align="left">flink-connector-kafka-0.8_2.11</td>
<td align="left">1.0.0 +</td>
<td align="left">FlinkKafkaConsumer08 <br/>FlinkKafkaProducer08</td>
<td align="left">0.8.x</td>
</tr>
<tr>
<td align="left">flink-connector-kafka-0.9_2.11</td>
<td align="left">1.0.0 +</td>
<td align="left">FlinkKafkaConsumer09<br/> FlinkKafkaProducer09</td>
<td align="left">0.9.x</td>
</tr>
<tr>
<td align="left">flink-connector-kafka-0.10_2.11</td>
<td align="left">1.2.0 +</td>
<td align="left">FlinkKafkaConsumer010 <br/>FlinkKafkaProducer010</td>
<td align="left">0.10.x</td>
</tr>
<tr>
<td align="left">flink-connector-kafka-0.11_2.11</td>
<td align="left">1.4.0 +</td>
<td align="left">FlinkKafkaConsumer011 <br/>FlinkKafkaProducer011</td>
<td align="left">0.11.x</td>
</tr>
<tr>
<td align="left">flink-connector-kafka_2.11</td>
<td align="left">1.7.0 +</td>
<td align="left">FlinkKafkaConsumer <br/>FlinkKafkaProducer</td>
<td align="left">&gt;= 1.0.0</td>
</tr>
</tbody></table>
<p>这里我使用的 Kafka 版本为 kafka_2.12-2.2.0，添加的依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-代码开发"><a href="#2-代码开发" class="headerlink" title="2. 代码开发"></a>2. 代码开发</h4><p>这里以最简单的场景为例，接收 Kafka 上的数据并打印，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// 指定Kafka的连接位置</span></span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line"><span class="comment">// 指定监听的主题，并定义Kafka字节消息到Flink对象之间的转换规则</span></span><br><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(<span class="string">&quot;flink-stream-in-topic&quot;</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br><span class="line">stream.print();</span><br><span class="line">env.execute(<span class="string">&quot;Flink Streaming&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="3-3-整合测试"><a href="#3-3-整合测试" class="headerlink" title="3.3 整合测试"></a>3.3 整合测试</h3><h4 id="1-启动-Kakfa"><a href="#1-启动-Kakfa" class="headerlink" title="1. 启动 Kakfa"></a>1. 启动 Kakfa</h4><p>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> zookeeper启动命令</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 内置zookeeper启动命令</span></span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>

<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure>

<p>win10：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动单节点 kafka 用于测试：</span></span><br><span class="line">./bin/windows/kafka-server-start.bat ./config/server.properties</span><br></pre></td></tr></table></figure>

<h4 id="2-创建-Topic"><a href="#2-创建-Topic" class="headerlink" title="2. 创建 Topic"></a>2. 创建 Topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用于测试主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic flink-stream-in-topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>

<p>win10:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="comment"># 创建用于测试主题</span></span></span><br><span class="line">./kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure>

<h4 id="3-启动-Producer"><a href="#3-启动-Producer" class="headerlink" title="3. 启动 Producer"></a>3. 启动 Producer</h4><p>这里 启动一个 Kafka 生产者，用于发送测试数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic flink-stream-in-topic</span><br></pre></td></tr></table></figure>

<p>win10:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动一个 Kafka 生产者，用于发送测试数据：</span></span><br><span class="line">./kafka-console-producer.bat --broker-list localhost:9092 --topic test</span><br></pre></td></tr></table></figure>

<h4 id="4-测试结果"><a href="#4-测试结果" class="headerlink" title="4. 测试结果"></a>4. 测试结果</h4><p>在 Producer 上输入任意测试数据，之后观察程序控制台的输出：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-kafka-datasource-producer.png" alt="flink-kafka-datasource-producer.png"><br>程序控制台的输出如下：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-kafka-datasource-console.png" alt="flink-kafka-datasource-console.png"><br>可以看到已经成功接收并打印出相关的数据。</p>
<h2 id="参考资料-1"><a href="#参考资料-1" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>data-sources：<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources</a> </li>
<li>Streaming Connectors：<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html</a></li>
<li>Apache Kafka Connector： <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html</a> </li>
</ol>
<h1 id="Flink-Transformation"><a href="#Flink-Transformation" class="headerlink" title="Flink Transformation"></a>Flink Transformation</h1><h2 id="一、Transformations-分类"><a href="#一、Transformations-分类" class="headerlink" title="一、Transformations 分类"></a>一、Transformations 分类</h2><p>Flink 的 Transformations 操作主要用于将一个和多个 DataStream 按需转换成新的 DataStream。它主要分为以下三类：</p>
<ul>
<li><strong>DataStream Transformations</strong>：进行数据流相关转换操作；</li>
<li><strong>Physical partitioning</strong>：物理分区。Flink 提供的底层 API ，允许用户定义数据的分区规则；</li>
<li><strong>Task chaining and resource groups</strong>：任务链和资源组。允许用户进行任务链和资源组的细粒度的控制。</li>
</ul>
<p>以下分别对其主要 API 进行介绍：</p>
<h2 id="二、DataStream-Transformations"><a href="#二、DataStream-Transformations" class="headerlink" title="二、DataStream Transformations"></a>二、DataStream Transformations</h2><h3 id="2-1-Map-DataStream-→-DataStream"><a href="#2-1-Map-DataStream-→-DataStream" class="headerlink" title="2.1 Map [DataStream → DataStream]"></a>2.1 Map [DataStream → DataStream]</h3><p>对一个 DataStream 中的每个元素都执行特定的转换操作：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; integerDataStream = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">integerDataStream.map((MapFunction&lt;Integer, Object&gt;) value -&gt; value * <span class="number">2</span>).print();</span><br><span class="line"><span class="comment">// 输出 2,4,6,8,10</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-FlatMap-DataStream-→-DataStream"><a href="#2-2-FlatMap-DataStream-→-DataStream" class="headerlink" title="2.2 FlatMap [DataStream → DataStream]"></a>2.2 FlatMap [DataStream → DataStream]</h3><p>FlatMap 与 Map 类似，但是 FlatMap 中的一个输入元素可以被映射成一个或者多个输出元素，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">String string01 = <span class="string">&quot;one one one two two&quot;</span>;</span><br><span class="line">String string02 = <span class="string">&quot;third third third four&quot;</span>;</span><br><span class="line">DataStream&lt;String&gt; stringDataStream = env.fromElements(string01, string02);</span><br><span class="line">stringDataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : value.split(<span class="string">&quot; &quot;</span>)) &#123;</span><br><span class="line">            out.collect(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"><span class="comment">// 输出每一个独立的单词，为节省排版，这里去掉换行，后文亦同</span></span><br><span class="line">one one one two two third third third four</span><br></pre></td></tr></table></figure>

<h3 id="2-3-Filter-DataStream-→-DataStream"><a href="#2-3-Filter-DataStream-→-DataStream" class="headerlink" title="2.3 Filter [DataStream → DataStream]"></a>2.3 Filter [DataStream → DataStream]</h3><p>用于过滤符合条件的数据：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).filter(x -&gt; x &gt; <span class="number">3</span>).print();</span><br></pre></td></tr></table></figure>

<h3 id="2-4-KeyBy-和-Reduce"><a href="#2-4-KeyBy-和-Reduce" class="headerlink" title="2.4 KeyBy 和 Reduce"></a>2.4 KeyBy 和 Reduce</h3><ul>
<li><strong>KeyBy [DataStream → KeyedStream]</strong> ：用于将相同 Key 值的数据分到相同的分区中；</li>
<li><strong>Reduce [KeyedStream → DataStream]</strong> ：用于对数据执行归约计算。</li>
</ul>
<p>如下例子将数据按照 key 值分区后，滚动进行求和计算：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2DataStream = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                                                                        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), </span><br><span class="line">                                                                        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), </span><br><span class="line">                                                                        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">5</span>));</span><br><span class="line">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = tuple2DataStream.keyBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.reduce((ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;) (value1, value2) -&gt;</span><br><span class="line">                   <span class="keyword">new</span> Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1)).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 持续进行求和计算，输出：</span></span><br><span class="line">(a,<span class="number">1</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br><span class="line">(b,<span class="number">3</span>)</span><br><span class="line">(b,<span class="number">8</span>)</span><br></pre></td></tr></table></figure>

<p>KeyBy 操作存在以下两个限制：</p>
<ul>
<li>KeyBy 操作用于用户自定义的 POJOs 类型时，该自定义类型必须重写 hashCode 方法；</li>
<li>KeyBy 操作不能用于数组类型。</li>
</ul>
<h3 id="2-5-Aggregations-KeyedStream-→-DataStream"><a href="#2-5-Aggregations-KeyedStream-→-DataStream" class="headerlink" title="2.5 Aggregations [KeyedStream → DataStream]"></a>2.5 Aggregations [KeyedStream → DataStream]</h3><p>Aggregations 是官方提供的聚合算子，封装了常用的聚合操作，如上利用 Reduce 进行求和的操作也可以利用 Aggregations 中的 sum 算子重写为下面的形式：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tuple2DataStream.keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>

<p>除了 sum 外，Flink 还提供了 min , max , minBy，maxBy 等常用聚合算子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 滚动计算指定key的最小值，可以通过index或者fieldName来指定key</span></span><br><span class="line">keyedStream.min(<span class="number">0</span>);</span><br><span class="line">keyedStream.min(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最大值</span></span><br><span class="line">keyedStream.max(<span class="number">0</span>);</span><br><span class="line">keyedStream.max(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最小值，并返回其对应的元素</span></span><br><span class="line">keyedStream.minBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.minBy(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最大值，并返回其对应的元素</span></span><br><span class="line">keyedStream.maxBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.maxBy(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="2-6-Union-DataStream-→-DataStream"><a href="#2-6-Union-DataStream-→-DataStream" class="headerlink" title="2.6 Union [DataStream* → DataStream]"></a>2.6 Union [DataStream* → DataStream]</h3><p>用于连接两个或者多个元素类型相同的 DataStream 。当然一个 DataStream 也可以与其本生进行连接，此时该 DataStream 中的每个元素都会被获取两次：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;(&quot;a&quot;, 1), </span><br><span class="line">                                                                            new Tuple2&lt;&gt;(&quot;a&quot;, 2));</span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource02 = env.fromElements(new Tuple2&lt;&gt;(&quot;b&quot;, 1), </span><br><span class="line">                                                                            new Tuple2&lt;&gt;(&quot;b&quot;, 2));</span><br><span class="line">streamSource01.union(streamSource02);</span><br><span class="line">streamSource01.union(streamSource01,streamSource02);</span><br></pre></td></tr></table></figure>

<h3 id="2-7-Connect-DataStream-DataStream-→-ConnectedStreams"><a href="#2-7-Connect-DataStream-DataStream-→-ConnectedStreams" class="headerlink" title="2.7 Connect [DataStream,DataStream → ConnectedStreams]"></a>2.7 Connect [DataStream,DataStream → ConnectedStreams]</h3><p>Connect 操作用于连接两个或者多个类型不同的 DataStream ，其返回的类型是 ConnectedStreams ，此时被连接的多个 DataStreams 可以共享彼此之间的数据状态。但是需要注意的是由于不同 DataStream 之间的数据类型是不同的，如果想要进行后续的计算操作，还需要通过 CoMap 或 CoFlatMap 将 ConnectedStreams  转换回 DataStream：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), </span><br><span class="line">                                                                            <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">5</span>));</span><br><span class="line">DataStreamSource&lt;Integer&gt; streamSource02 = env.fromElements(<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>);</span><br><span class="line"><span class="comment">// 使用connect进行连接</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; connect = streamSource01.connect(streamSource02);</span><br><span class="line">connect.map(<span class="keyword">new</span> CoMapFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">map1</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.f1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">map2</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).map(x -&gt; x * <span class="number">100</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line"><span class="number">300</span> <span class="number">500</span> <span class="number">200</span> <span class="number">900</span> <span class="number">300</span></span><br></pre></td></tr></table></figure>

<h3 id="2-8-Split-和-Select"><a href="#2-8-Split-和-Select" class="headerlink" title="2.8 Split 和 Select"></a>2.8 Split 和 Select</h3><ul>
<li>**Split [DataStream → SplitStream]**：用于将一个 DataStream 按照指定规则进行拆分为多个 DataStream，需要注意的是这里进行的是逻辑拆分，即 Split 只是将数据贴上不同的类型标签，但最终返回的仍然只是一个 SplitStream；</li>
<li>**Select [SplitStream → DataStream]**：想要从逻辑拆分的 SplitStream 中获取真实的不同类型的 DataStream，需要使用 Select 算子，示例如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Integer&gt; streamSource = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>);</span><br><span class="line"><span class="comment">// 标记</span></span><br><span class="line">SplitStream&lt;Integer&gt; split = streamSource.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        output.add(value % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">&quot;even&quot;</span> : <span class="string">&quot;odd&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 获取偶数数据集</span></span><br><span class="line">split.select(<span class="string">&quot;even&quot;</span>).print();</span><br><span class="line"><span class="comment">// 输出 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="2-9-project-DataStream-→-DataStream"><a href="#2-9-project-DataStream-→-DataStream" class="headerlink" title="2.9 project [DataStream → DataStream]"></a>2.9 project [DataStream → DataStream]</h3><p>project 主要用于获取 tuples 中的指定字段集，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple3&lt;String, Integer, String&gt;&gt; streamSource = env.fromElements(</span><br><span class="line">                                                                         <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="string">&quot;li&quot;</span>, <span class="number">22</span>, <span class="string">&quot;2018-09-23&quot;</span>),</span><br><span class="line">                                                                         <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="string">&quot;ming&quot;</span>, <span class="number">33</span>, <span class="string">&quot;2020-09-23&quot;</span>));</span><br><span class="line">streamSource.project(<span class="number">0</span>,<span class="number">2</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(li,<span class="number">2018</span>-09-<span class="number">23</span>)</span><br><span class="line">(ming,<span class="number">2020</span>-09-<span class="number">23</span>)</span><br></pre></td></tr></table></figure>

<h2 id="三、物理分区"><a href="#三、物理分区" class="headerlink" title="三、物理分区"></a>三、物理分区</h2><p>物理分区 (Physical partitioning) 是 Flink 提供的底层的 API，允许用户采用内置的分区规则或者自定义的分区规则来对数据进行分区，从而避免数据在某些分区上过于倾斜，常用的分区规则如下：</p>
<h3 id="3-1-Random-partitioning-DataStream-→-DataStream"><a href="#3-1-Random-partitioning-DataStream-→-DataStream" class="headerlink" title="3.1 Random partitioning [DataStream → DataStream]"></a>3.1 Random partitioning [DataStream → DataStream]</h3><p>随机分区 (Random partitioning) 用于随机的将数据分布到所有下游分区中，通过 shuffle 方法来进行实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.shuffle();</span><br></pre></td></tr></table></figure>

<h3 id="3-2-Rebalancing-DataStream-→-DataStream"><a href="#3-2-Rebalancing-DataStream-→-DataStream" class="headerlink" title="3.2 Rebalancing [DataStream → DataStream]"></a>3.2 Rebalancing [DataStream → DataStream]</h3><p>Rebalancing 采用轮询的方式将数据进行分区，其适合于存在数据倾斜的场景下，通过 rebalance 方法进行实现：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.rebalance();</span><br></pre></td></tr></table></figure>

<h3 id="3-3-Rescaling-DataStream-→-DataStream"><a href="#3-3-Rescaling-DataStream-→-DataStream" class="headerlink" title="3.3 Rescaling [DataStream → DataStream]"></a>3.3 Rescaling [DataStream → DataStream]</h3><p>当采用 Rebalancing 进行分区平衡时，其实现的是全局性的负载均衡，数据会通过网络传输到其他节点上并完成分区数据的均衡。 而 Rescaling 则是低配版本的 rebalance，它不需要额外的网络开销，它只会对上下游的算子之间进行重新均衡，通过 rescale 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.rescale();</span><br></pre></td></tr></table></figure>

<p>ReScale 这个单词具有重新缩放的意义，其对应的操作也是如此，具体如下：如果上游 operation 并行度为 2，而下游的 operation 并行度为 6，则其中 1 个上游的 operation 会将元素分发到 3 个下游 operation，另 1 个上游 operation 则会将元素分发到另外 3 个下游 operation。反之亦然，如果上游的 operation 并行度为 6，而下游 operation 并行度为 2，则其中 3 个上游 operation 会将元素分发到 1 个下游 operation，另 3 个上游 operation 会将元素分发到另外 1 个下游operation：</p>
<p><img src="/2021/04/30/Flink%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/flink-Rescaling.png" alt="flink-Rescaling.png"></p>
<h3 id="3-4-Broadcasting-DataStream-→-DataStream"><a href="#3-4-Broadcasting-DataStream-→-DataStream" class="headerlink" title="3.4 Broadcasting [DataStream → DataStream]"></a>3.4 Broadcasting [DataStream → DataStream]</h3><p>将数据分发到所有分区上。通常用于小数据集与大数据集进行关联的情况下，此时可以将小数据集广播到所有分区上，避免频繁的跨分区关联，通过 broadcast 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.broadcast();</span><br></pre></td></tr></table></figure>

<h3 id="3-5-Custom-partitioning-DataStream-→-DataStream"><a href="#3-5-Custom-partitioning-DataStream-→-DataStream" class="headerlink" title="3.5 Custom partitioning [DataStream → DataStream]"></a>3.5 Custom partitioning [DataStream → DataStream]</h3><p>Flink 运行用户采用自定义的分区规则来实现分区，此时需要通过实现 Partitioner 接口来自定义分区规则，并指定对应的分区键，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;Hadoop&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;Spark&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;Flink-streaming&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;Flink-batch&quot;</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;Storm&quot;</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">&quot;HBase&quot;</span>, <span class="number">3</span>));</span><br><span class="line">streamSource.partitionCustom(<span class="keyword">new</span> Partitioner&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将第一个字段包含flink的Tuple2分配到同一个分区</span></span><br><span class="line">        <span class="keyword">return</span> key.toLowerCase().contains(<span class="string">&quot;flink&quot;</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;, <span class="number">0</span>).print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出如下：</span></span><br><span class="line"><span class="number">1</span>&gt; (Flink-streaming,<span class="number">2</span>)</span><br><span class="line"><span class="number">1</span>&gt; (Flink-batch,<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Hadoop,<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Spark,<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Storm,<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span>&gt; (HBase,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>



<h2 id="四、任务链和资源组"><a href="#四、任务链和资源组" class="headerlink" title="四、任务链和资源组"></a>四、任务链和资源组</h2><p>任务链和资源组 ( Task chaining and resource groups ) 也是 Flink 提供的底层 API，用于控制任务链和资源分配。默认情况下，如果操作允许 (例如相邻的两次 map 操作) ，则 Flink 会尝试将它们在同一个线程内进行，从而可以获取更好的性能。但是 Flink 也允许用户自己来控制这些行为，这就是任务链和资源组 API：</p>
<h3 id="4-1-startNewChain"><a href="#4-1-startNewChain" class="headerlink" title="4.1 startNewChain"></a>4.1 startNewChain</h3><p>startNewChain 用于基于当前 operation 开启一个新的任务链。如下所示，基于第一个 map 开启一个新的任务链，此时前一个 map 和 后一个 map 将处于同一个新的任务链中，但它们与 filter 操作则分别处于不同的任务链中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">someStream.filter(...).map(...).startNewChain().map(...);</span><br></pre></td></tr></table></figure>

<h3 id="4-2-disableChaining"><a href="#4-2-disableChaining" class="headerlink" title="4.2 disableChaining"></a>4.2 disableChaining</h3><p> disableChaining 操作用于禁止将其他操作与当前操作放置于同一个任务链中，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">someStream.map(...).disableChaining();</span><br></pre></td></tr></table></figure>

<h3 id="4-3-slotSharingGroup"><a href="#4-3-slotSharingGroup" class="headerlink" title="4.3 slotSharingGroup"></a>4.3 slotSharingGroup</h3><p>slot 是任务管理器  (TaskManager) 所拥有资源的固定子集，每个操作 (operation) 的子任务 (sub task) 都需要获取 slot 来执行计算，但每个操作所需要资源的大小都是不相同的，为了更好地利用资源，Flink 允许不同操作的子任务被部署到同一 slot 中。slotSharingGroup 用于设置操作的 slot 共享组 (slot sharing group) ，Flink 会将具有相同 slot 共享组的操作放到同一个 slot 中 。示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">someStream.filter(...).slotSharingGroup(<span class="string">&quot;slotSharingGroupName&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="参考资料-2"><a href="#参考资料-2" class="headerlink" title="参考资料"></a>参考资料</h2><p>Flink Operators： <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/</a> </p>
<h1 id="Flink-Sink-定义数据流最终的输出位置"><a href="#Flink-Sink-定义数据流最终的输出位置" class="headerlink" title="Flink Sink 定义数据流最终的输出位置"></a>Flink Sink 定义数据流最终的输出位置</h1><h2 id="一、Data-Sinks"><a href="#一、Data-Sinks" class="headerlink" title="一、Data Sinks"></a>一、Data Sinks</h2><p>在使用 Flink 进行数据处理时，数据经 Data Source 流入，然后通过系列 Transformations 的转化，最终可以通过 Sink 将计算结果进行输出，Flink Data Sinks 就是用于定义数据流最终的输出位置。Flink 提供了几个较为简单的 Sink API 用于日常的开发，具体如下：</p>
<h3 id="1-1-writeAsText"><a href="#1-1-writeAsText" class="headerlink" title="1.1 writeAsText"></a>1.1 writeAsText</h3><p><code>writeAsText</code> 用于将计算结果以文本的方式并行地写入到指定文件夹下，除了路径参数是必选外，该方法还可以通过指定第二个参数来定义输出模式，它有以下两个可选值：</p>
<ul>
<li><strong>WriteMode.NO_OVERWRITE</strong>：当指定路径上不存在任何文件时，才执行写出操作；</li>
<li><strong>WriteMode.OVERWRITE</strong>：不论指定路径上是否存在文件，都执行写出操作；如果原来已有文件，则进行覆盖。</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamSource.writeAsText(<span class="string">&quot;D:\\out&quot;</span>, FileSystem.WriteMode.OVERWRITE);</span><br></pre></td></tr></table></figure>

<p>以上写出是以并行的方式写出到多个文件，如果想要将输出结果全部写出到一个文件，需要设置其并行度为 1：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamSource.writeAsText(<span class="string">&quot;D:\\out&quot;</span>, FileSystem.WriteMode.OVERWRITE).setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<h3 id="1-2-writeAsCsv"><a href="#1-2-writeAsCsv" class="headerlink" title="1.2 writeAsCsv"></a>1.2 writeAsCsv</h3><p><code>writeAsCsv</code> 用于将计算结果以 CSV 的文件格式写出到指定目录，除了路径参数是必选外，该方法还支持传入输出模式，行分隔符，和字段分隔符三个额外的参数，其方法定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writeAsCsv(String path, WriteMode writeMode, String rowDelimiter, String fieldDelimiter) </span><br></pre></td></tr></table></figure>

<h3 id="1-3-print-printToErr"><a href="#1-3-print-printToErr" class="headerlink" title="1.3 print \ printToErr"></a>1.3 print \ printToErr</h3><p><code>print \ printToErr</code> 是测试当中最常用的方式，用于将计算结果以标准输出流或错误输出流的方式打印到控制台上。</p>
<h3 id="1-4-writeUsingOutputFormat"><a href="#1-4-writeUsingOutputFormat" class="headerlink" title="1.4 writeUsingOutputFormat"></a>1.4 writeUsingOutputFormat</h3><p>采用自定义的输出格式将计算结果写出，上面介绍的 <code>writeAsText</code> 和 <code>writeAsCsv</code> 其底层调用的都是该方法，源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> DataStreamSink&lt;T&gt; <span class="title">writeAsText</span><span class="params">(String path, WriteMode writeMode)</span> </span>&#123;</span><br><span class="line">    TextOutputFormat&lt;T&gt; tof = <span class="keyword">new</span> TextOutputFormat&lt;&gt;(<span class="keyword">new</span> Path(path));</span><br><span class="line">    tof.setWriteMode(writeMode);</span><br><span class="line">    <span class="keyword">return</span> writeUsingOutputFormat(tof);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-5-writeToSocket"><a href="#1-5-writeToSocket" class="headerlink" title="1.5 writeToSocket"></a>1.5 writeToSocket</h3><p><code>writeToSocket</code> 用于将计算结果以指定的格式写出到 Socket 中，使用示例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamSource.writeToSocket(&quot;192.168.0.226&quot;, 9999, new SimpleStringSchema());</span><br></pre></td></tr></table></figure>



<h2 id="二、Streaming-Connectors"><a href="#二、Streaming-Connectors" class="headerlink" title="二、Streaming Connectors"></a>二、Streaming Connectors</h2><p>除了上述 API 外，Flink 中还内置了系列的 Connectors 连接器，用于将计算结果输入到常用的存储系统或者消息中间件中，具体如下：</p>
<ul>
<li>Apache Kafka (支持 source 和 sink)</li>
<li>Apache Cassandra (sink)</li>
<li>Amazon Kinesis Streams (source/sink)</li>
<li>Elasticsearch (sink)</li>
<li>Hadoop FileSystem (sink)</li>
<li>RabbitMQ (source/sink)</li>
<li>Apache NiFi (source/sink)</li>
<li>Google PubSub (source/sink)</li>
</ul>
<p>除了内置的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink Sink 相关的连接器如下：</p>
<ul>
<li>Apache ActiveMQ (source/sink)</li>
<li>Apache Flume (sink)</li>
<li>Redis (sink)</li>
<li>Akka (sink)</li>
</ul>
<p>这里接着在 Data Sources 章节介绍的整合 Kafka Source 的基础上，将 Kafka Sink 也一并进行整合，具体步骤如下。</p>
<h2 id="三、整合-Kafka-Sink"><a href="#三、整合-Kafka-Sink" class="headerlink" title="三、整合 Kafka Sink"></a>三、整合 Kafka Sink</h2><h3 id="3-1-addSink"><a href="#3-1-addSink" class="headerlink" title="3.1 addSink"></a>3.1 addSink</h3><p>Flink 提供了 addSink 方法用来调用自定义的 Sink 或者第三方的连接器，想要将计算结果写出到 Kafka，需要使用该方法来调用 Kafka 的生产者 FlinkKafkaProducer，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.指定Kafka的相关配置属性</span></span><br><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.200.0:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.接收Kafka上的数据</span></span><br><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(<span class="string">&quot;flink-stream-in-topic&quot;</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.定义计算结果到 Kafka ProducerRecord 的转换</span></span><br><span class="line">KafkaSerializationSchema&lt;String&gt; kafkaSerializationSchema = <span class="keyword">new</span> KafkaSerializationSchema&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; serialize(String element, <span class="meta">@Nullable</span> Long timestamp) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;flink-stream-out-topic&quot;</span>, element.getBytes());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 4. 定义Flink Kafka生产者</span></span><br><span class="line">FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(<span class="string">&quot;flink-stream-out-topic&quot;</span>,</span><br><span class="line">                                                                    kafkaSerializationSchema,</span><br><span class="line">                                                                    properties,</span><br><span class="line">                                               FlinkKafkaProducer.Semantic.AT_LEAST_ONCE, <span class="number">5</span>);</span><br><span class="line"><span class="comment">// 5. 将接收到输入元素*2后写出到Kafka</span></span><br><span class="line">stream.map((MapFunction&lt;String, String&gt;) value -&gt; value + value).addSink(kafkaProducer);</span><br><span class="line">env.execute(<span class="string">&quot;Flink Streaming&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="3-2-创建输出主题"><a href="#3-2-创建输出主题" class="headerlink" title="3.2 创建输出主题"></a>3.2 创建输出主题</h3><p>创建用于输出测试的主题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic flink-stream-out-topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>

<h3 id="3-3-启动消费者"><a href="#3-3-启动消费者" class="headerlink" title="3.3 启动消费者"></a>3.3 启动消费者</h3><p>启动一个 Kafka 消费者，用于查看 Flink 程序的输出情况：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop001:<span class="number">9092</span> --topic flink-stream-out-topic</span><br></pre></td></tr></table></figure>

<h3 id="3-4-测试结果"><a href="#3-4-测试结果" class="headerlink" title="3.4 测试结果"></a>3.4 测试结果</h3><p>在 Kafka 生产者上发送消息到 Flink 程序，观察 Flink 程序转换后的输出情况，具体如下：</p>
<div align="center"> <img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/flink-kafka-producer-consumer.png"/> </div>


<p>可以看到 Kafka 生成者发出的数据已经被 Flink 程序正常接收到，并经过转换后又输出到 Kafka 对应的 Topic 上。</p>
<h2 id="四、自定义-Sink"><a href="#四、自定义-Sink" class="headerlink" title="四、自定义 Sink"></a>四、自定义 Sink</h2><p>除了使用内置的第三方连接器外，Flink 还支持使用自定义的 Sink 来满足多样化的输出需求。想要实现自定义的 Sink ，需要直接或者间接实现 SinkFunction 接口。通常情况下，我们都是实现其抽象类 RichSinkFunction，相比于 SinkFunction ，其提供了更多的与生命周期相关的方法。两者间的关系如下：</p>
<div align="center"> <img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/flink-richsink.png"/> </div>


<p>这里我们以自定义一个 FlinkToMySQLSink 为例，将计算结果写出到 MySQL 数据库中，具体步骤如下：</p>
<h3 id="4-1-导入依赖"><a href="#4-1-导入依赖" class="headerlink" title="4.1 导入依赖"></a>4.1 导入依赖</h3><p>首先需要导入 MySQL 相关的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.16<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-2-自定义-Sink"><a href="#4-2-自定义-Sink" class="headerlink" title="4.2 自定义 Sink"></a>4.2 自定义 Sink</h3><p>继承自 RichSinkFunction，实现自定义的 Sink ：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkToMySQLSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Employee</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> PreparedStatement stmt;</span><br><span class="line">    <span class="keyword">private</span> Connection conn;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Class.forName(<span class="string">&quot;com.mysql.cj.jdbc.Driver&quot;</span>);</span><br><span class="line">        conn = DriverManager.getConnection(<span class="string">&quot;jdbc:mysql://192.168.0.229:3306/employees&quot;</span> +</span><br><span class="line">                                           <span class="string">&quot;?characterEncoding=UTF-8&amp;serverTimezone=UTC&amp;useSSL=false&quot;</span>, </span><br><span class="line">                                           <span class="string">&quot;root&quot;</span>, </span><br><span class="line">                                           <span class="string">&quot;123456&quot;</span>);</span><br><span class="line">        String sql = <span class="string">&quot;insert into emp(name, age, birthday) values(?, ?, ?)&quot;</span>;</span><br><span class="line">        stmt = conn.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Employee value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        stmt.setString(<span class="number">1</span>, value.getName());</span><br><span class="line">        stmt.setInt(<span class="number">2</span>, value.getAge());</span><br><span class="line">        stmt.setDate(<span class="number">3</span>, value.getBirthday());</span><br><span class="line">        stmt.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (stmt != <span class="keyword">null</span>) &#123;</span><br><span class="line">            stmt.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-使用自定义-Sink"><a href="#4-3-使用自定义-Sink" class="headerlink" title="4.3 使用自定义 Sink"></a>4.3 使用自定义 Sink</h3><p>想要使用自定义的 Sink，同样是需要调用 addSink 方法，具体如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Date date = <span class="keyword">new</span> Date(System.currentTimeMillis());</span><br><span class="line">DataStreamSource&lt;Employee&gt; streamSource = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Employee(<span class="string">&quot;hei&quot;</span>, <span class="number">10</span>, date),</span><br><span class="line">    <span class="keyword">new</span> Employee(<span class="string">&quot;bai&quot;</span>, <span class="number">20</span>, date),</span><br><span class="line">    <span class="keyword">new</span> Employee(<span class="string">&quot;ying&quot;</span>, <span class="number">30</span>, date));</span><br><span class="line">streamSource.addSink(<span class="keyword">new</span> FlinkToMySQLSink());</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="4-4-测试结果"><a href="#4-4-测试结果" class="headerlink" title="4.4 测试结果"></a>4.4 测试结果</h3><p>启动程序，观察数据库写入情况：</p>
<div align="center"> <img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/flink-mysql-sink.png"/> </div>


<p>数据库成功写入，代表自定义 Sink 整合成功。</p>
<blockquote>
<p>以上所有用例的源码见本仓库：<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Flink/flink-kafka-integration">flink-kafka-integration</a></p>
</blockquote>
<h2 id="参考资料-3"><a href="#参考资料-3" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>data-sinks： <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks</a> </li>
<li>Streaming Connectors：<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html</a></li>
<li>Apache Kafka Connector： <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html</a> </li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E7%BB%BC%E8%BF%B0.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Flink核心概念综述.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Flink_Data_Source.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Flink_Data_Source.md</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Flink/" rel="tag"># Flink</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/25/Ubuntu%E9%85%8D%E7%BD%AEJava%E7%8E%AF%E5%A2%83/" rel="prev" title="Ubuntu配置Java环境">
      <i class="fa fa-chevron-left"></i> Ubuntu配置Java环境
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">Flink核心概念概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Flink%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">一、Flink简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Flink%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84"><span class="nav-number">1.2.</span> <span class="nav-text">二、Flink核心架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Flink%E5%88%86%E5%B1%82API"><span class="nav-number">1.3.</span> <span class="nav-text">三、Flink分层API</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Flink%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84"><span class="nav-number">1.4.</span> <span class="nav-text">四、Flink集群架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 核心组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Task-amp-SubTask"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2  Task &amp; SubTask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3  资源管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E7%BB%84%E4%BB%B6%E9%80%9A%E8%AE%AF"><span class="nav-number">1.4.4.</span> <span class="nav-text">4.4 组件通讯</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81Flink-%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.5.</span> <span class="nav-text">五、Flink 的优点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.6.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink-Data-Source"><span class="nav-number">2.</span> <span class="nav-text">Flink Data Source</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%86%85%E7%BD%AE-Data-Source"><span class="nav-number">2.1.</span> <span class="nav-text">一、内置 Data Source</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E6%9E%84%E5%BB%BA"><span class="nav-number">2.1.1.</span> <span class="nav-text">1.1 基于文件构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%9F%BA%E4%BA%8E%E9%9B%86%E5%90%88%E6%9E%84%E5%BB%BA"><span class="nav-number">2.1.2.</span> <span class="nav-text">1.2 基于集合构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%9F%BA%E4%BA%8E-Socket-%E6%9E%84%E5%BB%BA"><span class="nav-number">2.1.3.</span> <span class="nav-text">1.3  基于 Socket 构建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89-Data-Source"><span class="nav-number">2.2.</span> <span class="nav-text">二、自定义 Data Source</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-SourceFunction"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.1 SourceFunction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-ParallelSourceFunction-%E5%92%8C-RichParallelSourceFunction"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2 ParallelSourceFunction 和 RichParallelSourceFunction</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Streaming-Connectors"><span class="nav-number">2.3.</span> <span class="nav-text">三、Streaming Connectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E5%86%85%E7%BD%AE%E8%BF%9E%E6%8E%A5%E5%99%A8"><span class="nav-number">2.3.1.</span> <span class="nav-text">3.1 内置连接器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%95%B4%E5%90%88-Kakfa"><span class="nav-number">2.3.2.</span> <span class="nav-text">3.2 整合 Kakfa</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">1. 导入依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%BC%80%E5%8F%91"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">2. 代码开发</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E6%95%B4%E5%90%88%E6%B5%8B%E8%AF%95"><span class="nav-number">2.3.3.</span> <span class="nav-text">3.3 整合测试</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%90%AF%E5%8A%A8-Kakfa"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">1. 启动 Kakfa</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%88%9B%E5%BB%BA-Topic"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">2. 创建 Topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%90%AF%E5%8A%A8-Producer"><span class="nav-number">2.3.3.3.</span> <span class="nav-text">3. 启动 Producer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-number">2.3.3.4.</span> <span class="nav-text">4. 测试结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-1"><span class="nav-number">2.4.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink-Transformation"><span class="nav-number">3.</span> <span class="nav-text">Flink Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Transformations-%E5%88%86%E7%B1%BB"><span class="nav-number">3.1.</span> <span class="nav-text">一、Transformations 分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81DataStream-Transformations"><span class="nav-number">3.2.</span> <span class="nav-text">二、DataStream Transformations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Map-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1 Map [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-FlatMap-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2 FlatMap [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Filter-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.3 Filter [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-KeyBy-%E5%92%8C-Reduce"><span class="nav-number">3.2.4.</span> <span class="nav-text">2.4 KeyBy 和 Reduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Aggregations-KeyedStream-%E2%86%92-DataStream"><span class="nav-number">3.2.5.</span> <span class="nav-text">2.5 Aggregations [KeyedStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-Union-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.2.6.</span> <span class="nav-text">2.6 Union [DataStream* → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-Connect-DataStream-DataStream-%E2%86%92-ConnectedStreams"><span class="nav-number">3.2.7.</span> <span class="nav-text">2.7 Connect [DataStream,DataStream → ConnectedStreams]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-Split-%E5%92%8C-Select"><span class="nav-number">3.2.8.</span> <span class="nav-text">2.8 Split 和 Select</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-9-project-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.2.9.</span> <span class="nav-text">2.9 project [DataStream → DataStream]</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E7%89%A9%E7%90%86%E5%88%86%E5%8C%BA"><span class="nav-number">3.3.</span> <span class="nav-text">三、物理分区</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Random-partitioning-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.1 Random partitioning [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Rebalancing-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.2 Rebalancing [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Rescaling-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3 Rescaling [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Broadcasting-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.3.4.</span> <span class="nav-text">3.4 Broadcasting [DataStream → DataStream]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Custom-partitioning-DataStream-%E2%86%92-DataStream"><span class="nav-number">3.3.5.</span> <span class="nav-text">3.5 Custom partitioning [DataStream → DataStream]</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E4%BB%BB%E5%8A%A1%E9%93%BE%E5%92%8C%E8%B5%84%E6%BA%90%E7%BB%84"><span class="nav-number">3.4.</span> <span class="nav-text">四、任务链和资源组</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-startNewChain"><span class="nav-number">3.4.1.</span> <span class="nav-text">4.1 startNewChain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-disableChaining"><span class="nav-number">3.4.2.</span> <span class="nav-text">4.2 disableChaining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-slotSharingGroup"><span class="nav-number">3.4.3.</span> <span class="nav-text">4.3 slotSharingGroup</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-2"><span class="nav-number">3.5.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink-Sink-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%B5%81%E6%9C%80%E7%BB%88%E7%9A%84%E8%BE%93%E5%87%BA%E4%BD%8D%E7%BD%AE"><span class="nav-number">4.</span> <span class="nav-text">Flink Sink 定义数据流最终的输出位置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Data-Sinks"><span class="nav-number">4.1.</span> <span class="nav-text">一、Data Sinks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-writeAsText"><span class="nav-number">4.1.1.</span> <span class="nav-text">1.1 writeAsText</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-writeAsCsv"><span class="nav-number">4.1.2.</span> <span class="nav-text">1.2 writeAsCsv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-print-printToErr"><span class="nav-number">4.1.3.</span> <span class="nav-text">1.3 print \ printToErr</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-writeUsingOutputFormat"><span class="nav-number">4.1.4.</span> <span class="nav-text">1.4 writeUsingOutputFormat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-writeToSocket"><span class="nav-number">4.1.5.</span> <span class="nav-text">1.5 writeToSocket</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Streaming-Connectors"><span class="nav-number">4.2.</span> <span class="nav-text">二、Streaming Connectors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%95%B4%E5%90%88-Kafka-Sink"><span class="nav-number">4.3.</span> <span class="nav-text">三、整合 Kafka Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-addSink"><span class="nav-number">4.3.1.</span> <span class="nav-text">3.1 addSink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E5%88%9B%E5%BB%BA%E8%BE%93%E5%87%BA%E4%B8%BB%E9%A2%98"><span class="nav-number">4.3.2.</span> <span class="nav-text">3.2 创建输出主题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%90%AF%E5%8A%A8%E6%B6%88%E8%B4%B9%E8%80%85"><span class="nav-number">4.3.3.</span> <span class="nav-text">3.3 启动消费者</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-number">4.3.4.</span> <span class="nav-text">3.4 测试结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89-Sink"><span class="nav-number">4.4.</span> <span class="nav-text">四、自定义 Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="nav-number">4.4.1.</span> <span class="nav-text">4.1 导入依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E8%87%AA%E5%AE%9A%E4%B9%89-Sink"><span class="nav-number">4.4.2.</span> <span class="nav-text">4.2 自定义 Sink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89-Sink"><span class="nav-number">4.4.3.</span> <span class="nav-text">4.3 使用自定义 Sink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-number">4.4.4.</span> <span class="nav-text">4.4 测试结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-3"><span class="nav-number">4.5.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Tommy Hu</p>
  <div class="site-description" itemprop="description">写点东西</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tommy Hu</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'e26b1eec8da027069747',
      clientSecret: '48d14ef26caf77c5a7154a3c0cf08d66f6d21549',
      repo        : 'BlogComments',
      owner       : 'RealTommyHu',
      admin       : ['RealTommyHu'],
      id          : '4879740c0969b357a0087cd075fd4226',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
