<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Spark简介一、简介Apache Spark是继 MapReduce 之后，最为广泛使用的分布式计算框架。相对于 MapReduce 的批处理计算，可以带来上百倍的性能提升。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark核心技术">
<meta property="og:url" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/index.html">
<meta property="og:site_name" content="TommyHu的技术小馆">
<meta property="og:description" content="Spark简介一、简介Apache Spark是继 MapReduce 之后，最为广泛使用的分布式计算框架。相对于 MapReduce 的批处理计算，可以带来上百倍的性能提升。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/framework-of-spark.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/cluster-arch.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/4-core-comp.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-io.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-microBatches.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E4%BB%A5reduceByKey%E4%B8%BA%E4%BE%8B%E8%A7%A3%E9%87%8Ashuffle%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/RDD%E5%92%8C%E5%85%B6%E7%88%B6RDD%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E6%A0%B9%E6%8D%AE%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%E5%88%92%E5%88%86%E8%AE%A1%E7%AE%97%E9%98%B6%E6%AE%B5.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/aggregateByKey%E6%A0%B7%E4%BE%8B%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.png">
<meta property="article:published_time" content="2021-04-23T13:43:46.000Z">
<meta property="article:modified_time" content="2021-04-26T09:55:50.010Z">
<meta property="article:author" content="Tommy Hu">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/framework-of-spark.png">

<link rel="canonical" href="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark核心技术 | TommyHu的技术小馆</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TommyHu的技术小馆</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">微信公众号：TommyHu的技术小馆</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tommy Hu">
      <meta itemprop="description" content="写点东西">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TommyHu的技术小馆">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark核心技术
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-04-23 21:43:46" itemprop="dateCreated datePublished" datetime="2021-04-23T21:43:46+08:00">2021-04-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-04-26 17:55:50" itemprop="dateModified" datetime="2021-04-26T17:55:50+08:00">2021-04-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Apache Spark是继 MapReduce 之后，最为广泛使用的分布式计算框架。相对于 MapReduce 的批处理计算，可以带来上百倍的性能提升。</p>
<span id="more"></span>

<h2 id="二、特点"><a href="#二、特点" class="headerlink" title="二、特点"></a>二、特点</h2><p>具有以下特点：</p>
<ul>
<li>多语言支持：Java、Scala、Python。</li>
<li>支持批处理、流处理。</li>
<li>丰富的类库支持：SQL，MLlib，GraphX，Spark Streaming。</li>
<li>丰富的部署模式：本地模式、集群模式，也支持在Hadoop、Mesos、Kubernetes上运行。</li>
<li>多数据源支持：支持访问HDFS、HBase、Hive等数百个数据源中的数据。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/framework-of-spark.png"></li>
</ul>
<h2 id="三、集群架构"><a href="#三、集群架构" class="headerlink" title="三、集群架构"></a>三、集群架构</h2><table>
<thead>
<tr>
<th>Term（术语）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。</td>
</tr>
<tr>
<td>Driver program</td>
<td>主应用程序，该进程运行应用的 main() 方法并且创建 SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>集群资源管理器（例如，Standlone Manager，Mesos，YARN）</td>
</tr>
<tr>
<td>Worker node</td>
<td>执行计算任务的工作节点</td>
</tr>
<tr>
<td>Executor</td>
<td>位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中</td>
</tr>
<tr>
<td>Task</td>
<td>被发送到 Executor 中的工作单元</td>
</tr>
</tbody></table>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/cluster-arch.png"></p>
<p>执行过程：</p>
<ol>
<li>用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor；</li>
<li>Driver 将计算程序划分为不同的执行阶段和多个 Task，之后将 Task 发送给 Executor；</li>
<li>Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。</li>
</ol>
<h2 id="四、核心组件"><a href="#四、核心组件" class="headerlink" title="四、核心组件"></a>四、核心组件</h2><p>Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/4-core-comp.png"></p>
<h3 id="1-Spark-SQL"><a href="#1-Spark-SQL" class="headerlink" title="1 Spark SQL"></a>1 Spark SQL</h3><p>Spark SQL 主要用于结构化数据的处理。其具有以下特点：</p>
<ul>
<li>允许使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li>支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC；</li>
<li>支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性，以提高查询效率。</li>
</ul>
<h3 id="2-Spark-Streaming"><a href="#2-Spark-Streaming" class="headerlink" title="2 Spark Streaming"></a>2 Spark Streaming</h3><p>Spark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-io.png"></p>
<p>Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-microBatches.png"></p>
<h3 id="3-MLlib"><a href="#3-MLlib" class="headerlink" title="3 MLlib"></a>3 MLlib</h3><p>MLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具：</p>
<ul>
<li>常见的机器学习算法：如分类，回归，聚类和协同过滤；</li>
<li>特征化：特征提取，转换，降维和选择；</li>
<li>管道：用于构建，评估和调整 ML 管道的工具；</li>
<li>持久性：保存和加载算法，模型，管道数据；</li>
<li>实用工具：线性代数，统计，数据处理等。</li>
</ul>
<h3 id="4-Graphx"><a href="#4-Graphx" class="headerlink" title="4 Graphx"></a>4 Graphx</h3><p>GraphX 是 Spark 中用于图计算和图并行计算的新组件。在高层次上，GraphX 通过引入一个新的图抽象来扩展 RDD(一种具有附加到每个顶点和连边的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如： subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图算法和构建器，以简化图分析任务。</p>
<h1 id="弹性分布式数据集RDDs"><a href="#弹性分布式数据集RDDs" class="headerlink" title="弹性分布式数据集RDDs"></a>弹性分布式数据集RDDs</h1><h2 id="一、RDD简介"><a href="#一、RDD简介" class="headerlink" title="一、RDD简介"></a>一、RDD简介</h2><p>Resilient Distributed Datasets，只读、分区记录的集合。支持并行，可由外部数据集或者其它RDD转换。有以下特性：</p>
<ul>
<li>由一个或多个分区组成，每个分区被一个计算任务处理，可在创建RDD时指定分区个数，未指定则默认采用程序所分配到的CPU核心数。</li>
<li>有一个函数compute来计算分区。</li>
<li>RDD的每次转换都会生成一个新的依赖关系，并且RDD会保存彼此间的依赖关系，在部分分区数据丢失后可通过这种依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
<li>Key-Value型的RDD拥有分区器来决定数据存储的分区，支持哈希分区和范围分区。</li>
<li>有一个可选的优先位置列表存储每个分区的优先位置，对于一个HDFS文件来说，这个列表保存的就是每个分区所在的块的位置，按“移动数据不如移动计算”的理念，在任务调度时会尽可能将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<p>RDD[T] 抽象类的部分相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 由子类实现以计算给定分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有分区</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有依赖关系</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取优先位置列表</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区器 由子类重写以指定它们的分区方式</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure>

<h2 id="二、创建RDD"><a href="#二、创建RDD" class="headerlink" title="二、创建RDD"></a>二、创建RDD</h2><p>RDD有两种创建方式，分别介绍如下：</p>
<h3 id="2-1-由现有集合创建"><a href="#2-1-由现有集合创建" class="headerlink" title="2.1 由现有集合创建"></a>2.1 由现有集合创建</h3><p>这里使用 spark-shell 进行测试，启动命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master <span class="built_in">local</span>[4]</span><br></pre></td></tr></table></figure>

<p>启动 spark-shell 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Spark shell&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>

<p>由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data) </span><br><span class="line"><span class="comment">// 查看分区数</span></span><br><span class="line">dataRDD.getNumPartitions <span class="comment">// 输出4</span></span><br><span class="line"><span class="comment">// 明确指定分区数</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data,<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 查看分区数</span></span><br><span class="line">dataRDD.getNumPartitions <span class="comment">// 输出2</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-应用外部存储系统中的数据集"><a href="#2-2-应用外部存储系统中的数据集" class="headerlink" title="2.2 应用外部存储系统中的数据集"></a>2.2 应用外部存储系统中的数据集</h3><p>引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">&quot;/usr/file/emp.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 获取第一行文本</span></span><br><span class="line">fileRDD.take(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>使用外部存储系统时需要注意以下两点：</p>
<ul>
<li>如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同；</li>
<li>支持目录路径，支持压缩文件，支持使用通配符。</li>
</ul>
<h3 id="2-3-textFile-amp-wholeTextFiles"><a href="#2-3-textFile-amp-wholeTextFiles" class="headerlink" title="2.3 textFile &amp; wholeTextFiles"></a>2.3 textFile &amp; wholeTextFiles</h3><p>两者都可以用来读取外部文件，但是返回格式是不同的：</p>
<ul>
<li>textFile：其返回格式是 RDD[String] ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；</li>
<li>wholeTextFiles：其返回格式是 RDD[(String, String)]，元组中第一个参数是文件路径，第二个参数是文件内容；</li>
<li>两者都提供第二个参数来控制最小分区数；</li>
<li>从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;...&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wholeTextFiles</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)]=&#123;..&#125;</span><br></pre></td></tr></table></figure>

<h2 id="三、操作RDD"><a href="#三、操作RDD" class="headerlink" title="三、操作RDD"></a>三、操作RDD</h2><p>RDD 支持两种类型的操作：transformations（转换，从现有数据集创建新数据集）和 actions（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 action 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">// map 是一个 transformations 操作，而 foreach 是一个 actions 操作</span></span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出： 10 20 30</span></span><br></pre></td></tr></table></figure>

<h2 id="四、缓存RDD"><a href="#四、缓存RDD" class="headerlink" title="四、缓存RDD"></a>四、缓存RDD</h2><h3 id="4-1-缓存级别"><a href="#4-1-缓存级别" class="headerlink" title="4.1 缓存级别"></a>4.1 缓存级别</h3><p>Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。</p>
<p>Spark 支持多种缓存级别 ：</p>
<table>
<thead>
<tr>
<th>Storage Level 存储级别</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>类似于 MEMORY_ONLY_SER，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>只在磁盘上缓存 RDD</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc</td>
<td>与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。</td>
</tr>
<tr>
<td>OFF_HEAP</td>
<td>与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。这需要启用堆外内存。</td>
</tr>
</tbody></table>
<blockquote>
<p>启动堆外内存需要配置两个参数：</p>
<ul>
<li>spark.memory.offHeap.enabled ：是否开启堆外内存，默认值为 false，需要设置为 true；</li>
<li>spark.memory.offHeap.size : 堆外内存空间的大小，默认值为 0，需要设置为正值。</li>
</ul>
</blockquote>
<h3 id="4-2-使用缓存"><a href="#4-2-使用缓存" class="headerlink" title="4.2 使用缓存"></a>4.2 使用缓存</h3><p>缓存数据的方法有两个：persist 和 cache 。cache 内部调用的也是 persist，它是 persist 的特殊化形式，等价于 persist(StorageLevel.MEMORY_ONLY)。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所有存储级别均定义在 StorageLevel 对象中</span></span><br><span class="line">fileRDD.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">fileRDD.cache()</span><br></pre></td></tr></table></figure>
<p>一个已分配存储级别的RDD不能直接更改存储级别。</p>
<h3 id="4-3-移除缓存"><a href="#4-3-移除缓存" class="headerlink" title="4.3 移除缓存"></a>4.3 移除缓存</h3><ul>
<li>Spark 自动监视每个节点上的缓存使用，按照最近最少使用（LRU）的规则删除旧数据分区。</li>
<li>也可以使用 RDD.unpersist() 方法进行手动删除。</li>
</ul>
<h2 id="五、理解shuffle"><a href="#五、理解shuffle" class="headerlink" title="五、理解shuffle"></a>五、理解shuffle</h2><h3 id="5-1-shuffle介绍"><a href="#5-1-shuffle介绍" class="headerlink" title="5.1 shuffle介绍"></a>5.1 shuffle介绍</h3><p>Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 reduceByKey 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 Shuffle。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E4%BB%A5reduceByKey%E4%B8%BA%E4%BE%8B%E8%A7%A3%E9%87%8Ashuffle%E8%BF%87%E7%A8%8B.png" alt="以reduceByKey为例解释shuffle过程"></p>
<h3 id="5-2-Shuffle的影响"><a href="#5-2-Shuffle的影响" class="headerlink" title="5.2 Shuffle的影响"></a>5.2 Shuffle的影响</h3><p>Shuffle 是项昂贵的操作，跨节点操作数据，涉及磁盘 I/O，网络 I/O，和数据序列化。消耗大量的堆内存，用堆内存来临时存储需要网络传输的数据。在磁盘上生成大量中间文件，避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 spark.local.dir 参数来指定这些临时文件的存储目录。</p>
<h3 id="5-3-导致Shuffle的操作"><a href="#5-3-导致Shuffle的操作" class="headerlink" title="5.3 导致Shuffle的操作"></a>5.3 导致Shuffle的操作</h3><p>Shuffle 操作对性能的影响比较大，所以需要特别注意使用，导致Shuffle的操作：</p>
<ul>
<li>涉及到重新分区操作： 如 repartition 和 coalesce；</li>
<li>所有涉及到 ByKey 的操作：如 groupByKey 和 reduceByKey，但 countByKey 除外；</li>
<li>联结操作：如 cogroup 和 join。</li>
</ul>
<h2 id="五、宽依赖和窄依赖"><a href="#五、宽依赖和窄依赖" class="headerlink" title="五、宽依赖和窄依赖"></a>五、宽依赖和窄依赖</h2><p>RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：</p>
<ul>
<li>窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；</li>
<li>宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。</li>
</ul>
<p>如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/RDD%E5%92%8C%E5%85%B6%E7%88%B6RDD%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png" alt="RDD和其父RDD之间的依赖关系"></p>
<p>区分这两种依赖是非常有用的：</p>
<ul>
<li>窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。</li>
<li>而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。</li>
<li>窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；</li>
<li>而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。</li>
</ul>
<h2 id="六、DAG的生成"><a href="#六、DAG的生成" class="headerlink" title="六、DAG的生成"></a>六、DAG的生成</h2><p>RDD(s)及其之间的依赖关系组成了 DAG(有向无环图)，通过依赖关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。Spark根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)：</p>
<ul>
<li>窄依赖，分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；</li>
<li>宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。</li>
</ul>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E6%A0%B9%E6%8D%AE%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%E5%88%92%E5%88%86%E8%AE%A1%E7%AE%97%E9%98%B6%E6%AE%B5.png" alt="根据宽窄依赖划分计算阶段"></p>
<h1 id="Transformation-和-Action-常用算子"><a href="#Transformation-和-Action-常用算子" class="headerlink" title="Transformation 和 Action 常用算子"></a>Transformation 和 Action 常用算子</h1><h2 id="一、Transformation"><a href="#一、Transformation" class="headerlink" title="一、Transformation"></a>一、Transformation</h2><p>spark 常用的 Transformation 算子如下表：</p>
<table>
<thead>
<tr>
<th>Transformation 算子</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素运用 <em>func</em> 函数，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素使用<em>func</em> 函数进行过滤，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 Seq ）。</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为  Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; ，其中 T 是 RDD 的类型，即 RDD[T]</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>与 mapPartitions 类似，但 <em>func</em> 类型为 (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; ，其中第一个参数为分区索引</td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）；</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td>合并两个 RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td>求两个 RDD 的交集</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numTasks</em>]))</td>
<td>去重</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>
<td>按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable&lt;V&gt;) <br/><strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 性能会更好<br><strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td>按照 key 值进行分组，并对分组后的数据执行归约操作。</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>
<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>
<td>按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples 的 dataset。</td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td>将 RDD 中的分区数减少为 numPartitions。</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。</td>
</tr>
</tbody></table>
<p>下面分别给出这些算子的基本使用示例：</p>
<h3 id="1-1-map"><a href="#1-1-map" class="headerlink" title="1.1 map"></a>1.1 map</h3><p>对原 RDD 中每个元素运用 func 函数，并生成新的 RDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果： 10 20 30 （这里为了节省篇幅去掉了换行,后文亦同）</span></span><br></pre></td></tr></table></figure>

<h3 id="1-2-filter"><a href="#1-2-filter" class="headerlink" title="1.2 filter"></a>1.2 filter</h3><p>对原 RDD 中每个元素使用func 函数进行过滤，并生成新的 RDD 。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">21</span>)</span><br><span class="line">sc.parallelize(list).filter(_ &gt;= <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 10 12 21</span></span><br></pre></td></tr></table></figure>

<h3 id="1-3-flatMap"><a href="#1-3-flatMap" class="headerlink" title="1.3 flatMap"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的 item 会被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 <code>Seq</code>）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>), <span class="type">List</span>(), <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">sc.parallelize(list).flatMap(_.toList).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果 ： 10 20 30 40 50</span></span><br></pre></td></tr></table></figure>

<p>flatMap 这个算子在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为 1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = <span class="type">List</span>(<span class="string">&quot;spark flume spark&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;hadoop flume hive&quot;</span>)</span><br><span class="line">sc.parallelize(lines).flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).</span><br><span class="line">map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line">(spark,<span class="number">2</span>)</span><br><span class="line">(hive,<span class="number">1</span>)</span><br><span class="line">(hadoop,<span class="number">1</span>)</span><br><span class="line">(flume,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-4-mapPartitions"><a href="#1-4-mapPartitions" class="headerlink" title="1.4 mapPartitions"></a>1.4 mapPartitions</h3><p>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 <code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中 T 是 RDD 的类型)，即输入和输出都必须是可迭代类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitions(iterator =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出结果</span></span><br><span class="line"><span class="number">100</span> <span class="number">200</span> <span class="number">300</span> <span class="number">400</span> <span class="number">500</span> <span class="number">600</span></span><br></pre></td></tr></table></figure>

<h3 id="1-5-mapPartitionsWithIndex"><a href="#1-5-mapPartitionsWithIndex" class="headerlink" title="1.5 mapPartitionsWithIndex"></a>1.5 mapPartitionsWithIndex</h3><p>  与 mapPartitions 类似，但 <em>func</em> 类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(index + <span class="string">&quot;分区:&quot;</span> + iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">0</span> 分区:<span class="number">100</span></span><br><span class="line"><span class="number">0</span> 分区:<span class="number">200</span></span><br><span class="line"><span class="number">1</span> 分区:<span class="number">300</span></span><br><span class="line"><span class="number">1</span> 分区:<span class="number">400</span></span><br><span class="line"><span class="number">2</span> 分区:<span class="number">500</span></span><br><span class="line"><span class="number">2</span> 分区:<span class="number">600</span></span><br></pre></td></tr></table></figure>

<h3 id="1-6-sample"><a href="#1-6-sample" class="headerlink" title="1.6 sample"></a>1.6 sample</h3><p>  数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list).sample(withReplacement = <span class="literal">false</span>, fraction = <span class="number">0.5</span>).foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="1-7-union"><a href="#1-7-union" class="headerlink" title="1.7 union"></a>1.7 union</h3><p>合并两个 RDD：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)</span><br><span class="line"><span class="comment">// 输出: 1 2 3 4 5 6</span></span><br></pre></td></tr></table></figure>

<h3 id="1-8-intersection"><a href="#1-8-intersection" class="headerlink" title="1.8 intersection"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)</span><br><span class="line"><span class="comment">// 输出:  4 5</span></span><br></pre></td></tr></table></figure>

<h3 id="1-9-distinct"><a href="#1-9-distinct" class="headerlink" title="1.9 distinct"></a>1.9 distinct</h3><p>去重：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">sc.parallelize(list).distinct().foreach(println)</span><br><span class="line"><span class="comment">// 输出: 4 1 2</span></span><br></pre></td></tr></table></figure>

<h3 id="1-10-groupByKey"><a href="#1-10-groupByKey" class="headerlink" title="1.10 groupByKey"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">(spark,<span class="type">List</span>(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">(hadoop,<span class="type">List</span>(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">(storm,<span class="type">List</span>(<span class="number">6</span>))</span><br></pre></td></tr></table></figure>

<h3 id="1-11-reduceByKey"><a href="#1-11-reduceByKey" class="headerlink" title="1.11 reduceByKey"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).reduceByKey(_ + _).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">(spark,<span class="number">8</span>)</span><br><span class="line">(hadoop,<span class="number">4</span>)</span><br><span class="line">(storm,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-12-sortBy-amp-sortByKey"><a href="#1-12-sortBy-amp-sortByKey" class="headerlink" title="1.12 sortBy &amp; sortByKey"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">100</span>, <span class="string">&quot;hadoop&quot;</span>), (<span class="number">90</span>, <span class="string">&quot;spark&quot;</span>), (<span class="number">120</span>, <span class="string">&quot;storm&quot;</span>))</span><br><span class="line">sc.parallelize(list01).sortByKey(ascending = <span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">120</span>,storm)</span><br><span class="line">(<span class="number">90</span>,spark)</span><br><span class="line">(<span class="number">100</span>,hadoop)</span><br></pre></td></tr></table></figure>

<p>按照指定元素进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>,<span class="number">100</span>), (<span class="string">&quot;spark&quot;</span>,<span class="number">90</span>), (<span class="string">&quot;storm&quot;</span>,<span class="number">120</span>))</span><br><span class="line">sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=<span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(storm,<span class="number">120</span>)</span><br><span class="line">(hadoop,<span class="number">100</span>)</span><br><span class="line">(spark,<span class="number">90</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-13-join"><a href="#1-13-join" class="headerlink" title="1.13 join"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的 Dataset 上调用时，返回一个 (K, (V, W)) 的 Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;student01&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;student02&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;student03&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;teacher01&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;teacher02&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;teacher03&quot;</span>))</span><br><span class="line">sc.parallelize(list01).join(sc.parallelize(list02)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">1</span>,(student01,teacher01))</span><br><span class="line">(<span class="number">3</span>,(student03,teacher03))</span><br><span class="line">(<span class="number">2</span>,(student02,teacher02))</span><br></pre></td></tr></table></figure>

<h3 id="1-14-cogroup"><a href="#1-14-cogroup" class="headerlink" title="1.14 cogroup"></a>1.14 cogroup</h3><blockquote>
<p><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</p>
</blockquote>
<p>在一个 (K, V) 对的 Dataset 上调用时，返回多个类型为 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) 的元组所组成的 Dataset。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>),(<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;e&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;A&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;B&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;E&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list03 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;[ab]&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;[bB]&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;eE&quot;</span>),(<span class="number">3</span>, <span class="string">&quot;eE&quot;</span>))</span><br><span class="line">sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 同一个 RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组</span></span><br><span class="line">(<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a, a),<span class="type">CompactBuffer</span>(<span class="type">A</span>),<span class="type">CompactBuffer</span>([ab])))</span><br><span class="line">(<span class="number">3</span>,(<span class="type">CompactBuffer</span>(e),<span class="type">CompactBuffer</span>(<span class="type">E</span>),<span class="type">CompactBuffer</span>(eE, eE)))</span><br><span class="line">(<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="type">B</span>),<span class="type">CompactBuffer</span>([bB])))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="1-15-cartesian"><a href="#1-15-cartesian" class="headerlink" title="1.15 cartesian"></a>1.15 cartesian</h3><p>计算笛卡尔积：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出笛卡尔积</span></span><br><span class="line">(<span class="type">A</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-16-aggregateByKey"><a href="#1-16-aggregateByKey" class="headerlink" title="1.16 aggregateByKey"></a>1.16 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 <code>groupByKey</code> 类似，reduce 任务的数量可通过第二个参数 <code>numPartitions</code> 进行配置。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为了清晰，以下所有参数均使用具名传参</span></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">8</span>))</span><br><span class="line">sc.parallelize(list,numSlices = <span class="number">2</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">      seqOp = math.max(_, _),</span><br><span class="line">      combOp = _ + _</span><br><span class="line">    ).collect.foreach(println)</span><br><span class="line"><span class="comment">//输出结果：</span></span><br><span class="line">(hadoop,<span class="number">3</span>)</span><br><span class="line">(storm,<span class="number">8</span>)</span><br><span class="line">(spark,<span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<p>这里使用了 <code>numSlices = 2</code> 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/aggregateByKey%E6%A0%B7%E4%BE%8B%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.png"></p>
<p>基于同样的执行流程，如果 <code>numSlices = 1</code>，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">(hadoop,3)</span></span><br><span class="line"><span class="attr">(storm,8)</span></span><br><span class="line"><span class="attr">(spark,4)</span></span><br></pre></td></tr></table></figure>

<p>同样的，如果每个单词对一个分区，即 <code>numSlices = 6</code>，此时相当于求和操作，执行结果为：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">(hadoop,5)</span></span><br><span class="line"><span class="attr">(storm,14)</span></span><br><span class="line"><span class="attr">(spark,7)</span></span><br></pre></td></tr></table></figure>

<p><code>aggregateByKey(zeroValue = 0,numPartitions = 3)</code> 的第二个参数 <code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 <code>getNumPartitions</code> 方法获取分区数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(list,numSlices = <span class="number">6</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">  seqOp = math.max(_, _),</span><br><span class="line">  combOp = _ + _</span><br><span class="line">).getNumPartitions</span><br></pre></td></tr></table></figure>

<h2 id="二、Action"><a href="#二、Action" class="headerlink" title="二、Action"></a>二、Action</h2><p>Spark 常用的 Action 算子如下：</p>
<table>
<thead>
<tr>
<th>Action（动作）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>使用函数<em>func</em>执行归约操作</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回 dataset 中元素的个数。</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回 dataset 中的第一个元素，等价于 take(1)。</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>对一个 dataset 进行随机抽样</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>使用 Java 序列化后存储，可以使用 <code>SparkContext.objectFile()</code> 进行加载。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>计算每个键出现的次数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>遍历 RDD 中每个元素，并对其执行<em>fun</em>函数</td>
</tr>
</tbody></table>
<h3 id="2-1-reduce"><a href="#2-1-reduce" class="headerlink" title="2.1 reduce"></a>2.1 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">sc.parallelize(list).reduce((x, y) =&gt; x + y)</span><br><span class="line">sc.parallelize(list).reduce(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出 15</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-takeOrdered"><a href="#2-2-takeOrdered" class="headerlink" title="2.2 takeOrdered"></a>2.2 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  .........</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义规则排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[(<span class="type">Int</span>, <span class="type">String</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: (<span class="type">Int</span>, <span class="type">String</span>), y: (<span class="type">Int</span>, <span class="type">String</span>)): <span class="type">Int</span></span><br><span class="line">    = <span class="keyword">if</span> (x._2.length &gt; y._2.length) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;hadoop&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;storm&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;azkaban&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;hive&quot;</span>))</span><br><span class="line"><span class="comment">//  引入隐式默认值</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> implicitOrdering = <span class="keyword">new</span> <span class="type">CustomOrdering</span></span><br><span class="line">sc.parallelize(list).takeOrdered(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Array((1,hive), (1,storm), (1,hadoop), (1,azkaban)</span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-countByKey"><a href="#2-3-countByKey" class="headerlink" title="2.3 countByKey"></a>2.3 countByKey</h3><p>计算每个键出现的次数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;azkaban&quot;</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Map(hadoop -&gt; 2, storm -&gt; 2, azkaban -&gt; 1)</span></span><br></pre></td></tr></table></figure>

<h3 id="2-4-saveAsTextFile"><a href="#2-4-saveAsTextFile" class="headerlink" title="2.4 saveAsTextFile"></a>2.4 saveAsTextFile</h3><p>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;azkaban&quot;</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).saveAsTextFile(<span class="string">&quot;/usr/file/temp&quot;</span>)</span><br></pre></td></tr></table></figure>


<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark%E7%AE%80%E4%BB%8B.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark%E7%AE%80%E4%BB%8B.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_RDD.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_RDD.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Transformation%E5%92%8CAction%E7%AE%97%E5%AD%90.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Transformation%E5%92%8CAction%E7%AE%97%E5%AD%90.md</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/22/%E9%80%9A%E8%BF%87GithubPages%E5%92%8C%E9%9D%99%E6%80%81%E7%AB%99%E7%82%B9%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" rel="prev" title="通过GithubPages和静态站点生成框架Hexo搭建博客">
      <i class="fa fa-chevron-left"></i> 通过GithubPages和静态站点生成框架Hexo搭建博客
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/04/25/Ubuntu%E6%9B%B4%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90%E6%8F%90%E9%AB%98%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6/" rel="next" title="Ubuntu更换国内源提高包管理器下载速度">
      Ubuntu更换国内源提高包管理器下载速度 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">Spark简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">一、简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">二、特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84"><span class="nav-number">1.3.</span> <span class="nav-text">三、集群架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">1.4.</span> <span class="nav-text">四、核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Spark-SQL"><span class="nav-number">1.4.1.</span> <span class="nav-text">1 Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Spark-Streaming"><span class="nav-number">1.4.2.</span> <span class="nav-text">2 Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-MLlib"><span class="nav-number">1.4.3.</span> <span class="nav-text">3 MLlib</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Graphx"><span class="nav-number">1.4.4.</span> <span class="nav-text">4 Graphx</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86RDDs"><span class="nav-number">2.</span> <span class="nav-text">弹性分布式数据集RDDs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81RDD%E7%AE%80%E4%BB%8B"><span class="nav-number">2.1.</span> <span class="nav-text">一、RDD简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%88%9B%E5%BB%BARDD"><span class="nav-number">2.2.</span> <span class="nav-text">二、创建RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%94%B1%E7%8E%B0%E6%9C%89%E9%9B%86%E5%90%88%E5%88%9B%E5%BB%BA"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.1 由现有集合创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%BA%94%E7%94%A8%E5%A4%96%E9%83%A8%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2 应用外部存储系统中的数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-textFile-amp-wholeTextFiles"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.3 textFile &amp; wholeTextFiles</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%93%8D%E4%BD%9CRDD"><span class="nav-number">2.3.</span> <span class="nav-text">三、操作RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E7%BC%93%E5%AD%98RDD"><span class="nav-number">2.4.</span> <span class="nav-text">四、缓存RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E7%BC%93%E5%AD%98%E7%BA%A7%E5%88%AB"><span class="nav-number">2.4.1.</span> <span class="nav-text">4.1 缓存级别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98"><span class="nav-number">2.4.2.</span> <span class="nav-text">4.2 使用缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E7%A7%BB%E9%99%A4%E7%BC%93%E5%AD%98"><span class="nav-number">2.4.3.</span> <span class="nav-text">4.3 移除缓存</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E7%90%86%E8%A7%A3shuffle"><span class="nav-number">2.5.</span> <span class="nav-text">五、理解shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-shuffle%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.5.1.</span> <span class="nav-text">5.1 shuffle介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Shuffle%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">2.5.2.</span> <span class="nav-text">5.2 Shuffle的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E5%AF%BC%E8%87%B4Shuffle%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">2.5.3.</span> <span class="nav-text">5.3 导致Shuffle的操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="nav-number">2.6.</span> <span class="nav-text">五、宽依赖和窄依赖</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81DAG%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">2.7.</span> <span class="nav-text">六、DAG的生成</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformation-%E5%92%8C-Action-%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="nav-number">3.</span> <span class="nav-text">Transformation 和 Action 常用算子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Transformation"><span class="nav-number">3.1.</span> <span class="nav-text">一、Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-map"><span class="nav-number">3.1.1.</span> <span class="nav-text">1.1 map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-filter"><span class="nav-number">3.1.2.</span> <span class="nav-text">1.2 filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-flatMap"><span class="nav-number">3.1.3.</span> <span class="nav-text">1.3 flatMap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-mapPartitions"><span class="nav-number">3.1.4.</span> <span class="nav-text">1.4 mapPartitions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-mapPartitionsWithIndex"><span class="nav-number">3.1.5.</span> <span class="nav-text">1.5 mapPartitionsWithIndex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-sample"><span class="nav-number">3.1.6.</span> <span class="nav-text">1.6 sample</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-union"><span class="nav-number">3.1.7.</span> <span class="nav-text">1.7 union</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-intersection"><span class="nav-number">3.1.8.</span> <span class="nav-text">1.8 intersection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-distinct"><span class="nav-number">3.1.9.</span> <span class="nav-text">1.9 distinct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-groupByKey"><span class="nav-number">3.1.10.</span> <span class="nav-text">1.10 groupByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-reduceByKey"><span class="nav-number">3.1.11.</span> <span class="nav-text">1.11 reduceByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-12-sortBy-amp-sortByKey"><span class="nav-number">3.1.12.</span> <span class="nav-text">1.12 sortBy &amp; sortByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-13-join"><span class="nav-number">3.1.13.</span> <span class="nav-text">1.13 join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-14-cogroup"><span class="nav-number">3.1.14.</span> <span class="nav-text">1.14 cogroup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-15-cartesian"><span class="nav-number">3.1.15.</span> <span class="nav-text">1.15 cartesian</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-16-aggregateByKey"><span class="nav-number">3.1.16.</span> <span class="nav-text">1.16 aggregateByKey</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Action"><span class="nav-number">3.2.</span> <span class="nav-text">二、Action</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-reduce"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1 reduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-takeOrdered"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2 takeOrdered</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-countByKey"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.3 countByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-saveAsTextFile"><span class="nav-number">3.2.4.</span> <span class="nav-text">2.4 saveAsTextFile</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Tommy Hu</p>
  <div class="site-description" itemprop="description">写点东西</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tommy Hu</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'e26b1eec8da027069747',
      clientSecret: '48d14ef26caf77c5a7154a3c0cf08d66f6d21549',
      repo        : 'BlogComments',
      owner       : 'RealTommyHu',
      admin       : ['RealTommyHu'],
      id          : '86fd3d94f39bc0b481858767bd132476',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
