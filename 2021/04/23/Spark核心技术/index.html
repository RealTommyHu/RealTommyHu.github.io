<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Spark简介一、简介Apache Spark是继 MapReduce 之后，最为广泛使用的分布式计算框架。相对于 MapReduce 的批处理计算，可以带来上百倍的性能提升。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark核心技术">
<meta property="og:url" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/index.html">
<meta property="og:site_name" content="TommyHu的技术小馆">
<meta property="og:description" content="Spark简介一、简介Apache Spark是继 MapReduce 之后，最为广泛使用的分布式计算框架。相对于 MapReduce 的批处理计算，可以带来上百倍的性能提升。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/framework-of-spark.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/cluster-arch.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/4-core-comp.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-io.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-microBatches.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E4%BB%A5reduceByKey%E4%B8%BA%E4%BE%8B%E8%A7%A3%E9%87%8Ashuffle%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/RDD%E5%92%8C%E5%85%B6%E7%88%B6RDD%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E6%A0%B9%E6%8D%AE%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%E5%88%92%E5%88%86%E8%AE%A1%E7%AE%97%E9%98%B6%E6%AE%B5.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/aggregateByKey%E6%A0%B7%E4%BE%8B%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E7%B4%AF%E5%8A%A0%E5%99%A81.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E6%96%B9%E6%B3%95.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E7%B4%AF%E5%8A%A0%E5%99%A82.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/sql-hive-arch.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-dataFrame+RDDs.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-unifed.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-%E8%BF%90%E8%A1%8C%E5%AE%89%E5%85%A8.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-%E8%BF%90%E8%A1%8C%E6%97%B6%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-structure-api.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Logical-Planning.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Physical-Planning.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-sql-shell.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-mysql-%E5%88%86%E5%8C%BA%E4%B8%8A%E4%B8%8B%E9%99%90.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-%E5%88%86%E5%8C%BA.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-sql-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/sql-join.jpg">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-sql-NATURAL-JOIN.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Big-table%E2%80%93to%E2%80%93big-table.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Big-table%E2%80%93to%E2%80%93small-table.png">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/01_data_at_rest_infrastructure.png">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/02_stream_processing_infrastructure.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/01_data_at_rest_infrastructure.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/02_stream_processing_infrastructure.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-arch.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-flow.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-word-count-v1.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-dstream-ops.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-word-count-v2.png">
<meta property="og:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-word-count-v3.png">
<meta property="article:published_time" content="2021-04-23T13:43:46.000Z">
<meta property="article:modified_time" content="2021-04-28T14:42:56.689Z">
<meta property="article:author" content="Tommy Hu">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/framework-of-spark.png">

<link rel="canonical" href="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark核心技术 | TommyHu的技术小馆</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TommyHu的技术小馆</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">微信公众号：TommyHu的技术小馆</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tommy Hu">
      <meta itemprop="description" content="写点东西">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TommyHu的技术小馆">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark核心技术
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-04-23 21:43:46" itemprop="dateCreated datePublished" datetime="2021-04-23T21:43:46+08:00">2021-04-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-04-28 22:42:56" itemprop="dateModified" datetime="2021-04-28T22:42:56+08:00">2021-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Apache Spark是继 MapReduce 之后，最为广泛使用的分布式计算框架。相对于 MapReduce 的批处理计算，可以带来上百倍的性能提升。</p>
<span id="more"></span>

<h2 id="二、特点"><a href="#二、特点" class="headerlink" title="二、特点"></a>二、特点</h2><p>具有以下特点：</p>
<ul>
<li>多语言支持：Java、Scala、Python。</li>
<li>支持批处理、流处理。</li>
<li>丰富的类库支持：SQL，MLlib，GraphX，Spark Streaming。</li>
<li>丰富的部署模式：本地模式、集群模式，也支持在Hadoop、Mesos、Kubernetes上运行。</li>
<li>多数据源支持：支持访问HDFS、HBase、Hive等数百个数据源中的数据。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/framework-of-spark.png"></li>
</ul>
<h2 id="三、集群架构"><a href="#三、集群架构" class="headerlink" title="三、集群架构"></a>三、集群架构</h2><table>
<thead>
<tr>
<th>Term（术语）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。</td>
</tr>
<tr>
<td>Driver program</td>
<td>主应用程序，该进程运行应用的 main() 方法并且创建 SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>集群资源管理器（例如，Standlone Manager，Mesos，YARN）</td>
</tr>
<tr>
<td>Worker node</td>
<td>执行计算任务的工作节点</td>
</tr>
<tr>
<td>Executor</td>
<td>位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中</td>
</tr>
<tr>
<td>Task</td>
<td>被发送到 Executor 中的工作单元</td>
</tr>
</tbody></table>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/cluster-arch.png"></p>
<p>执行过程：</p>
<ol>
<li>用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor；</li>
<li>Driver 将计算程序划分为不同的执行阶段和多个 Task，之后将 Task 发送给 Executor；</li>
<li>Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。</li>
</ol>
<h2 id="四、核心组件"><a href="#四、核心组件" class="headerlink" title="四、核心组件"></a>四、核心组件</h2><p>Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/4-core-comp.png"></p>
<h3 id="1-Spark-SQL"><a href="#1-Spark-SQL" class="headerlink" title="1 Spark SQL"></a>1 Spark SQL</h3><p>Spark SQL 主要用于结构化数据的处理。其具有以下特点：</p>
<ul>
<li>允许使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li>支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC；</li>
<li>支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性，以提高查询效率。</li>
</ul>
<h3 id="2-Spark-Streaming"><a href="#2-Spark-Streaming" class="headerlink" title="2 Spark Streaming"></a>2 Spark Streaming</h3><p>Spark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-io.png"></p>
<p>Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。<br><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-microBatches.png"></p>
<h3 id="3-MLlib"><a href="#3-MLlib" class="headerlink" title="3 MLlib"></a>3 MLlib</h3><p>MLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具：</p>
<ul>
<li>常见的机器学习算法：如分类，回归，聚类和协同过滤；</li>
<li>特征化：特征提取，转换，降维和选择；</li>
<li>管道：用于构建，评估和调整 ML 管道的工具；</li>
<li>持久性：保存和加载算法，模型，管道数据；</li>
<li>实用工具：线性代数，统计，数据处理等。</li>
</ul>
<h3 id="4-Graphx"><a href="#4-Graphx" class="headerlink" title="4 Graphx"></a>4 Graphx</h3><p>GraphX 是 Spark 中用于图计算和图并行计算的新组件。在高层次上，GraphX 通过引入一个新的图抽象来扩展 RDD(一种具有附加到每个顶点和连边的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如： subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图算法和构建器，以简化图分析任务。</p>
<h1 id="弹性分布式数据集RDDs"><a href="#弹性分布式数据集RDDs" class="headerlink" title="弹性分布式数据集RDDs"></a>弹性分布式数据集RDDs</h1><h2 id="一、RDD简介"><a href="#一、RDD简介" class="headerlink" title="一、RDD简介"></a>一、RDD简介</h2><p>Resilient Distributed Datasets，只读、分区记录的集合。支持并行，可由外部数据集或者其它RDD转换。有以下特性：</p>
<ul>
<li>由一个或多个分区组成，每个分区被一个计算任务处理，可在创建RDD时指定分区个数，未指定则默认采用程序所分配到的CPU核心数。</li>
<li>有一个函数compute来计算分区。</li>
<li>RDD的每次转换都会生成一个新的依赖关系，并且RDD会保存彼此间的依赖关系，在部分分区数据丢失后可通过这种依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
<li>Key-Value型的RDD拥有分区器来决定数据存储的分区，支持哈希分区和范围分区。</li>
<li>有一个可选的优先位置列表存储每个分区的优先位置，对于一个HDFS文件来说，这个列表保存的就是每个分区所在的块的位置，按“移动数据不如移动计算”的理念，在任务调度时会尽可能将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<p>RDD[T] 抽象类的部分相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 由子类实现以计算给定分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有分区</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有依赖关系</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取优先位置列表</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区器 由子类重写以指定它们的分区方式</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure>

<h2 id="二、创建RDD"><a href="#二、创建RDD" class="headerlink" title="二、创建RDD"></a>二、创建RDD</h2><p>RDD有两种创建方式，分别介绍如下：</p>
<h3 id="2-1-由现有集合创建"><a href="#2-1-由现有集合创建" class="headerlink" title="2.1 由现有集合创建"></a>2.1 由现有集合创建</h3><p>这里使用 spark-shell 进行测试，启动命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master <span class="built_in">local</span>[4]</span><br></pre></td></tr></table></figure>

<p>启动 spark-shell 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Spark shell&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>

<p>由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data) </span><br><span class="line"><span class="comment">// 查看分区数</span></span><br><span class="line">dataRDD.getNumPartitions <span class="comment">// 输出4</span></span><br><span class="line"><span class="comment">// 明确指定分区数</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data,<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 查看分区数</span></span><br><span class="line">dataRDD.getNumPartitions <span class="comment">// 输出2</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-应用外部存储系统中的数据集"><a href="#2-2-应用外部存储系统中的数据集" class="headerlink" title="2.2 应用外部存储系统中的数据集"></a>2.2 应用外部存储系统中的数据集</h3><p>引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">&quot;/usr/file/emp.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 获取第一行文本</span></span><br><span class="line">fileRDD.take(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>使用外部存储系统时需要注意以下两点：</p>
<ul>
<li>如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同；</li>
<li>支持目录路径，支持压缩文件，支持使用通配符。</li>
</ul>
<h3 id="2-3-textFile-amp-wholeTextFiles"><a href="#2-3-textFile-amp-wholeTextFiles" class="headerlink" title="2.3 textFile &amp; wholeTextFiles"></a>2.3 textFile &amp; wholeTextFiles</h3><p>两者都可以用来读取外部文件，但是返回格式是不同的：</p>
<ul>
<li>textFile：其返回格式是 RDD[String] ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；</li>
<li>wholeTextFiles：其返回格式是 RDD[(String, String)]，元组中第一个参数是文件路径，第二个参数是文件内容；</li>
<li>两者都提供第二个参数来控制最小分区数；</li>
<li>从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;...&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wholeTextFiles</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)]=&#123;..&#125;</span><br></pre></td></tr></table></figure>

<h2 id="三、操作RDD"><a href="#三、操作RDD" class="headerlink" title="三、操作RDD"></a>三、操作RDD</h2><p>RDD 支持两种类型的操作：transformations（转换，从现有数据集创建新数据集）和 actions（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 action 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">// map 是一个 transformations 操作，而 foreach 是一个 actions 操作</span></span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出： 10 20 30</span></span><br></pre></td></tr></table></figure>

<h2 id="四、缓存RDD"><a href="#四、缓存RDD" class="headerlink" title="四、缓存RDD"></a>四、缓存RDD</h2><h3 id="4-1-缓存级别"><a href="#4-1-缓存级别" class="headerlink" title="4.1 缓存级别"></a>4.1 缓存级别</h3><p>Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。</p>
<p>Spark 支持多种缓存级别 ：</p>
<table>
<thead>
<tr>
<th>Storage Level 存储级别</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>类似于 MEMORY_ONLY_SER，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>只在磁盘上缓存 RDD</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc</td>
<td>与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。</td>
</tr>
<tr>
<td>OFF_HEAP</td>
<td>与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。这需要启用堆外内存。</td>
</tr>
</tbody></table>
<blockquote>
<p>启动堆外内存需要配置两个参数：</p>
<ul>
<li>spark.memory.offHeap.enabled ：是否开启堆外内存，默认值为 false，需要设置为 true；</li>
<li>spark.memory.offHeap.size : 堆外内存空间的大小，默认值为 0，需要设置为正值。</li>
</ul>
</blockquote>
<h3 id="4-2-使用缓存"><a href="#4-2-使用缓存" class="headerlink" title="4.2 使用缓存"></a>4.2 使用缓存</h3><p>缓存数据的方法有两个：persist 和 cache 。cache 内部调用的也是 persist，它是 persist 的特殊化形式，等价于 persist(StorageLevel.MEMORY_ONLY)。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所有存储级别均定义在 StorageLevel 对象中</span></span><br><span class="line">fileRDD.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">fileRDD.cache()</span><br></pre></td></tr></table></figure>
<p>一个已分配存储级别的RDD不能直接更改存储级别。</p>
<h3 id="4-3-移除缓存"><a href="#4-3-移除缓存" class="headerlink" title="4.3 移除缓存"></a>4.3 移除缓存</h3><ul>
<li>Spark 自动监视每个节点上的缓存使用，按照最近最少使用（LRU）的规则删除旧数据分区。</li>
<li>也可以使用 RDD.unpersist() 方法进行手动删除。</li>
</ul>
<h2 id="五、理解shuffle"><a href="#五、理解shuffle" class="headerlink" title="五、理解shuffle"></a>五、理解shuffle</h2><h3 id="5-1-shuffle介绍"><a href="#5-1-shuffle介绍" class="headerlink" title="5.1 shuffle介绍"></a>5.1 shuffle介绍</h3><p>Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 reduceByKey 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 Shuffle。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E4%BB%A5reduceByKey%E4%B8%BA%E4%BE%8B%E8%A7%A3%E9%87%8Ashuffle%E8%BF%87%E7%A8%8B.png" alt="以reduceByKey为例解释shuffle过程"></p>
<h3 id="5-2-Shuffle的影响"><a href="#5-2-Shuffle的影响" class="headerlink" title="5.2 Shuffle的影响"></a>5.2 Shuffle的影响</h3><p>Shuffle 是项昂贵的操作，跨节点操作数据，涉及磁盘 I/O，网络 I/O，和数据序列化。消耗大量的堆内存，用堆内存来临时存储需要网络传输的数据。在磁盘上生成大量中间文件，避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 spark.local.dir 参数来指定这些临时文件的存储目录。</p>
<h3 id="5-3-导致Shuffle的操作"><a href="#5-3-导致Shuffle的操作" class="headerlink" title="5.3 导致Shuffle的操作"></a>5.3 导致Shuffle的操作</h3><p>Shuffle 操作对性能的影响比较大，所以需要特别注意使用，导致Shuffle的操作：</p>
<ul>
<li>涉及到重新分区操作： 如 repartition 和 coalesce；</li>
<li>所有涉及到 ByKey 的操作：如 groupByKey 和 reduceByKey，但 countByKey 除外；</li>
<li>联结操作：如 cogroup 和 join。</li>
</ul>
<h2 id="五、宽依赖和窄依赖"><a href="#五、宽依赖和窄依赖" class="headerlink" title="五、宽依赖和窄依赖"></a>五、宽依赖和窄依赖</h2><p>RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：</p>
<ul>
<li>窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；</li>
<li>宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。</li>
</ul>
<p>如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/RDD%E5%92%8C%E5%85%B6%E7%88%B6RDD%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png" alt="RDD和其父RDD之间的依赖关系"></p>
<p>区分这两种依赖是非常有用的：</p>
<ul>
<li>窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。</li>
<li>而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。</li>
<li>窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；</li>
<li>而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。</li>
</ul>
<h2 id="六、DAG的生成"><a href="#六、DAG的生成" class="headerlink" title="六、DAG的生成"></a>六、DAG的生成</h2><p>RDD(s)及其之间的依赖关系组成了 DAG(有向无环图)，通过依赖关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。Spark根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)：</p>
<ul>
<li>窄依赖，分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；</li>
<li>宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。</li>
</ul>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/%E6%A0%B9%E6%8D%AE%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%E5%88%92%E5%88%86%E8%AE%A1%E7%AE%97%E9%98%B6%E6%AE%B5.png" alt="根据宽窄依赖划分计算阶段"></p>
<h1 id="Transformation-和-Action-常用算子"><a href="#Transformation-和-Action-常用算子" class="headerlink" title="Transformation 和 Action 常用算子"></a>Transformation 和 Action 常用算子</h1><h2 id="一、Transformation"><a href="#一、Transformation" class="headerlink" title="一、Transformation"></a>一、Transformation</h2><p>spark 常用的 Transformation 算子如下表：</p>
<table>
<thead>
<tr>
<th>Transformation 算子</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素运用 <em>func</em> 函数，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素使用<em>func</em> 函数进行过滤，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 Seq ）。</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为  Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; ，其中 T 是 RDD 的类型，即 RDD[T]</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>与 mapPartitions 类似，但 <em>func</em> 类型为 (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; ，其中第一个参数为分区索引</td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）；</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td>合并两个 RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td>求两个 RDD 的交集</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numTasks</em>]))</td>
<td>去重</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>
<td>按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable&lt;V&gt;) <br/><strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 性能会更好<br><strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td>按照 key 值进行分组，并对分组后的数据执行归约操作。</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>
<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>
<td>按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples 的 dataset。</td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td>将 RDD 中的分区数减少为 numPartitions。</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。</td>
</tr>
</tbody></table>
<p>下面分别给出这些算子的基本使用示例：</p>
<h3 id="1-1-map"><a href="#1-1-map" class="headerlink" title="1.1 map"></a>1.1 map</h3><p>对原 RDD 中每个元素运用 func 函数，并生成新的 RDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果： 10 20 30 （这里为了节省篇幅去掉了换行,后文亦同）</span></span><br></pre></td></tr></table></figure>

<h3 id="1-2-filter"><a href="#1-2-filter" class="headerlink" title="1.2 filter"></a>1.2 filter</h3><p>对原 RDD 中每个元素使用func 函数进行过滤，并生成新的 RDD 。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">21</span>)</span><br><span class="line">sc.parallelize(list).filter(_ &gt;= <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 10 12 21</span></span><br></pre></td></tr></table></figure>

<h3 id="1-3-flatMap"><a href="#1-3-flatMap" class="headerlink" title="1.3 flatMap"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的 item 会被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 <code>Seq</code>）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>), <span class="type">List</span>(), <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">sc.parallelize(list).flatMap(_.toList).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果 ： 10 20 30 40 50</span></span><br></pre></td></tr></table></figure>

<p>flatMap 这个算子在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为 1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = <span class="type">List</span>(<span class="string">&quot;spark flume spark&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;hadoop flume hive&quot;</span>)</span><br><span class="line">sc.parallelize(lines).flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).</span><br><span class="line">map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line">(spark,<span class="number">2</span>)</span><br><span class="line">(hive,<span class="number">1</span>)</span><br><span class="line">(hadoop,<span class="number">1</span>)</span><br><span class="line">(flume,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-4-mapPartitions"><a href="#1-4-mapPartitions" class="headerlink" title="1.4 mapPartitions"></a>1.4 mapPartitions</h3><p>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 <code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中 T 是 RDD 的类型)，即输入和输出都必须是可迭代类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitions(iterator =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出结果</span></span><br><span class="line"><span class="number">100</span> <span class="number">200</span> <span class="number">300</span> <span class="number">400</span> <span class="number">500</span> <span class="number">600</span></span><br></pre></td></tr></table></figure>

<h3 id="1-5-mapPartitionsWithIndex"><a href="#1-5-mapPartitionsWithIndex" class="headerlink" title="1.5 mapPartitionsWithIndex"></a>1.5 mapPartitionsWithIndex</h3><p>  与 mapPartitions 类似，但 <em>func</em> 类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(index + <span class="string">&quot;分区:&quot;</span> + iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">0</span> 分区:<span class="number">100</span></span><br><span class="line"><span class="number">0</span> 分区:<span class="number">200</span></span><br><span class="line"><span class="number">1</span> 分区:<span class="number">300</span></span><br><span class="line"><span class="number">1</span> 分区:<span class="number">400</span></span><br><span class="line"><span class="number">2</span> 分区:<span class="number">500</span></span><br><span class="line"><span class="number">2</span> 分区:<span class="number">600</span></span><br></pre></td></tr></table></figure>

<h3 id="1-6-sample"><a href="#1-6-sample" class="headerlink" title="1.6 sample"></a>1.6 sample</h3><p>  数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list).sample(withReplacement = <span class="literal">false</span>, fraction = <span class="number">0.5</span>).foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="1-7-union"><a href="#1-7-union" class="headerlink" title="1.7 union"></a>1.7 union</h3><p>合并两个 RDD：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)</span><br><span class="line"><span class="comment">// 输出: 1 2 3 4 5 6</span></span><br></pre></td></tr></table></figure>

<h3 id="1-8-intersection"><a href="#1-8-intersection" class="headerlink" title="1.8 intersection"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)</span><br><span class="line"><span class="comment">// 输出:  4 5</span></span><br></pre></td></tr></table></figure>

<h3 id="1-9-distinct"><a href="#1-9-distinct" class="headerlink" title="1.9 distinct"></a>1.9 distinct</h3><p>去重：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">sc.parallelize(list).distinct().foreach(println)</span><br><span class="line"><span class="comment">// 输出: 4 1 2</span></span><br></pre></td></tr></table></figure>

<h3 id="1-10-groupByKey"><a href="#1-10-groupByKey" class="headerlink" title="1.10 groupByKey"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">(spark,<span class="type">List</span>(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">(hadoop,<span class="type">List</span>(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">(storm,<span class="type">List</span>(<span class="number">6</span>))</span><br></pre></td></tr></table></figure>

<h3 id="1-11-reduceByKey"><a href="#1-11-reduceByKey" class="headerlink" title="1.11 reduceByKey"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).reduceByKey(_ + _).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">(spark,<span class="number">8</span>)</span><br><span class="line">(hadoop,<span class="number">4</span>)</span><br><span class="line">(storm,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-12-sortBy-amp-sortByKey"><a href="#1-12-sortBy-amp-sortByKey" class="headerlink" title="1.12 sortBy &amp; sortByKey"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">100</span>, <span class="string">&quot;hadoop&quot;</span>), (<span class="number">90</span>, <span class="string">&quot;spark&quot;</span>), (<span class="number">120</span>, <span class="string">&quot;storm&quot;</span>))</span><br><span class="line">sc.parallelize(list01).sortByKey(ascending = <span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">120</span>,storm)</span><br><span class="line">(<span class="number">90</span>,spark)</span><br><span class="line">(<span class="number">100</span>,hadoop)</span><br></pre></td></tr></table></figure>

<p>按照指定元素进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>,<span class="number">100</span>), (<span class="string">&quot;spark&quot;</span>,<span class="number">90</span>), (<span class="string">&quot;storm&quot;</span>,<span class="number">120</span>))</span><br><span class="line">sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=<span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(storm,<span class="number">120</span>)</span><br><span class="line">(hadoop,<span class="number">100</span>)</span><br><span class="line">(spark,<span class="number">90</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-13-join"><a href="#1-13-join" class="headerlink" title="1.13 join"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的 Dataset 上调用时，返回一个 (K, (V, W)) 的 Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;student01&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;student02&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;student03&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;teacher01&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;teacher02&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;teacher03&quot;</span>))</span><br><span class="line">sc.parallelize(list01).join(sc.parallelize(list02)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">1</span>,(student01,teacher01))</span><br><span class="line">(<span class="number">3</span>,(student03,teacher03))</span><br><span class="line">(<span class="number">2</span>,(student02,teacher02))</span><br></pre></td></tr></table></figure>

<h3 id="1-14-cogroup"><a href="#1-14-cogroup" class="headerlink" title="1.14 cogroup"></a>1.14 cogroup</h3><blockquote>
<p><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</p>
</blockquote>
<p>在一个 (K, V) 对的 Dataset 上调用时，返回多个类型为 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) 的元组所组成的 Dataset。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>),(<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;e&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;A&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;B&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;E&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list03 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;[ab]&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;[bB]&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;eE&quot;</span>),(<span class="number">3</span>, <span class="string">&quot;eE&quot;</span>))</span><br><span class="line">sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 同一个 RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组</span></span><br><span class="line">(<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a, a),<span class="type">CompactBuffer</span>(<span class="type">A</span>),<span class="type">CompactBuffer</span>([ab])))</span><br><span class="line">(<span class="number">3</span>,(<span class="type">CompactBuffer</span>(e),<span class="type">CompactBuffer</span>(<span class="type">E</span>),<span class="type">CompactBuffer</span>(eE, eE)))</span><br><span class="line">(<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="type">B</span>),<span class="type">CompactBuffer</span>([bB])))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="1-15-cartesian"><a href="#1-15-cartesian" class="headerlink" title="1.15 cartesian"></a>1.15 cartesian</h3><p>计算笛卡尔积：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出笛卡尔积</span></span><br><span class="line">(<span class="type">A</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-16-aggregateByKey"><a href="#1-16-aggregateByKey" class="headerlink" title="1.16 aggregateByKey"></a>1.16 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 <code>groupByKey</code> 类似，reduce 任务的数量可通过第二个参数 <code>numPartitions</code> 进行配置。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为了清晰，以下所有参数均使用具名传参</span></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">8</span>))</span><br><span class="line">sc.parallelize(list,numSlices = <span class="number">2</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">      seqOp = math.max(_, _),</span><br><span class="line">      combOp = _ + _</span><br><span class="line">    ).collect.foreach(println)</span><br><span class="line"><span class="comment">//输出结果：</span></span><br><span class="line">(hadoop,<span class="number">3</span>)</span><br><span class="line">(storm,<span class="number">8</span>)</span><br><span class="line">(spark,<span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<p>这里使用了 <code>numSlices = 2</code> 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/aggregateByKey%E6%A0%B7%E4%BE%8B%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.png" alt="aggregateByKey样例执行流程"></p>
<p>基于同样的执行流程，如果 <code>numSlices = 1</code>，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">(hadoop,3)</span></span><br><span class="line"><span class="attr">(storm,8)</span></span><br><span class="line"><span class="attr">(spark,4)</span></span><br></pre></td></tr></table></figure>

<p>同样的，如果每个单词对一个分区，即 <code>numSlices = 6</code>，此时相当于求和操作，执行结果为：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">(hadoop,5)</span></span><br><span class="line"><span class="attr">(storm,14)</span></span><br><span class="line"><span class="attr">(spark,7)</span></span><br></pre></td></tr></table></figure>

<p><code>aggregateByKey(zeroValue = 0,numPartitions = 3)</code> 的第二个参数 <code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 <code>getNumPartitions</code> 方法获取分区数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(list,numSlices = <span class="number">6</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">  seqOp = math.max(_, _),</span><br><span class="line">  combOp = _ + _</span><br><span class="line">).getNumPartitions</span><br></pre></td></tr></table></figure>

<h2 id="二、Action"><a href="#二、Action" class="headerlink" title="二、Action"></a>二、Action</h2><p>Spark 常用的 Action 算子如下：</p>
<table>
<thead>
<tr>
<th>Action（动作）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>使用函数<em>func</em>执行归约操作</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回 dataset 中元素的个数。</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回 dataset 中的第一个元素，等价于 take(1)。</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>对一个 dataset 进行随机抽样</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>使用 Java 序列化后存储，可以使用 <code>SparkContext.objectFile()</code> 进行加载。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>计算每个键出现的次数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>遍历 RDD 中每个元素，并对其执行<em>fun</em>函数</td>
</tr>
</tbody></table>
<h3 id="2-1-reduce"><a href="#2-1-reduce" class="headerlink" title="2.1 reduce"></a>2.1 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">sc.parallelize(list).reduce((x, y) =&gt; x + y)</span><br><span class="line">sc.parallelize(list).reduce(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出 15</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-takeOrdered"><a href="#2-2-takeOrdered" class="headerlink" title="2.2 takeOrdered"></a>2.2 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  .........</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义规则排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[(<span class="type">Int</span>, <span class="type">String</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: (<span class="type">Int</span>, <span class="type">String</span>), y: (<span class="type">Int</span>, <span class="type">String</span>)): <span class="type">Int</span></span><br><span class="line">    = <span class="keyword">if</span> (x._2.length &gt; y._2.length) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;hadoop&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;storm&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;azkaban&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;hive&quot;</span>))</span><br><span class="line"><span class="comment">//  引入隐式默认值</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> implicitOrdering = <span class="keyword">new</span> <span class="type">CustomOrdering</span></span><br><span class="line">sc.parallelize(list).takeOrdered(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Array((1,hive), (1,storm), (1,hadoop), (1,azkaban)</span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-countByKey"><a href="#2-3-countByKey" class="headerlink" title="2.3 countByKey"></a>2.3 countByKey</h3><p>计算每个键出现的次数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;azkaban&quot;</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Map(hadoop -&gt; 2, storm -&gt; 2, azkaban -&gt; 1)</span></span><br></pre></td></tr></table></figure>

<h3 id="2-4-saveAsTextFile"><a href="#2-4-saveAsTextFile" class="headerlink" title="2.4 saveAsTextFile"></a>2.4 saveAsTextFile</h3><p>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;storm&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;azkaban&quot;</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).saveAsTextFile(<span class="string">&quot;/usr/file/temp&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Spark累加器与广播变量"><a href="#Spark累加器与广播变量" class="headerlink" title="Spark累加器与广播变量"></a>Spark累加器与广播变量</h1><h2 id="一、简介-1"><a href="#一、简介-1" class="headerlink" title="一、简介"></a>一、简介</h2><p>在 Spark 中，提供了两种类型的共享变量：累加器 (accumulator) 与广播变量 (broadcast variable)：</p>
<ul>
<li><strong>累加器</strong>：用来对信息进行聚合，主要用于累计计数等场景；</li>
<li><strong>广播变量</strong>：主要用于在节点间高效分发大对象。</li>
</ul>
<h2 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h2><p>这里先看一个具体的场景，对于正常的累计求和，如果在集群模式中使用下面的代码进行计算，会发现执行结果并非预期：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">sc.parallelize(data).foreach(x =&gt; counter += x)</span><br><span class="line">println(counter)</span><br></pre></td></tr></table></figure>

<p>counter 最后的结果是 0，导致这个问题的主要原因是闭包。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E7%B4%AF%E5%8A%A0%E5%99%A81.png" alt="spark累加器1.png"></p>
<h3 id="2-1-理解闭包"><a href="#2-1-理解闭包" class="headerlink" title="2.1 理解闭包"></a>2.1 理解闭包</h3><p><strong>1. Scala 中闭包的概念</strong></p>
<p>这里先介绍一下 Scala 中关于闭包的概念：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var more &#x3D; 10</span><br><span class="line">val addMore &#x3D; (x: Int) &#x3D;&gt; x + more</span><br></pre></td></tr></table></figure>

<p>如上函数 <code>addMore</code> 中有两个变量 x 和 more:</p>
<ul>
<li><strong>x</strong> : 是一个绑定变量 (bound variable)，因为其是该函数的入参，在函数的上下文中有明确的定义；</li>
<li><strong>more</strong> : 是一个自由变量 (free variable)，因为函数字面量本生并没有给 more 赋予任何含义。</li>
</ul>
<p>按照定义：在创建函数时，如果需要捕获自由变量，那么包含指向被捕获变量的引用的函数就被称为闭包函数。</p>
<p><strong>2. Spark 中的闭包</strong></p>
<p>在实际计算时，Spark 会将对 RDD 操作分解为 Task，Task 运行在 Worker Node 上。在执行之前，Spark 会对任务进行闭包，如果闭包内涉及到自由变量，则程序会进行拷贝，并将副本变量放在闭包中，之后闭包被序列化并发送给每个执行者。因此，当在 foreach 函数中引用 <code>counter</code> 时，它将不再是 Driver 节点上的 <code>counter</code>，而是闭包中的副本 <code>counter</code>，默认情况下，副本 <code>counter</code> 更新后的值不会回传到 Driver，所以 <code>counter</code> 的最终值仍然为零。</p>
<p>需要注意的是：在 Local 模式下，有可能执行 <code>foreach</code> 的 Worker Node 与 Diver 处在相同的 JVM，并引用相同的原始 <code>counter</code>，这时候更新可能是正确的，但是在集群模式下一定不正确。所以在遇到此类问题时应优先使用累加器。</p>
<p>累加器的原理实际上很简单：就是将每个副本变量的最终值传回 Driver，由 Driver 聚合后得到最终值，并更新原始变量。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F.png" alt="spark集群模式.png"></p>
<h3 id="2-2-使用累加器"><a href="#2-2-使用累加器" class="headerlink" title="2.2 使用累加器"></a>2.2 使用累加器</h3><p><code>SparkContext</code> 中定义了所有创建累加器的方法，需要注意的是：被中横线划掉的累加器方法在 Spark 2.0.0 之后被标识为废弃。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E6%96%B9%E6%B3%95.png" alt="spark累加器方法.png"></p>
<p>使用示例和执行结果分别如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">// 定义累加器</span></span><br><span class="line"><span class="keyword">val</span> accum = sc.longAccumulator(<span class="string">&quot;My Accumulator&quot;</span>)</span><br><span class="line">sc.parallelize(data).foreach(x =&gt; accum.add(x))</span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark%E7%B4%AF%E5%8A%A0%E5%99%A82.png" alt="spark累加器2.png"></p>
<h2 id="三、广播变量"><a href="#三、广播变量" class="headerlink" title="三、广播变量"></a>三、广播变量</h2><p>在上面介绍中闭包的过程中我们说道每个 Task 任务的闭包都会持有自由变量的副本，如果变量很大且 Task 任务很多的情况下，这必然会对网络 IO 造成压力，为了解决这个情况，Spark 提供了广播变量。</p>
<p>广播变量的做法很简单：就是不把副本变量分发到每个 Task 中，而是将其分发到每个 Executor，Executor 中的所有 Task 共享一个副本变量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 把一个数组定义为一个广播变量</span></span><br><span class="line"><span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">// 之后用到该数组时应优先使用广播变量，而不是原值</span></span><br><span class="line">sc.parallelize(broadcastVar.value).map(_ * <span class="number">10</span>).collect()</span><br></pre></td></tr></table></figure>

<h1 id="SparkSQL-Dataset和DataFrame简介"><a href="#SparkSQL-Dataset和DataFrame简介" class="headerlink" title="SparkSQL_Dataset和DataFrame简介"></a>SparkSQL_Dataset和DataFrame简介</h1><h2 id="一、Spark-SQL简介"><a href="#一、Spark-SQL简介" class="headerlink" title="一、Spark SQL简介"></a>一、Spark SQL简介</h2><p>Spark SQL 是 Spark 中的一个子模块，主要用于操作结构化数据。它具有以下特点：</p>
<ul>
<li> 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li> 支持多种开发语言；</li>
<li>支持多达上百种的外部数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC 等；</li>
<li>支持 HiveQL 语法以及 Hive SerDes 和 UDF，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性；</li>
<li>支持扩展并能保证容错。</li>
</ul>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/sql-hive-arch.png" alt="sql-hive-arch.png"></p>
<h2 id="二、DataFrame-amp-DataSet"><a href="#二、DataFrame-amp-DataSet" class="headerlink" title="二、DataFrame &amp; DataSet"></a>二、DataFrame &amp; DataSet</h2><h3 id="2-1-DataFrame"><a href="#2-1-DataFrame" class="headerlink" title="2.1 DataFrame"></a>2.1 DataFrame</h3><p>为了支持结构化数据的处理，Spark SQL 提供了新的数据结构 DataFrame。DataFrame 是一个由具名列组成的数据集。它在概念上等同于关系数据库中的表或 R/Python 语言中的 <code>data frame</code>。 由于 Spark SQL 支持多种语言的开发，所以每种语言都定义了 <code>DataFrame</code> 的抽象，主要如下：</p>
<table>
<thead>
<tr>
<th>语言</th>
<th>主要抽象</th>
</tr>
</thead>
<tbody><tr>
<td>Scala</td>
<td>Dataset[T] &amp; DataFrame (Dataset[Row] 的别名)</td>
</tr>
<tr>
<td>Java</td>
<td>Dataset[T]</td>
</tr>
<tr>
<td>Python</td>
<td>DataFrame</td>
</tr>
<tr>
<td>R</td>
<td>DataFrame</td>
</tr>
</tbody></table>
<h3 id="2-2-DataFrame-对比-RDDs"><a href="#2-2-DataFrame-对比-RDDs" class="headerlink" title="2.2 DataFrame 对比 RDDs"></a>2.2 DataFrame 对比 RDDs</h3><p>DataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据，它们内部的数据结构如下：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-dataFrame+RDDs.png" alt="spark-dataFrame+RDDs.png"></p>
<p>DataFrame 内部的有明确 Scheme 结构，即列名、列字段类型都是已知的，这带来的好处是可以减少数据读取以及更好地优化执行计划，从而保证查询效率。</p>
<p><strong>DataFrame 和 RDDs 应该如何选择？</strong></p>
<ul>
<li>如果你想使用函数式编程而不是 DataFrame API，则使用 RDDs；</li>
<li>如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs，</li>
<li>如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。</li>
</ul>
<h3 id="2-3-DataSet"><a href="#2-3-DataSet" class="headerlink" title="2.3 DataSet"></a>2.3 DataSet</h3><p>Dataset 也是分布式的数据集合，在 Spark 1.6 版本被引入，它集成了 RDD 和 DataFrame 的优点，具备强类型的特点，同时支持 Lambda 函数，但只能在 Scala 和 Java 语言中使用。在 Spark 2.0 后，为了方便开发者，Spark 将 DataFrame 和 Dataset 的 API 融合到一起，提供了结构化的 API(Structured API)，即用户可以通过一套标准的 API 就能完成对两者的操作。</p>
<blockquote>
<p>这里注意一下：DataFrame 被标记为 Untyped API，而 DataSet 被标记为 Typed API，后文会对两者做出解释。</p>
</blockquote>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-unifed.png" alt="spark-unifed.png"></p>
<h3 id="2-4-静态类型与运行时类型安全"><a href="#2-4-静态类型与运行时类型安全" class="headerlink" title="2.4 静态类型与运行时类型安全"></a>2.4 静态类型与运行时类型安全</h3><p>静态类型 (Static-typing) 与运行时类型安全 (runtime type-safety) 主要表现如下:</p>
<p>在实际使用中，如果你用的是 Spark SQL 的查询语句，则直到运行时你才会发现有语法错误，而如果你用的是 DataFrame 和 Dataset，则在编译时就可以发现错误 (这节省了开发时间和整体代价)。DataFrame 和 Dataset 主要区别在于：</p>
<p>在 DataFrame 中，当你调用了 API 之外的函数，编译器就会报错，但如果你使用了一个不存在的字段名字，编译器依然无法发现。而 Dataset 的 API 都是用 Lambda 函数和 JVM 类型对象表示的，所有不匹配的类型参数在编译时就会被发现。</p>
<p>以上这些最终都被解释成关于类型安全图谱，对应开发中的语法和分析错误。在图谱中，Dataset 最严格，但对于开发者来说效率最高。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-%E8%BF%90%E8%A1%8C%E5%AE%89%E5%85%A8.png" alt="spark-运行安全.png"></p>
<p>上面的描述可能并没有那么直观，下面的给出一个 IDEA 中代码编译的示例：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-%E8%BF%90%E8%A1%8C%E6%97%B6%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8.png" alt="spark-运行时类型安全.png"></p>
<p>这里一个可能的疑惑是 DataFrame 明明是有确定的 Scheme 结构 (即列名、列字段类型都是已知的)，但是为什么还是无法对列名进行推断和错误判断，这是因为 DataFrame 是 Untyped 的。</p>
<h3 id="2-5-Untyped-amp-Typed"><a href="#2-5-Untyped-amp-Typed" class="headerlink" title="2.5 Untyped &amp; Typed"></a>2.5 Untyped &amp; Typed</h3><p>在上面我们介绍过 DataFrame API 被标记为 <code>Untyped API</code>，而 DataSet API 被标记为 <code>Typed API</code>。DataFrame 的 <code>Untyped</code> 是相对于语言或 API 层面而言，它确实有明确的 Scheme 结构，即列名，列类型都是确定的，但这些信息完全由 Spark 来维护，Spark 只会在运行时检查这些类型和指定类型是否一致。这也就是为什么在 Spark 2.0 之后，官方推荐把 DataFrame 看做是 <code>DatSet[Row]</code>，Row 是 Spark 中定义的一个 <code>trait</code>，其子类中封装了列字段的信息。</p>
<p>相对而言，DataSet 是 <code>Typed</code> 的，即强类型。如下面代码，DataSet 的类型由 Case Class(Scala) 或者 Java Bean(Java) 来明确指定的，在这里即每一行数据代表一个 <code>Person</code>，这些信息由 JVM 来保证正确性，所以字段名错误和类型错误在编译的时候就会被 IDE 所发现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">dataSet</span></span>: <span class="type">Dataset</span>[<span class="type">Person</span>] = spark.read.json(<span class="string">&quot;people.json&quot;</span>).as[<span class="type">Person</span>]</span><br></pre></td></tr></table></figure>

<h2 id="三、DataFrame-amp-DataSet-amp-RDDs-总结"><a href="#三、DataFrame-amp-DataSet-amp-RDDs-总结" class="headerlink" title="三、DataFrame &amp; DataSet  &amp; RDDs 总结"></a>三、DataFrame &amp; DataSet  &amp; RDDs 总结</h2><p>这里对三者做一下简单的总结：</p>
<ul>
<li>RDDs 适合非结构化数据的处理，而 DataFrame &amp; DataSet 更适合结构化数据和半结构化的处理；</li>
<li>DataFrame &amp; DataSet 可以通过统一的 Structured API 进行访问，而 RDDs 则更适合函数式编程的场景；</li>
<li>相比于 DataFrame 而言，DataSet 是强类型的 (Typed)，有着更为严格的静态类型检查；</li>
<li>DataSets、DataFrames、SQL 的底层都依赖了 RDDs API，并对外提供结构化的访问接口。</li>
</ul>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-structure-api.png" alt="spark-structure-api.png"></p>
<h2 id="四、Spark-SQL的运行原理"><a href="#四、Spark-SQL的运行原理" class="headerlink" title="四、Spark SQL的运行原理"></a>四、Spark SQL的运行原理</h2><p>DataFrame、DataSet 和 Spark SQL 的实际执行流程都是相同的：</p>
<ol>
<li>进行 DataFrame/Dataset/SQL 编程；</li>
<li>如果是有效的代码，即代码没有编译错误，Spark 会将其转换为一个逻辑计划；</li>
<li>Spark 将此逻辑计划转换为物理计划，同时进行代码优化；</li>
<li>Spark 然后在集群上执行这个物理计划 (基于 RDD 操作) 。</li>
</ol>
<h3 id="4-1-逻辑计划-Logical-Plan"><a href="#4-1-逻辑计划-Logical-Plan" class="headerlink" title="4.1 逻辑计划(Logical Plan)"></a>4.1 逻辑计划(Logical Plan)</h3><p>执行的第一个阶段是将用户代码转换成一个逻辑计划。它首先将用户代码转换成 <code>unresolved logical plan</code>(未解决的逻辑计划)，之所以这个计划是未解决的，是因为尽管您的代码在语法上是正确的，但是它引用的表或列可能不存在。 Spark 使用 <code>analyzer</code>(分析器) 基于 <code>catalog</code>(存储的所有表和 <code>DataFrames</code> 的信息) 进行解析。解析失败则拒绝执行，解析成功则将结果传给 <code>Catalyst</code> 优化器 (<code>Catalyst Optimizer</code>)，优化器是一组规则的集合，用于优化逻辑计划，通过谓词下推等方式进行优化，最终输出优化后的逻辑执行计划。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Logical-Planning.png" alt="spark-Logical-Planning.png"></p>
<h3 id="4-2-物理计划-Physical-Plan"><a href="#4-2-物理计划-Physical-Plan" class="headerlink" title="4.2 物理计划(Physical Plan)"></a>4.2 物理计划(Physical Plan)</h3><p>得到优化后的逻辑计划后，Spark 就开始了物理计划过程。 它通过生成不同的物理执行策略，并通过成本模型来比较它们，从而选择一个最优的物理计划在集群上面执行的。物理规划的输出结果是一系列的 RDDs 和转换关系 (transformations)。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Physical-Planning.png" alt="spark-Physical-Planning.png"></p>
<h3 id="4-3-执行"><a href="#4-3-执行" class="headerlink" title="4.3 执行"></a>4.3 执行</h3><p>在选择一个物理计划后，Spark 运行其 RDDs 代码，并在运行时执行进一步的优化，生成本地 Java 字节码，最后将运行结果返回给用户。 </p>
<h1 id="Structured-API基本使用"><a href="#Structured-API基本使用" class="headerlink" title="Structured API基本使用"></a>Structured API基本使用</h1><h2 id="一、创建DataFrame和Dataset"><a href="#一、创建DataFrame和Dataset" class="headerlink" title="一、创建DataFrame和Dataset"></a>一、创建DataFrame和Dataset</h2><h3 id="1-1-创建DataFrame"><a href="#1-1-创建DataFrame" class="headerlink" title="1.1 创建DataFrame"></a>1.1 创建DataFrame</h3><p>Spark 中所有功能的入口点是 <code>SparkSession</code>，可以使用 <code>SparkSession.builder()</code> 创建。创建后应用程序就可以从现有 RDD，Hive 表或 Spark 数据源创建 DataFrame。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;Spark-SQL&quot;</span>).master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 建议在进行 spark SQL 编程前导入下面的隐式转换，因为 DataFrames 和 dataSets 中很多操作都依赖了隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<p>可以使用 <code>spark-shell</code> 进行测试，需要注意的是 <code>spark-shell</code> 启动后会自动创建一个名为 <code>spark</code> 的 <code>SparkSession</code>，在命令行中可以直接引用即可：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-sql-shell.png" alt="spark-sql-shell.png"></p>
<br/>

<h3 id="1-2-创建Dataset"><a href="#1-2-创建Dataset" class="headerlink" title="1.2 创建Dataset"></a>1.2 创建Dataset</h3><p>Spark 支持由内部数据集和外部数据集来创建 DataSet，其创建方式分别如下：</p>
<h4 id="1-由外部数据集创建"><a href="#1-由外部数据集创建" class="headerlink" title="1. 由外部数据集创建"></a>1. 由外部数据集创建</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.需要导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建 case class,等价于 Java Bean</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">ename: <span class="type">String</span>, comm: <span class="type">Double</span>, deptno: <span class="type">Long</span>, empno: <span class="type">Long</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">               hiredate: <span class="type">String</span>, job: <span class="type">String</span>, mgr: <span class="type">Long</span>, sal: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// 3.由外部数据集创建 Datasets</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= spark.read.json(<span class="string">&quot;/usr/file/emp.json&quot;</span>).as[<span class="type">Emp</span>]</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure>

<h4 id="2-由内部数据集创建"><a href="#2-由内部数据集创建" class="headerlink" title="2. 由内部数据集创建"></a>2. 由内部数据集创建</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.需要导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建 case class,等价于 Java Bean</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">ename: <span class="type">String</span>, comm: <span class="type">Double</span>, deptno: <span class="type">Long</span>, empno: <span class="type">Long</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">               hiredate: <span class="type">String</span>, job: <span class="type">String</span>, mgr: <span class="type">Long</span>, sal: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// 3.由内部数据集创建 Datasets</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Emp</span>(<span class="string">&quot;ALLEN&quot;</span>, <span class="number">300.0</span>, <span class="number">30</span>, <span class="number">7499</span>, <span class="string">&quot;1981-02-20 00:00:00&quot;</span>, <span class="string">&quot;SALESMAN&quot;</span>, <span class="number">7698</span>, <span class="number">1600.0</span>),</span><br><span class="line">                      <span class="type">Emp</span>(<span class="string">&quot;JONES&quot;</span>, <span class="number">300.0</span>, <span class="number">30</span>, <span class="number">7499</span>, <span class="string">&quot;1981-02-20 00:00:00&quot;</span>, <span class="string">&quot;SALESMAN&quot;</span>, <span class="number">7698</span>, <span class="number">1600.0</span>))</span><br><span class="line">                    .toDS()</span><br><span class="line">caseClassDS.show()</span><br></pre></td></tr></table></figure>

<br/>

<h3 id="1-3-由RDD创建DataFrame"><a href="#1-3-由RDD创建DataFrame" class="headerlink" title="1.3 由RDD创建DataFrame"></a>1.3 由RDD创建DataFrame</h3><p>Spark 支持两种方式把 RDD 转换为 DataFrame，分别是使用反射推断和指定 Schema 转换：</p>
<h4 id="1-使用反射推断"><a href="#1-使用反射推断" class="headerlink" title="1. 使用反射推断"></a>1. 使用反射推断</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建部门类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Dept</span>(<span class="params">deptno: <span class="type">Long</span>, dname: <span class="type">String</span>, loc: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// 3.创建 RDD 并转换为 dataSet</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rddToDS</span> </span>= spark.sparkContext</span><br><span class="line">  .textFile(<span class="string">&quot;/usr/file/dept.txt&quot;</span>)</span><br><span class="line">  .map(_.split(<span class="string">&quot;\t&quot;</span>))</span><br><span class="line">  .map(line =&gt; <span class="type">Dept</span>(line(<span class="number">0</span>).trim.toLong, line(<span class="number">1</span>), line(<span class="number">2</span>)))</span><br><span class="line">  .toDS()  <span class="comment">// 如果调用 toDF() 则转换为 dataFrame </span></span><br></pre></td></tr></table></figure>

<h4 id="2-以编程方式指定Schema"><a href="#2-以编程方式指定Schema" class="headerlink" title="2. 以编程方式指定Schema"></a>2. 以编程方式指定Schema</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.定义每个列的列类型</span></span><br><span class="line"><span class="keyword">val</span> fields = <span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;deptno&quot;</span>, <span class="type">LongType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">                   <span class="type">StructField</span>(<span class="string">&quot;dname&quot;</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">                   <span class="type">StructField</span>(<span class="string">&quot;loc&quot;</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建 schema</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.创建 RDD</span></span><br><span class="line"><span class="keyword">val</span> deptRDD = spark.sparkContext.textFile(<span class="string">&quot;/usr/file/dept.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rowRDD = deptRDD.map(_.split(<span class="string">&quot;\t&quot;</span>)).map(line =&gt; <span class="type">Row</span>(line(<span class="number">0</span>).toLong, line(<span class="number">1</span>), line(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.将 RDD 转换为 dataFrame</span></span><br><span class="line"><span class="keyword">val</span> deptDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">deptDF.show()</span><br></pre></td></tr></table></figure>

<br/>

<h3 id="1-4-DataFrames与Datasets互相转换"><a href="#1-4-DataFrames与Datasets互相转换" class="headerlink" title="1.4  DataFrames与Datasets互相转换"></a>1.4  DataFrames与Datasets互相转换</h3><p>Spark 提供了非常简单的转换方法用于 DataFrame 与 Dataset 间的互相转换，示例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> DataFrames转Datasets</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.as[Emp]</span></span><br><span class="line">res1: org.apache.spark.sql.Dataset[Emp] = [COMM: double, DEPTNO: bigint ... 6 more fields]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Datasets转DataFrames</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> ds.toDF()</span></span><br><span class="line">res2: org.apache.spark.sql.DataFrame = [COMM: double, DEPTNO: bigint ... 6 more fields]</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="二、Columns列操作"><a href="#二、Columns列操作" class="headerlink" title="二、Columns列操作"></a>二、Columns列操作</h2><h3 id="2-1-引用列"><a href="#2-1-引用列" class="headerlink" title="2.1 引用列"></a>2.1 引用列</h3><p>Spark 支持多种方法来构造和引用列，最简单的是使用 <code>col() </code> 或 <code>column() </code> 函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">col(<span class="string">&quot;colName&quot;</span>)</span><br><span class="line">column(<span class="string">&quot;colName&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于 Scala 语言而言，还可以使用$&quot;myColumn&quot;和&#x27;myColumn 这两种语法糖进行引用。</span></span><br><span class="line">df.select($<span class="string">&quot;ename&quot;</span>, $<span class="string">&quot;job&quot;</span>).show()</span><br><span class="line">df.select(<span class="symbol">&#x27;ename</span>, <span class="symbol">&#x27;job</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-2-新增列"><a href="#2-2-新增列" class="headerlink" title="2.2 新增列"></a>2.2 新增列</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基于已有列值新增列</span></span><br><span class="line">df.withColumn(<span class="string">&quot;upSal&quot;</span>,$<span class="string">&quot;sal&quot;</span>+<span class="number">1000</span>)</span><br><span class="line"><span class="comment">// 基于固定值新增列</span></span><br><span class="line">df.withColumn(<span class="string">&quot;intCol&quot;</span>,lit(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>

<h3 id="2-3-删除列"><a href="#2-3-删除列" class="headerlink" title="2.3 删除列"></a>2.3 删除列</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 支持删除多个列</span></span><br><span class="line">df.drop(<span class="string">&quot;comm&quot;</span>,<span class="string">&quot;job&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-4-重命名列"><a href="#2-4-重命名列" class="headerlink" title="2.4 重命名列"></a>2.4 重命名列</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.withColumnRenamed(<span class="string">&quot;comm&quot;</span>, <span class="string">&quot;common&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<p>需要说明的是新增，删除，重命名列都会产生新的 DataFrame，原来的 DataFrame 不会被改变。</p>
<br/>

<h2 id="三、使用Structured-API进行基本查询"><a href="#三、使用Structured-API进行基本查询" class="headerlink" title="三、使用Structured API进行基本查询"></a>三、使用Structured API进行基本查询</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.查询员工姓名及工作</span></span><br><span class="line">df.select($<span class="string">&quot;ename&quot;</span>, $<span class="string">&quot;job&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.filter 查询工资大于 2000 的员工信息</span></span><br><span class="line">df.filter($<span class="string">&quot;sal&quot;</span> &gt; <span class="number">2000</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.orderBy 按照部门编号降序，工资升序进行查询</span></span><br><span class="line">df.orderBy(desc(<span class="string">&quot;deptno&quot;</span>), asc(<span class="string">&quot;sal&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.limit 查询工资最高的 3 名员工的信息</span></span><br><span class="line">df.orderBy(desc(<span class="string">&quot;sal&quot;</span>)).limit(<span class="number">3</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5.distinct 查询所有部门编号</span></span><br><span class="line">df.select(<span class="string">&quot;deptno&quot;</span>).distinct().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6.groupBy 分组统计部门人数</span></span><br><span class="line">df.groupBy(<span class="string">&quot;deptno&quot;</span>).count().show()</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="四、使用Spark-SQL进行基本查询"><a href="#四、使用Spark-SQL进行基本查询" class="headerlink" title="四、使用Spark SQL进行基本查询"></a>四、使用Spark SQL进行基本查询</h2><h3 id="4-1-Spark-SQL基本使用"><a href="#4-1-Spark-SQL基本使用" class="headerlink" title="4.1 Spark  SQL基本使用"></a>4.1 Spark  SQL基本使用</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.首先需要将 DataFrame 注册为临时视图</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;emp&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.查询员工姓名及工作</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT ename,job FROM emp&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.查询工资大于 2000 的员工信息</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp where sal &gt; 2000&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.orderBy 按照部门编号降序，工资升序进行查询</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp ORDER BY deptno DESC,sal ASC&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5.limit  查询工资最高的 3 名员工的信息</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp ORDER BY sal DESC LIMIT 3&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6.distinct 查询所有部门编号</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT DISTINCT(deptno) FROM emp&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7.分组统计部门人数</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT deptno,count(ename) FROM emp group by deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="4-2-全局临时视图"><a href="#4-2-全局临时视图" class="headerlink" title="4.2 全局临时视图"></a>4.2 全局临时视图</h3><p>上面使用 <code>createOrReplaceTempView</code> 创建的是会话临时视图，它的生命周期仅限于会话范围，会随会话的结束而结束。</p>
<p>你也可以使用 <code>createGlobalTempView</code> 创建全局临时视图，全局临时视图可以在所有会话之间共享，并直到整个 Spark 应用程序终止后才会消失。全局临时视图被定义在内置的 <code>global_temp</code> 数据库下，需要使用限定名称进行引用，如 <code>SELECT * FROM global_temp.view1</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册为全局临时视图</span></span><br><span class="line">df.createGlobalTempView(<span class="string">&quot;gemp&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用限定名称进行引用</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT ename,job FROM global_temp.gemp&quot;</span>).show()</span><br></pre></td></tr></table></figure>


<h1 id="Spark-SQL-外部数据源"><a href="#Spark-SQL-外部数据源" class="headerlink" title="Spark SQL 外部数据源"></a>Spark SQL 外部数据源</h1><h2 id="一、简介-2"><a href="#一、简介-2" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-多数据源支持"><a href="#1-1-多数据源支持" class="headerlink" title="1.1 多数据源支持"></a>1.1 多数据源支持</h3><p>Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。</p>
<ul>
<li>CSV</li>
<li>JSON</li>
<li>Parquet</li>
<li>ORC</li>
<li>JDBC/ODBC connections</li>
<li>Plain-text files</li>
</ul>
<blockquote>
<p>注：以下所有测试文件均可从本仓库的<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录进行下载</p>
</blockquote>
<h3 id="1-2-读数据格式"><a href="#1-2-读数据格式" class="headerlink" title="1.2 读数据格式"></a>1.2 读数据格式</h3><p>所有读取 API 遵循以下调用格式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameReader</span>.format(...).option(<span class="string">&quot;key&quot;</span>, <span class="string">&quot;value&quot;</span>).schema(...).load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 示例</span></span><br><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)          <span class="comment">// 读取模式</span></span><br><span class="line">.option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)       <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/file(s)&quot;</span>)   <span class="comment">// 文件路径</span></span><br><span class="line">.schema(someSchema)                  <span class="comment">// 使用预定义的 schema      </span></span><br><span class="line">.load()</span><br></pre></td></tr></table></figure>

<p>读取模式有以下三种可选项：</p>
<table>
<thead>
<tr>
<th>读模式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>permissive</code></td>
<td>当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中</td>
</tr>
<tr>
<td><code>dropMalformed</code></td>
<td>删除格式不正确的行</td>
</tr>
<tr>
<td><code>failFast</code></td>
<td>遇到格式不正确的数据时立即失败</td>
</tr>
</tbody></table>
<h3 id="1-3-写数据格式"><a href="#1-3-写数据格式" class="headerlink" title="1.3 写数据格式"></a>1.3 写数据格式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameWriter</span>.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()</span><br><span class="line"></span><br><span class="line"><span class="comment">//示例</span></span><br><span class="line">dataframe.write.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;OVERWRITE&quot;</span>)         <span class="comment">//写模式</span></span><br><span class="line">.option(<span class="string">&quot;dateFormat&quot;</span>, <span class="string">&quot;yyyy-MM-dd&quot;</span>)  <span class="comment">//日期格式</span></span><br><span class="line">.option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/file(s)&quot;</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>

<p>写数据模式有以下四种可选项：</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code></td>
<td align="left">如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left">数据以追加的方式写入</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td align="left">数据以覆盖的方式写入</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td align="left">如果给定的路径已经存在文件，则不做任何操作</td>
</tr>
</tbody></table>
<br/>

<h2 id="二、CSV"><a href="#二、CSV" class="headerlink" title="二、CSV"></a>二、CSV</h2><p>CSV 是一种常见的文本文件格式，其中每一行表示一条记录，记录中的每个字段用逗号分隔。</p>
<h3 id="2-1-读取CSV文件"><a href="#2-1-读取CSV文件" class="headerlink" title="2.1 读取CSV文件"></a>2.1 读取CSV文件</h3><p>自动推断类型读取读取示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;false&quot;</span>)        <span class="comment">// 文件中的第一行是否为列的名称</span></span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)      <span class="comment">// 是否快速失败</span></span><br><span class="line">.option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)   <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.load(<span class="string">&quot;/usr/file/csv/dept.csv&quot;</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<p>使用预定义类型：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>,<span class="type">LongType</span>&#125;</span><br><span class="line"><span class="comment">//预定义数据格式</span></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;deptno&quot;</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;dname&quot;</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;loc&quot;</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>)</span><br><span class="line">))</span><br><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)</span><br><span class="line">.schema(myManualSchema)</span><br><span class="line">.load(<span class="string">&quot;/usr/file/csv/dept.csv&quot;</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<h3 id="2-2-写入CSV文件"><a href="#2-2-写入CSV文件" class="headerlink" title="2.2 写入CSV文件"></a>2.2 写入CSV文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;csv&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/csv/dept2&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>也可以指定具体的分隔符：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;csv&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>).save(<span class="string">&quot;/tmp/csv/dept2&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-可选配置"><a href="#2-3-可选配置" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.1 小节。</p>
<br/>

<h2 id="三、JSON"><a href="#三、JSON" class="headerlink" title="三、JSON"></a>三、JSON</h2><h3 id="3-1-读取JSON文件"><a href="#3-1-读取JSON文件" class="headerlink" title="3.1 读取JSON文件"></a>3.1 读取JSON文件</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(&quot;json&quot;).option(&quot;mode&quot;, &quot;FAILFAST&quot;).load(&quot;/usr/file/json/dept.json&quot;).show(5)</span><br></pre></td></tr></table></figure>

<p>需要注意的是：默认不支持一条数据记录跨越多行 (如下)，可以通过配置 <code>multiLine</code> 为 <code>true</code> 来进行更改，其默认值为 <code>false</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认支持单行</span></span><br><span class="line">&#123;<span class="attr">&quot;DEPTNO&quot;</span>: <span class="number">10</span>,<span class="attr">&quot;DNAME&quot;</span>: <span class="string">&quot;ACCOUNTING&quot;</span>,<span class="attr">&quot;LOC&quot;</span>: <span class="string">&quot;NEW YORK&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//默认不支持多行</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;DEPTNO&quot;</span>: <span class="number">10</span>,</span><br><span class="line">  <span class="attr">&quot;DNAME&quot;</span>: <span class="string">&quot;ACCOUNTING&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;LOC&quot;</span>: <span class="string">&quot;NEW YORK&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-写入JSON文件"><a href="#3-2-写入JSON文件" class="headerlink" title="3.2 写入JSON文件"></a>3.2 写入JSON文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;json&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/spark/json/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-3-可选配置"><a href="#3-3-可选配置" class="headerlink" title="3.3 可选配置"></a>3.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.2 小节。</p>
<br/>

<h2 id="四、Parquet"><a href="#四、Parquet" class="headerlink" title="四、Parquet"></a>四、Parquet</h2><p> Parquet 是一个开源的面向列的数据存储，它提供了多种存储优化，允许读取单独的列非整个文件，这不仅节省了存储空间而且提升了读取效率，它是 Spark 是默认的文件格式。</p>
<h3 id="4-1-读取Parquet文件"><a href="#4-1-读取Parquet文件" class="headerlink" title="4.1 读取Parquet文件"></a>4.1 读取Parquet文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;parquet&quot;</span>).load(<span class="string">&quot;/usr/file/parquet/dept.parquet&quot;</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-写入Parquet文件"><a href="#2-2-写入Parquet文件" class="headerlink" title="2.2 写入Parquet文件"></a>2.2 写入Parquet文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;parquet&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/spark/parquet/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-可选配置-1"><a href="#2-3-可选配置-1" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>Parquet 文件有着自己的存储规则，因此其可选配置项比较少，常用的有如下两个：</p>
<table>
<thead>
<tr>
<th>读写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Write</td>
<td>compression or codec</td>
<td>None,uncompressed, bzip2, deflate, gzip, lz4, or snappy</td>
<td>None</td>
<td>压缩文件格式</td>
</tr>
<tr>
<td>Read</td>
<td>mergeSchema</td>
<td>true, false</td>
<td>取决于配置项 <code>spark.sql.parquet.mergeSchema</code></td>
<td>当为真时，Parquet 数据源将所有数据文件收集的 Schema 合并在一起，否则将从摘要文件中选择 Schema，如果没有可用的摘要文件，则从随机数据文件中选择 Schema。</td>
</tr>
</tbody></table>
<blockquote>
<p>更多可选配置可以参阅官方文档：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</a></p>
</blockquote>
<br/>

<h2 id="五、ORC"><a href="#五、ORC" class="headerlink" title="五、ORC"></a>五、ORC</h2><p>ORC 是一种自描述的、类型感知的列文件格式，它针对大型数据的读写进行了优化，也是大数据中常用的文件格式。</p>
<h3 id="5-1-读取ORC文件"><a href="#5-1-读取ORC文件" class="headerlink" title="5.1 读取ORC文件"></a>5.1 读取ORC文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;orc&quot;</span>).load(<span class="string">&quot;/usr/file/orc/dept.orc&quot;</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-2-写入ORC文件"><a href="#4-2-写入ORC文件" class="headerlink" title="4.2 写入ORC文件"></a>4.2 写入ORC文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csvFile.write.format(<span class="string">&quot;orc&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/spark/orc/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="六、SQL-Databases"><a href="#六、SQL-Databases" class="headerlink" title="六、SQL Databases"></a>六、SQL Databases</h2><p>Spark 同样支持与传统的关系型数据库进行数据读写。但是 Spark 程序默认是没有提供数据库驱动的，所以在使用前需要将对应的数据库驱动上传到安装目录下的 <code>jars</code> 目录中。下面示例使用的是 Mysql 数据库，使用前需要将对应的 <code>mysql-connector-java-x.x.x.jar</code> 上传到 <code>jars</code> 目录下。</p>
<h3 id="6-1-读取数据"><a href="#6-1-读取数据" class="headerlink" title="6.1 读取数据"></a>6.1 读取数据</h3><p>读取全表数据示例如下，这里的 <code>help_keyword</code> 是 mysql 内置的字典表，只有 <code>help_keyword_id</code> 和 <code>name</code> 两个字段。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)            <span class="comment">//驱动</span></span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>)   <span class="comment">//数据库地址</span></span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;help_keyword&quot;</span>)                    <span class="comment">//表名</span></span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;root&quot;</span>).load().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>从查询结果读取数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pushDownQuery = <span class="string">&quot;&quot;&quot;(SELECT * FROM help_keyword WHERE help_keyword_id &lt;20) AS help_keywords&quot;&quot;&quot;</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, pushDownQuery)</span><br><span class="line">.load().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|             <span class="number">10</span>|      <span class="type">ALTER</span>|</span><br><span class="line">|             <span class="number">11</span>|    <span class="type">ANALYSE</span>|</span><br><span class="line">|             <span class="number">12</span>|    <span class="type">ANALYZE</span>|</span><br><span class="line">|             <span class="number">13</span>|        <span class="type">AND</span>|</span><br><span class="line">|             <span class="number">14</span>|    <span class="type">ARCHIVE</span>|</span><br><span class="line">|             <span class="number">15</span>|       <span class="type">AREA</span>|</span><br><span class="line">|             <span class="number">16</span>|         <span class="type">AS</span>|</span><br><span class="line">|             <span class="number">17</span>|   <span class="type">ASBINARY</span>|</span><br><span class="line">|             <span class="number">18</span>|        <span class="type">ASC</span>|</span><br><span class="line">|             <span class="number">19</span>|     <span class="type">ASTEXT</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>

<p>也可以使用如下的写法进行数据的过滤：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> java.util.<span class="type">Properties</span></span><br><span class="line">props.setProperty(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> predicates = <span class="type">Array</span>(<span class="string">&quot;help_keyword_id &lt; 10  OR name = &#x27;WHEN&#x27;&quot;</span>)   <span class="comment">//指定数据过滤条件</span></span><br><span class="line">spark.read.jdbc(<span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>, <span class="string">&quot;help_keyword&quot;</span>, predicates, props).show() </span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|            <span class="number">604</span>|       <span class="type">WHEN</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>

<p>可以使用 <code>numPartitions</code> 指定读取数据的并行度：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">option(<span class="string">&quot;numPartitions&quot;</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>在这里，除了可以指定分区外，还可以设置上界和下界，任何小于下界的值都会被分配在第一个分区中，任何大于上界的值都会被分配在最后一个分区中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> colName = <span class="string">&quot;help_keyword_id&quot;</span>   <span class="comment">//用于判断上下界的列</span></span><br><span class="line"><span class="keyword">val</span> lowerBound = <span class="number">300</span>L    <span class="comment">//下界</span></span><br><span class="line"><span class="keyword">val</span> upperBound = <span class="number">500</span>L    <span class="comment">//上界</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = <span class="number">10</span>   <span class="comment">//分区综述</span></span><br><span class="line"><span class="keyword">val</span> jdbcDf = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>,<span class="string">&quot;help_keyword&quot;</span>,</span><br><span class="line">                             colName,lowerBound,upperBound,numPartitions,props)</span><br></pre></td></tr></table></figure>

<p>想要验证分区内容，可以使用 <code>mapPartitionsWithIndex</code> 这个算子，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jdbcDf.rdd.mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">        buffer.append(index + <span class="string">&quot;分区:&quot;</span> + iterator.next())</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br></pre></td></tr></table></figure>

<p>执行结果如下：<code>help_keyword</code> 这张表只有 600 条左右的数据，本来数据应该均匀分布在 10 个分区，但是 0 分区里面却有 319 条数据，这是因为设置了下限，所有小于 300 的数据都会被限制在第一个分区，即 0 分区。同理所有大于 500 的数据被分配在 9 分区，即最后一个分区。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-mysql-%E5%88%86%E5%8C%BA%E4%B8%8A%E4%B8%8B%E9%99%90.png" alt="spark-mysql-分区上下限.png"></p>
<h3 id="6-2-写入数据"><a href="#6-2-写入数据" class="headerlink" title="6.2 写入数据"></a>6.2 写入数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line">df.write</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;emp&quot;</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="七、Text"><a href="#七、Text" class="headerlink" title="七、Text"></a>七、Text</h2><p>Text 文件在读写性能方面并没有任何优势，且不能表达明确的数据结构，所以其使用的比较少，读写操作如下：</p>
<h3 id="7-1-读取Text数据"><a href="#7-1-读取Text数据" class="headerlink" title="7.1 读取Text数据"></a>7.1 读取Text数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.textFile(<span class="string">&quot;/usr/file/txt/dept.txt&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="7-2-写入Text数据"><a href="#7-2-写入Text数据" class="headerlink" title="7.2 写入Text数据"></a>7.2 写入Text数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.text(<span class="string">&quot;/tmp/spark/txt/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="八、数据读写高级特性"><a href="#八、数据读写高级特性" class="headerlink" title="八、数据读写高级特性"></a>八、数据读写高级特性</h2><h3 id="8-1-并行读"><a href="#8-1-并行读" class="headerlink" title="8.1 并行读"></a>8.1 并行读</h3><p>多个 Executors 不能同时读取同一个文件，但它们可以同时读取不同的文件。这意味着当您从一个包含多个文件的文件夹中读取数据时，这些文件中的每一个都将成为 DataFrame 中的一个分区，并由可用的 Executors 并行读取。</p>
<h3 id="8-2-并行写"><a href="#8-2-并行写" class="headerlink" title="8.2 并行写"></a>8.2 并行写</h3><p>写入的文件或数据的数量取决于写入数据时 DataFrame 拥有的分区数量。默认情况下，每个数据分区写一个文件。</p>
<h3 id="8-3-分区写入"><a href="#8-3-分区写入" class="headerlink" title="8.3 分区写入"></a>8.3 分区写入</h3><p>分区和分桶这两个概念和 Hive 中分区表和分桶表是一致的。都是将数据按照一定规则进行拆分存储。需要注意的是 <code>partitionBy</code> 指定的分区和 RDD 中分区不是一个概念：这里的<strong>分区表现为输出目录的子目录</strong>，数据分别存储在对应的子目录中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line">df.write.mode(<span class="string">&quot;overwrite&quot;</span>).partitionBy(<span class="string">&quot;deptno&quot;</span>).save(<span class="string">&quot;/tmp/spark/partitions&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：可以看到输出被按照部门编号分为三个子目录，子目录中才是对应的输出文件。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-%E5%88%86%E5%8C%BA.png" alt="spark-分区.png"></p>
<h3 id="8-3-分桶写入"><a href="#8-3-分桶写入" class="headerlink" title="8.3 分桶写入"></a>8.3 分桶写入</h3><p>分桶写入就是将数据按照指定的列和桶数进行散列，目前分桶写入只支持保存为表，实际上这就是 Hive 的分桶表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numberBuckets = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> columnToBucketBy = <span class="string">&quot;empno&quot;</span></span><br><span class="line">df.write.format(<span class="string">&quot;parquet&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(<span class="string">&quot;bucketedFiles&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="8-5-文件大小管理"><a href="#8-5-文件大小管理" class="headerlink" title="8.5 文件大小管理"></a>8.5 文件大小管理</h3><p>如果写入产生小文件数量过多，这时会产生大量的元数据开销。Spark 和 HDFS 一样，都不能很好的处理这个问题，这被称为“small file problem”。同时数据文件也不能过大，否则在查询时会有不必要的性能开销，因此要把文件大小控制在一个合理的范围内。</p>
<p>在上文我们已经介绍过可以通过分区数量来控制生成文件的数量，从而间接控制文件大小。Spark 2.2 引入了一种新的方法，以更自动化的方式控制文件大小，这就是 <code>maxRecordsPerFile</code> 参数，它允许你通过控制写入文件的记录数来控制文件大小。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// Spark 将确保文件最多包含 5000 条记录</span></span><br><span class="line">df.write.option(“maxRecordsPerFile”, <span class="number">5000</span>)</span><br></pre></td></tr></table></figure>

<br>

<h2 id="九、可选配置附录"><a href="#九、可选配置附录" class="headerlink" title="九、可选配置附录"></a>九、可选配置附录</h2><h3 id="9-1-CSV读写可选配置"><a href="#9-1-CSV读写可选配置" class="headerlink" title="9.1 CSV读写可选配置"></a>9.1 CSV读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Both</td>
<td>seq</td>
<td>任意字符</td>
<td><code>,</code>(逗号)</td>
<td>分隔符</td>
</tr>
<tr>
<td>Both</td>
<td>header</td>
<td>true, false</td>
<td>false</td>
<td>文件中的第一行是否为列的名称。</td>
</tr>
<tr>
<td>Read</td>
<td>escape</td>
<td>任意字符</td>
<td>\</td>
<td>转义字符</td>
</tr>
<tr>
<td>Read</td>
<td>inferSchema</td>
<td>true, false</td>
<td>false</td>
<td>是否自动推断列类型</td>
</tr>
<tr>
<td>Read</td>
<td>ignoreLeadingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值前面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>ignoreTrailingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值后面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>nullValue</td>
<td>任意字符</td>
<td>“”</td>
<td>声明文件中哪个字符表示空值</td>
</tr>
<tr>
<td>Both</td>
<td>nanValue</td>
<td>任意字符</td>
<td>NaN</td>
<td>声明哪个值表示 NaN 或者缺省值</td>
</tr>
<tr>
<td>Both</td>
<td>positiveInf</td>
<td>任意字符</td>
<td>Inf</td>
<td>正无穷</td>
</tr>
<tr>
<td>Both</td>
<td>negativeInf</td>
<td>任意字符</td>
<td>-Inf</td>
<td>负无穷</td>
</tr>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br/>uncompressed,<br/>bzip2, deflate,<br/>gzip, lz4, or<br/>snappy</td>
<td>none</td>
<td>文件压缩格式</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 <br/>SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
<td>日期格式</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 <br/>SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
<td>时间戳格式</td>
</tr>
<tr>
<td>Read</td>
<td>maxColumns</td>
<td>任意整数</td>
<td>20480</td>
<td>声明文件中的最大列数</td>
</tr>
<tr>
<td>Read</td>
<td>maxCharsPerColumn</td>
<td>任意整数</td>
<td>1000000</td>
<td>声明一个列中的最大字符数。</td>
</tr>
<tr>
<td>Read</td>
<td>escapeQuotes</td>
<td>true, false</td>
<td>true</td>
<td>是否应该转义行中的引号。</td>
</tr>
<tr>
<td>Read</td>
<td>maxMalformedLogPerPartition</td>
<td>任意整数</td>
<td>10</td>
<td>声明每个分区中最多允许多少条格式错误的数据，超过这个值后格式错误的数据将不会被读取</td>
</tr>
<tr>
<td>Write</td>
<td>quoteAll</td>
<td>true, false</td>
<td>false</td>
<td>指定是否应该将所有值都括在引号中，而不只是转义具有引号字符的值。</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
<td>是否允许每条完整记录跨域多行</td>
</tr>
</tbody></table>
<h3 id="9-2-JSON读写可选配置"><a href="#9-2-JSON读写可选配置" class="headerlink" title="9.2 JSON读写可选配置"></a>9.2 JSON读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br/>uncompressed,<br/>bzip2, deflate,<br/>gzip, lz4, or<br/>snappy</td>
<td>none</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
</tr>
<tr>
<td>Read</td>
<td>primitiveAsString</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowComments</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowUnquotedFieldNames</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowSingleQuotes</td>
<td>true, false</td>
<td>true</td>
</tr>
<tr>
<td>Read</td>
<td>allowNumericLeadingZeros</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowBackslashEscapingAnyCharacter</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>columnNameOfCorruptRecord</td>
<td>true, false</td>
<td>Value of spark.sql.column&amp;NameOf</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
</tr>
</tbody></table>
<h3 id="9-3-数据库读写可选配置"><a href="#9-3-数据库读写可选配置" class="headerlink" title="9.3 数据库读写可选配置"></a>9.3 数据库读写可选配置</h3><table>
<thead>
<tr>
<th>属性名称</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>url</td>
<td>数据库地址</td>
</tr>
<tr>
<td>dbtable</td>
<td>表名称</td>
</tr>
<tr>
<td>driver</td>
<td>数据库驱动</td>
</tr>
<tr>
<td>partitionColumn,<br/>lowerBound, upperBoun</td>
<td>分区总数，上界，下界</td>
</tr>
<tr>
<td>numPartitions</td>
<td>可用于表读写并行性的最大分区数。如果要写的分区数量超过这个限制，那么可以调用 coalesce(numpartition) 重置分区数。</td>
</tr>
<tr>
<td>fetchsize</td>
<td>每次往返要获取多少行数据。此选项仅适用于读取数据。</td>
</tr>
<tr>
<td>batchsize</td>
<td>每次往返插入多少行数据，这个选项只适用于写入数据。默认值是 1000。</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>事务隔离级别：可以是 NONE，READ_COMMITTED, READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE，即标准事务隔离级别。<br/>默认值是 READ_UNCOMMITTED。这个选项只适用于数据读取。</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>写入数据时自定义创建表的相关配置</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>写入数据时自定义创建列的列类型</td>
</tr>
</tbody></table>
<blockquote>
<p>数据库读写更多配置可以参阅官方文档：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources.html">https://spark.apache.org/docs/latest/sql-data-sources.html</a></li>
</ol>
<h2 id="参考资料-1"><a href="#参考资料-1" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide">RDD Programming Guide</a></p>
<h1 id="SparkSQL常用聚合函数"><a href="#SparkSQL常用聚合函数" class="headerlink" title="SparkSQL常用聚合函数"></a>SparkSQL常用聚合函数</h1><h2 id="一、简单聚合"><a href="#一、简单聚合" class="headerlink" title="一、简单聚合"></a>一、简单聚合</h2><h3 id="1-1-数据准备"><a href="#1-1-数据准备" class="headerlink" title="1.1 数据准备"></a>1.1 数据准备</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要导入 spark sql 内置的函数包</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;aggregations&quot;</span>).master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> empDF = spark.read.json(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line"><span class="comment">// 注册为临时视图，用于后面演示 SQL 查询</span></span><br><span class="line">empDF.createOrReplaceTempView(<span class="string">&quot;emp&quot;</span>)</span><br><span class="line">empDF.show()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：emp.json 可以从本仓库的<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下载。</p>
</blockquote>
<h3 id="1-2-count"><a href="#1-2-count" class="headerlink" title="1.2 count"></a>1.2 count</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算员工人数</span></span><br><span class="line">empDF.select(count(<span class="string">&quot;ename&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-3-countDistinct"><a href="#1-3-countDistinct" class="headerlink" title="1.3 countDistinct"></a>1.3 countDistinct</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算姓名不重复的员工人数</span></span><br><span class="line">empDF.select(countDistinct(<span class="string">&quot;deptno&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-4-approx-count-distinct"><a href="#1-4-approx-count-distinct" class="headerlink" title="1.4 approx_count_distinct"></a>1.4 approx_count_distinct</h3><p>通常在使用大型数据集时，你可能关注的只是近似值而不是准确值，这时可以使用 approx_count_distinct 函数，并可以使用第二个参数指定最大允许误差。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.select(approx_count_distinct (<span class="string">&quot;ename&quot;</span>,<span class="number">0.1</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-5-first-amp-last"><a href="#1-5-first-amp-last" class="headerlink" title="1.5 first &amp; last"></a>1.5 first &amp; last</h3><p>获取 DataFrame 中指定列的第一个值或者最后一个值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.select(first(<span class="string">&quot;ename&quot;</span>),last(<span class="string">&quot;job&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-6-min-amp-max"><a href="#1-6-min-amp-max" class="headerlink" title="1.6 min &amp; max"></a>1.6 min &amp; max</h3><p>获取 DataFrame 中指定列的最小值或者最大值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.select(min(<span class="string">&quot;sal&quot;</span>),max(<span class="string">&quot;sal&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-7-sum-amp-sumDistinct"><a href="#1-7-sum-amp-sumDistinct" class="headerlink" title="1.7 sum &amp; sumDistinct"></a>1.7 sum &amp; sumDistinct</h3><p>求和以及求指定列所有不相同的值的和。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.select(sum(<span class="string">&quot;sal&quot;</span>)).show()</span><br><span class="line">empDF.select(sumDistinct(<span class="string">&quot;sal&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-8-avg"><a href="#1-8-avg" class="headerlink" title="1.8 avg"></a>1.8 avg</h3><p>内置的求平均数的函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.select(avg(<span class="string">&quot;sal&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-9-数学函数"><a href="#1-9-数学函数" class="headerlink" title="1.9 数学函数"></a>1.9 数学函数</h3><p>Spark SQL 中还支持多种数学聚合函数，用于通常的数学计算，以下是一些常用的例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.计算总体方差、均方差、总体标准差、样本标准差</span></span><br><span class="line">empDF.select(var_pop(<span class="string">&quot;sal&quot;</span>), var_samp(<span class="string">&quot;sal&quot;</span>), stddev_pop(<span class="string">&quot;sal&quot;</span>), stddev_samp(<span class="string">&quot;sal&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.计算偏度和峰度</span></span><br><span class="line">empDF.select(skewness(<span class="string">&quot;sal&quot;</span>), kurtosis(<span class="string">&quot;sal&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 计算两列的皮尔逊相关系数、样本协方差、总体协方差。(这里只是演示，员工编号和薪资两列实际上并没有什么关联关系)</span></span><br><span class="line">empDF.select(corr(<span class="string">&quot;empno&quot;</span>, <span class="string">&quot;sal&quot;</span>), covar_samp(<span class="string">&quot;empno&quot;</span>, <span class="string">&quot;sal&quot;</span>),covar_pop(<span class="string">&quot;empno&quot;</span>, <span class="string">&quot;sal&quot;</span>)).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-10-聚合数据到集合"><a href="#1-10-聚合数据到集合" class="headerlink" title="1.10 聚合数据到集合"></a>1.10 聚合数据到集合</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  empDF.agg(collect_set(<span class="string">&quot;job&quot;</span>), collect_list(<span class="string">&quot;ename&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">+--------------------+--------------------+</span><br><span class="line">|    collect_set(job)| collect_list(ename)|</span><br><span class="line">+--------------------+--------------------+</span><br><span class="line">|[<span class="type">MANAGER</span>, <span class="type">SALESMA</span>...|[<span class="type">SMITH</span>, <span class="type">ALLEN</span>, <span class="type">WA</span>...|</span><br><span class="line">+--------------------+--------------------+</span><br></pre></td></tr></table></figure>



<h2 id="二、分组聚合"><a href="#二、分组聚合" class="headerlink" title="二、分组聚合"></a>二、分组聚合</h2><h3 id="2-1-简单分组"><a href="#2-1-简单分组" class="headerlink" title="2.1 简单分组"></a>2.1 简单分组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">empDF.groupBy(<span class="string">&quot;deptno&quot;</span>, <span class="string">&quot;job&quot;</span>).count().show()</span><br><span class="line"><span class="comment">//等价 SQL</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT deptno, job, count(*) FROM emp GROUP BY deptno, job&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">+------+---------+-----+</span><br><span class="line">|deptno|      job|count|</span><br><span class="line">+------+---------+-----+</span><br><span class="line">|    <span class="number">10</span>|<span class="type">PRESIDENT</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">30</span>|    <span class="type">CLERK</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">10</span>|  <span class="type">MANAGER</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">30</span>|  <span class="type">MANAGER</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">20</span>|    <span class="type">CLERK</span>|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">30</span>| <span class="type">SALESMAN</span>|    <span class="number">4</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">ANALYST</span>|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">10</span>|    <span class="type">CLERK</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">MANAGER</span>|    <span class="number">1</span>|</span><br><span class="line">+------+---------+-----+</span><br></pre></td></tr></table></figure>

<h3 id="2-2-分组聚合"><a href="#2-2-分组聚合" class="headerlink" title="2.2 分组聚合"></a>2.2 分组聚合</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">empDF.groupBy(<span class="string">&quot;deptno&quot;</span>).agg(count(<span class="string">&quot;ename&quot;</span>).alias(<span class="string">&quot;人数&quot;</span>), sum(<span class="string">&quot;sal&quot;</span>).alias(<span class="string">&quot;总工资&quot;</span>)).show()</span><br><span class="line"><span class="comment">// 等价语法</span></span><br><span class="line">empDF.groupBy(<span class="string">&quot;deptno&quot;</span>).agg(<span class="string">&quot;ename&quot;</span>-&gt;<span class="string">&quot;count&quot;</span>,<span class="string">&quot;sal&quot;</span>-&gt;<span class="string">&quot;sum&quot;</span>).show()</span><br><span class="line"><span class="comment">// 等价 SQL</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT deptno, count(ename) ,sum(sal) FROM emp GROUP BY deptno&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">+------+----+------+</span><br><span class="line">|deptno|人数|总工资|</span><br><span class="line">+------+----+------+</span><br><span class="line">|    <span class="number">10</span>|   <span class="number">3</span>|<span class="number">8750.0</span>|</span><br><span class="line">|    <span class="number">30</span>|   <span class="number">6</span>|<span class="number">9400.0</span>|</span><br><span class="line">|    <span class="number">20</span>|   <span class="number">5</span>|<span class="number">9375.0</span>|</span><br><span class="line">+------+----+------+</span><br></pre></td></tr></table></figure>



<h2 id="三、自定义聚合函数"><a href="#三、自定义聚合函数" class="headerlink" title="三、自定义聚合函数"></a>三、自定义聚合函数</h2><p>Scala 提供了两种自定义聚合函数的方法，分别如下：</p>
<ul>
<li>有类型的自定义聚合函数，主要适用于 DataSet；</li>
<li>无类型的自定义聚合函数，主要适用于 DataFrame。</li>
</ul>
<p>以下分别使用两种方式来自定义一个求平均值的聚合函数，这里以计算员工平均工资为例。两种自定义方式分别如下：</p>
<h3 id="3-1-有类型的自定义函数"><a href="#3-1-有类型的自定义函数" class="headerlink" title="3.1 有类型的自定义函数"></a>3.1 有类型的自定义函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>, functions&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.定义员工类,对于可能存在 null 值的字段需要使用 Option 进行包装</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">ename: <span class="type">String</span>, comm: scala.<span class="type">Option</span>[<span class="type">Double</span>], deptno: <span class="type">Long</span>, empno: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">               hiredate: <span class="type">String</span>, job: <span class="type">String</span>, mgr: scala.<span class="type">Option</span>[<span class="type">Long</span>], sal: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// 2.定义聚合操作的中间输出类型</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">SumAndCount</span>(<span class="params">var sum: <span class="type">Double</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">/* 3.自定义聚合函数</span></span></span><br><span class="line"><span class="class"><span class="comment"> * @IN  聚合操作的输入类型</span></span></span><br><span class="line"><span class="class"><span class="comment"> * @BUF reduction 操作输出值的类型</span></span></span><br><span class="line"><span class="class"><span class="comment"> * @OUT 聚合操作的输出类型</span></span></span><br><span class="line"><span class="class"><span class="comment"> */</span></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Emp</span>, <span class="type">SumAndCount</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4.用于聚合操作的的初始零值</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">SumAndCount</span> = <span class="type">SumAndCount</span>(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 5.同一分区中的 reduce 操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(avg: <span class="type">SumAndCount</span>, emp: <span class="type">Emp</span>): <span class="type">SumAndCount</span> = &#123;</span><br><span class="line">        avg.sum += emp.sal</span><br><span class="line">        avg.count += <span class="number">1</span></span><br><span class="line">        avg</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6.不同分区中的 merge 操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(avg1: <span class="type">SumAndCount</span>, avg2: <span class="type">SumAndCount</span>): <span class="type">SumAndCount</span> = &#123;</span><br><span class="line">        avg1.sum += avg2.sum</span><br><span class="line">        avg1.count += avg2.count</span><br><span class="line">        avg1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7.定义最终的输出类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">SumAndCount</span>): <span class="type">Double</span> = reduction.sum / reduction.count</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 8.中间类型的编码转换</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">SumAndCount</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 9.输出类型的编码转换</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSqlApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;Spark-SQL&quot;</span>).master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> ds = spark.read.json(<span class="string">&quot;file/emp.json&quot;</span>).as[<span class="type">Emp</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 10.使用内置 avg() 函数和自定义函数分别进行计算，验证自定义函数是否正确</span></span><br><span class="line">        <span class="keyword">val</span> myAvg = ds.select(<span class="type">MyAverage</span>.toColumn.name(<span class="string">&quot;average_sal&quot;</span>)).first()</span><br><span class="line">        <span class="keyword">val</span> avg = ds.select(functions.avg(ds.col(<span class="string">&quot;sal&quot;</span>))).first().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        println(<span class="string">&quot;自定义 average 函数 : &quot;</span> + myAvg)</span><br><span class="line">        println(<span class="string">&quot;内置的 average 函数 : &quot;</span> + avg)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义聚合函数需要实现的方法比较多，这里以绘图的方式来演示其执行流程，以及每个方法的作用：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-sql-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0.png" alt="spark-sql-自定义函数.png"></p>
<p>关于 <code>zero</code>,<code>reduce</code>,<code>merge</code>,<code>finish</code> 方法的作用在上图都有说明，这里解释一下中间类型和输出类型的编码转换，这个写法比较固定，基本上就是两种情况：</p>
<ul>
<li>自定义类型 Case Class 或者元组就使用 <code>Encoders.product</code> 方法；</li>
<li>基本类型就使用其对应名称的方法，如 <code>scalaByte </code>，<code>scalaFloat</code>，<code>scalaShort</code> 等，示例如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">SumAndCount</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br></pre></td></tr></table></figure>



<h3 id="3-2-无类型的自定义聚合函数"><a href="#3-2-无类型的自定义聚合函数" class="headerlink" title="3.2 无类型的自定义聚合函数"></a>3.2 无类型的自定义聚合函数</h3><p>理解了有类型的自定义聚合函数后，无类型的定义方式也基本相同，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1.聚合操作输入参数的类型,字段名称可以自定义</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;MyInputColumn&quot;</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.聚合操作中间值的类型,字段名称可以自定义</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">&quot;MyCount&quot;</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.聚合操作输出参数的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4.此函数是否始终在相同输入上返回相同的输出,通常为 true</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5.定义零值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6.同一分区中的 reduce 操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7.不同分区中的 merge 操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 8.计算最终的输出值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSqlApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 测试方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;Spark-SQL&quot;</span>).master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="comment">// 9.注册自定义的聚合函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;myAverage&quot;</span>, <span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;file/emp.json&quot;</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;emp&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 10.使用自定义函数和内置函数分别进行计算</span></span><br><span class="line">    <span class="keyword">val</span> myAvg = spark.sql(<span class="string">&quot;SELECT myAverage(sal) as avg_sal FROM emp&quot;</span>).first()</span><br><span class="line">    <span class="keyword">val</span> avg = spark.sql(<span class="string">&quot;SELECT avg(sal) as avg_sal FROM emp&quot;</span>).first()</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;自定义 average 函数 : &quot;</span> + myAvg)</span><br><span class="line">    println(<span class="string">&quot;内置的 average 函数 : &quot;</span> + avg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="参考资料-2"><a href="#参考资料-2" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
</ol>
<h1 id="SparkSQL联结操作"><a href="#SparkSQL联结操作" class="headerlink" title="SparkSQL联结操作"></a>SparkSQL联结操作</h1><h2 id="一、-数据准备"><a href="#一、-数据准备" class="headerlink" title="一、 数据准备"></a>一、 数据准备</h2><p>本文主要介绍 Spark SQL 的多表连接，需要预先准备测试数据。分别创建员工和部门的 Datafame，并注册为临时视图，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;aggregations&quot;</span>).master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> empDF = spark.read.json(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line">empDF.createOrReplaceTempView(<span class="string">&quot;emp&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> deptDF = spark.read.json(<span class="string">&quot;/usr/file/json/dept.json&quot;</span>)</span><br><span class="line">deptDF.createOrReplaceTempView(<span class="string">&quot;dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>两表的主要字段如下：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">emp</span> <span class="string">员工表</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">ENAME: 员工姓名</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">DEPTNO: 部门编号</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">EMPNO: 员工编号</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">HIREDATE: 入职时间</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">JOB: 职务</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">MGR: 上级编号</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">SAL: 薪资</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">COMM: 奖金  </span></span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dept</span> <span class="string">部门表</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">DEPTNO: 部门编号</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">DNAME:  部门名称</span></span><br><span class="line"> <span class="meta">|--</span> <span class="string">LOC:    部门所在城市</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：emp.json，dept.json 可以在本仓库的<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录进行下载。</p>
</blockquote>
<h2 id="二、连接类型"><a href="#二、连接类型" class="headerlink" title="二、连接类型"></a>二、连接类型</h2><p>Spark 中支持多种连接类型：</p>
<ul>
<li><strong>Inner Join</strong> : 内连接；</li>
<li><strong>Full Outer Join</strong> :  全外连接；</li>
<li><strong>Left Outer Join</strong> :  左外连接；</li>
<li><strong>Right Outer Join</strong> :  右外连接；</li>
<li><strong>Left Semi Join</strong> :  左半连接；</li>
<li><strong>Left Anti Join</strong> :  左反连接；</li>
<li><strong>Natural Join</strong> :  自然连接；</li>
<li><strong>Cross (or Cartesian) Join</strong> :  交叉 (或笛卡尔) 连接。</li>
</ul>
<p>其中内，外连接，笛卡尔积均与普通关系型数据库中的相同，如下图所示：</p>
<p><img src="/sql-join.jpg" alt="sql-join.jpg"></p>
<p>这里解释一下左半连接和左反连接，这两个连接等价于关系型数据库中的 <code>IN</code> 和 <code>NOT IN</code> 字句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- LEFT SEMI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT ANTI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> ANTI <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br></pre></td></tr></table></figure>

<p>所有连接类型的示例代码如下：</p>
<h3 id="2-1-INNER-JOIN"><a href="#2-1-INNER-JOIN" class="headerlink" title="2.1 INNER JOIN"></a>2.1 INNER JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.定义连接表达式</span></span><br><span class="line"><span class="keyword">val</span> joinExpression = empDF.col(<span class="string">&quot;deptno&quot;</span>) === deptDF.col(<span class="string">&quot;deptno&quot;</span>)</span><br><span class="line"><span class="comment">// 2.连接查询 </span></span><br><span class="line">empDF.join(deptDF,joinExpression).select(<span class="string">&quot;ename&quot;</span>,<span class="string">&quot;dname&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价 SQL 如下：</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT ename,dname FROM emp JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-2-FULL-OUTER-JOIN"><a href="#2-2-FULL-OUTER-JOIN" class="headerlink" title="2.2 FULL OUTER JOIN"></a>2.2 FULL OUTER JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">&quot;outer&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp FULL OUTER JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-3-LEFT-OUTER-JOIN"><a href="#2-3-LEFT-OUTER-JOIN" class="headerlink" title="2.3 LEFT OUTER JOIN"></a>2.3 LEFT OUTER JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">&quot;left_outer&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp LEFT OUTER JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-4-RIGHT-OUTER-JOIN"><a href="#2-4-RIGHT-OUTER-JOIN" class="headerlink" title="2.4 RIGHT OUTER JOIN"></a>2.4 RIGHT OUTER JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">&quot;right_outer&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp RIGHT OUTER JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-5-LEFT-SEMI-JOIN"><a href="#2-5-LEFT-SEMI-JOIN" class="headerlink" title="2.5 LEFT SEMI JOIN"></a>2.5 LEFT SEMI JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">&quot;left_semi&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-6-LEFT-ANTI-JOIN"><a href="#2-6-LEFT-ANTI-JOIN" class="headerlink" title="2.6 LEFT ANTI JOIN"></a>2.6 LEFT ANTI JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">&quot;left_anti&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp LEFT ANTI JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-7-CROSS-JOIN"><a href="#2-7-CROSS-JOIN" class="headerlink" title="2.7 CROSS JOIN"></a>2.7 CROSS JOIN</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">&quot;cross&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp CROSS JOIN dept ON emp.deptno = dept.deptno&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="2-8-NATURAL-JOIN"><a href="#2-8-NATURAL-JOIN" class="headerlink" title="2.8 NATURAL JOIN"></a>2.8 NATURAL JOIN</h3><p>自然连接是在两张表中寻找那些数据类型和列名都相同的字段，然后自动地将他们连接起来，并返回所有符合条件的结果。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM emp NATURAL JOIN dept&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<p>以下是一个自然连接的查询结果，程序自动推断出使用两张表都存在的 dept 列进行连接，其实际等价于：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;SELECT * FROM emp JOIN dept ON emp.deptno = dept.deptno&quot;).<span class="keyword">show</span>()</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-sql-NATURAL-JOIN.png" alt="spark-sql-NATURAL-JOIN.png"></p>
<p>由于自然连接常常会产生不可预期的结果，所以并不推荐使用。</p>
<h2 id="三、连接的执行"><a href="#三、连接的执行" class="headerlink" title="三、连接的执行"></a>三、连接的执行</h2><p>在对大表与大表之间进行连接操作时，通常都会触发 <code>Shuffle Join</code>，两表的所有分区节点会进行 <code>All-to-All</code> 的通讯，这种查询通常比较昂贵，会对网络 IO 会造成比较大的负担。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Big-table%E2%80%93to%E2%80%93big-table.png" alt="spark-Big-table–to–big-table.png"></p>
<p>而对于大表和小表的连接操作，Spark 会在一定程度上进行优化，如果小表的数据量小于 Worker Node 的内存空间，Spark 会考虑将小表的数据广播到每一个 Worker Node，在每个工作节点内部执行连接计算，这可以降低网络的 IO，但会加大每个 Worker Node 的 CPU 负担。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-Big-table%E2%80%93to%E2%80%93small-table.png" alt="spark-Big-table–to–small-table.png"></p>
<p>是否采用广播方式进行 <code>Join</code> 取决于程序内部对小表的判断，如果想明确使用广播方式进行 <code>Join</code>，则可以在 DataFrame API 中使用 <code>broadcast</code> 方法指定需要广播的小表：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(broadcast(deptDF), joinExpression).show()</span><br></pre></td></tr></table></figure>

<h1 id="Spark-Streaming与流处理"><a href="#Spark-Streaming与流处理" class="headerlink" title="Spark Streaming与流处理"></a>Spark Streaming与流处理</h1><h2 id="一、流处理"><a href="#一、流处理" class="headerlink" title="一、流处理"></a>一、流处理</h2><h3 id="1-1-静态数据处理"><a href="#1-1-静态数据处理" class="headerlink" title="1.1 静态数据处理"></a>1.1 静态数据处理</h3><p>在流处理之前，数据通常存储在数据库，文件系统或其他形式的存储系统中。应用程序根据需要查询数据或计算数据。这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。</p>
<div align="center"> <img  src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/01_data_at_rest_infrastructure.png"/> </div>



<h3 id="1-2-流处理"><a href="#1-2-流处理" class="headerlink" title="1.2 流处理"></a>1.2 流处理</h3><p>而流处理则是直接对运动中的数据的处理，在接收数据时直接计算数据。</p>
<p>大多数数据都是连续的流：传感器事件，网站上的用户活动，金融交易等等 ，所有这些数据都是随着时间的推移而创建的。</p>
<p>接收和发送数据流并执行应用程序或分析逻辑的系统称为<strong>流处理器</strong>。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。</p>
<div align="center"> <img  src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/02_stream_processing_infrastructure.png"/> </div>



<p>流处理带来了静态数据处理所不具备的众多优点：</p>
<ul>
<li><strong>应用程序立即对数据做出反应</strong>：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期；</li>
<li><strong>流处理可以处理更大的数据量</strong>：直接处理数据流，并且只保留数据中有意义的子集，并将其传送到下一个处理单元，逐级过滤数据，降低需要处理的数据量，从而能够承受更大的数据量；</li>
<li><strong>流处理更贴近现实的数据模型</strong>：在实际的环境中，一切数据都是持续变化的，要想能够通过过去的数据推断未来的趋势，必须保证数据的不断输入和模型的不断修正，典型的就是金融市场、股票市场，流处理能更好的应对这些数据的连续性的特征和及时性的需求；</li>
<li><strong>流处理分散和分离基础设施</strong>：流式处理减少了对大型数据库的需求。相反，每个流处理程序通过流处理框架维护了自己的数据和状态，这使得流处理程序更适合微服务架构。</li>
</ul>
<h2 id="二、Spark-Streaming"><a href="#二、Spark-Streaming" class="headerlink" title="二、Spark Streaming"></a>二、Spark Streaming</h2><h2 id="一、流处理-1"><a href="#一、流处理-1" class="headerlink" title="一、流处理"></a>一、流处理</h2><h3 id="1-1-静态数据处理-1"><a href="#1-1-静态数据处理-1" class="headerlink" title="1.1 静态数据处理"></a>1.1 静态数据处理</h3><p>在流处理之前，数据通常存储在数据库，文件系统或其他形式的存储系统中。应用程序根据需要查询数据或计算数据。这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/01_data_at_rest_infrastructure.png" alt="01_data_at_rest_infrastructure.png"></p>
<h3 id="1-2-流处理-1"><a href="#1-2-流处理-1" class="headerlink" title="1.2 流处理"></a>1.2 流处理</h3><p>而流处理则是直接对运动中的数据的处理，在接收数据时直接计算数据。</p>
<p>大多数数据都是连续的流：传感器事件，网站上的用户活动，金融交易等等 ，所有这些数据都是随着时间的推移而创建的。</p>
<p>接收和发送数据流并执行应用程序或分析逻辑的系统称为<strong>流处理器</strong>。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/02_stream_processing_infrastructure.png" alt="02_stream_processing_infrastructure.png"></p>
<p>流处理带来了静态数据处理所不具备的众多优点：</p>
<ul>
<li><strong>应用程序立即对数据做出反应</strong>：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期；</li>
<li><strong>流处理可以处理更大的数据量</strong>：直接处理数据流，并且只保留数据中有意义的子集，并将其传送到下一个处理单元，逐级过滤数据，降低需要处理的数据量，从而能够承受更大的数据量；</li>
<li><strong>流处理更贴近现实的数据模型</strong>：在实际的环境中，一切数据都是持续变化的，要想能够通过过去的数据推断未来的趋势，必须保证数据的不断输入和模型的不断修正，典型的就是金融市场、股票市场，流处理能更好的应对这些数据的连续性的特征和及时性的需求；</li>
<li><strong>流处理分散和分离基础设施</strong>：流式处理减少了对大型数据库的需求。相反，每个流处理程序通过流处理框架维护了自己的数据和状态，这使得流处理程序更适合微服务架构。</li>
</ul>
<h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>Spark Streaming 是 Spark 的一个子模块，用于快速构建可扩展，高吞吐量，高容错的流处理程序。具有以下特点：</p>
<ul>
<li>通过高级 API 构建应用程序，简单易用；</li>
<li>支持多种语言，如 Java，Scala 和 Python；</li>
<li>良好的容错性，Spark Streaming 支持快速从失败中恢复丢失的操作状态；</li>
<li>能够和 Spark 其他模块无缝集成，将流处理与批处理完美结合；</li>
<li>Spark Streaming 可以从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，也支持自定义数据源。</li>
</ul>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-arch.png" alt="spark-streaming-arch.png"></p>
<h3 id="2-2-DStream"><a href="#2-2-DStream" class="headerlink" title="2.2 DStream"></a>2.2 DStream</h3><p>Spark Streaming 提供称为离散流 (DStream) 的高级抽象，用于表示连续的数据流。 DStream 可以从来自 Kafka，Flume 和 Kinesis 等数据源的输入数据流创建，也可以由其他 DStream 转化而来。<strong>在内部，DStream 表示为一系列 RDD</strong>。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-flow.png" alt="spark-streaming-flow.png"></p>
<h3 id="2-3-Spark-amp-Storm-amp-Flink"><a href="#2-3-Spark-amp-Storm-amp-Flink" class="headerlink" title="2.3 Spark &amp; Storm &amp; Flink"></a>2.3 Spark &amp; Storm &amp; Flink</h3><p>storm 和 Flink 都是真正意义上的流计算框架，但 Spark Streaming 只是将数据流进行极小粒度的拆分，拆分为多个批处理，使得其能够得到接近于流处理的效果，但其本质上还是批处理（或微批处理）。</p>
<h2 id="参考资料-3"><a href="#参考资料-3" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming Programming Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ververica.com/what-is-stream-processing">What is stream processing?</a></li>
</ol>
<h1 id="Spark-Streaming-基本操作"><a href="#Spark-Streaming-基本操作" class="headerlink" title="Spark Streaming 基本操作"></a>Spark Streaming 基本操作</h1><h2 id="一、案例"><a href="#一、案例" class="headerlink" title="一、案例"></a>一、案例</h2><p>演示流的创建：获取指定端口上的数据进行词频统计。</p>
<p>项目依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">/*指定时间间隔为 5s*/</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;NetworkWordCount&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*创建文本流，并进行词频统计*/</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;hadoop1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    lines.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*启动服务*/</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">/*等待服务结束*/</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以直接执行官方示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/run-example streaming.NetworkWordCount localhost 9999</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参考: <a target="_blank" rel="noopener" href="https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/streaming-guide.html">https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/streaming-guide.html</a></p>
</blockquote>
<h1 id="Spark-Streaming-基本操作-1"><a href="#Spark-Streaming-基本操作-1" class="headerlink" title="Spark Streaming 基本操作"></a>Spark Streaming 基本操作</h1><h2 id="一、案例引入"><a href="#一、案例引入" class="headerlink" title="一、案例引入"></a>一、案例引入</h2><p>这里先引入一个基本的案例来演示流的创建：获取指定端口上的数据并进行词频统计。项目依赖和代码实现如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*指定时间间隔为 5s*/</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;NetworkWordCount&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*创建文本输入流,并进行词频统计*/</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;hadoop001&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    lines.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*启动服务*/</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">/*等待服务结束*/</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用本地模式启动 Spark 程序，然后使用 <code>nc -lk 9999</code> 打开端口并输入测试数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]#  nc -lk 9999</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br></pre></td></tr></table></figure>

<p>此时控制台输出如下，可以看到已经接收到数据并按行进行了词频统计。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-word-count-v1.png" alt="spark-streaming-word-count-v1.png"><br><br/></p>
<p>下面针对示例代码进行讲解：</p>
<h3 id="3-1-StreamingContext"><a href="#3-1-StreamingContext" class="headerlink" title="3.1 StreamingContext"></a>3.1 StreamingContext</h3><p>Spark Streaming 编程的入口类是 StreamingContext，在创建时候需要指明 <code>sparkConf</code> 和 <code>batchDuration</code>(批次时间)，Spark 流处理本质是将流数据拆分为一个个批次，然后进行微批处理，<code>batchDuration</code> 就是批次拆分的时间间隔。这个时间可以根据业务需求和服务器性能进行指定，如果业务要求低延迟并且服务器性能也允许，则这个时间可以指定得很短。</p>
<p>这里需要注意的是：示例代码使用的是本地模式，配置为 <code>local[2]</code>，这里不能配置为 <code>local[1]</code>。这是因为对于流数据的处理，Spark 必须有一个独立的 Executor 来接收数据，然后再由其他的 Executors 来处理，所以为了保证数据能够被处理，至少要有 2 个 Executors。这里我们的程序只有一个数据流，在并行读取多个数据流的时候，也需要保证有足够的 Executors 来接收和处理数据。</p>
<h3 id="3-2-数据源"><a href="#3-2-数据源" class="headerlink" title="3.2 数据源"></a>3.2 数据源</h3><p>在示例代码中使用的是 <code>socketTextStream</code> 来创建基于 Socket 的数据流，实际上 Spark 还支持多种数据源，分为以下两类：</p>
<ul>
<li><strong>基本数据源</strong>：包括文件系统、Socket 连接等；</li>
<li><strong>高级数据源</strong>：包括 Kafka，Flume，Kinesis 等。</li>
</ul>
<p>在基本数据源中，Spark 支持监听 HDFS 上指定目录，当有新文件加入时，会获取其文件内容作为输入流。创建方式如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对于文本文件，指明监听目录即可</span></span><br><span class="line">streamingContext.textFileStream(dataDirectory)</span><br><span class="line"><span class="comment">// 对于其他文件，需要指明目录，以及键的类型、值的类型、和输入格式</span></span><br><span class="line">streamingContext.fileStream[<span class="type">KeyClass</span>, <span class="type">ValueClass</span>, <span class="type">InputFormatClass</span>](dataDirectory)</span><br></pre></td></tr></table></figure>

<p>被监听的目录可以是具体目录，如 <code>hdfs://host:8040/logs/</code>；也可以使用通配符，如 <code>hdfs://host:8040/logs/2017/*</code>。</p>
<blockquote>
<p>关于高级数据源的整合单独整理至：<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_Streaming%E6%95%B4%E5%90%88Flume.md">Spark Streaming 整合 Flume</a> 和 <a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_Streaming%E6%95%B4%E5%90%88Kafka.md">Spark Streaming 整合 Kafka</a></p>
</blockquote>
<h3 id="3-3-服务的启动与停止"><a href="#3-3-服务的启动与停止" class="headerlink" title="3.3 服务的启动与停止"></a>3.3 服务的启动与停止</h3><p>在示例代码中，使用 <code>streamingContext.start()</code> 代表启动服务，此时还要使用 <code>streamingContext.awaitTermination()</code> 使服务处于等待和可用的状态，直到发生异常或者手动使用 <code>streamingContext.stop()</code> 进行终止。</p>
<h2 id="二、Transformation"><a href="#二、Transformation" class="headerlink" title="二、Transformation"></a>二、Transformation</h2><h3 id="2-1-DStream与RDDs"><a href="#2-1-DStream与RDDs" class="headerlink" title="2.1 DStream与RDDs"></a>2.1 DStream与RDDs</h3><p>DStream 是 Spark Streaming 提供的基本抽象。它表示连续的数据流。在内部，DStream 由一系列连续的 RDD 表示。所以从本质上而言，应用于 DStream 的任何操作都会转换为底层 RDD 上的操作。例如，在示例代码中 flatMap 算子的操作实际上是作用在每个 RDDs 上 (如下图)。因为这个原因，所以 DStream 能够支持 RDD 大部分的<em>transformation</em>算子。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-dstream-ops.png" alt="spark-streaming-dstream-ops.png"></p>
<h3 id="2-2-updateStateByKey"><a href="#2-2-updateStateByKey" class="headerlink" title="2.2 updateStateByKey"></a>2.2 updateStateByKey</h3><p>除了能够支持 RDD 的算子外，DStream 还有部分独有的<em>transformation</em>算子，这当中比较常用的是 <code>updateStateByKey</code>。文章开头的词频统计程序，只能统计每一次输入文本中单词出现的数量，想要统计所有历史输入中单词出现的数量，可以使用 <code>updateStateByKey</code> 算子。代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCountV2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 本地测试时最好指定 hadoop 用户名,否则会默认使用本地电脑的用户名,</span></span><br><span class="line"><span class="comment">     * 此时在 HDFS 上创建目录时可能会抛出权限不足的异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;NetworkWordCountV2&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">/*必须要设置检查点*/</span></span><br><span class="line">    ssc.checkpoint(<span class="string">&quot;hdfs://hadoop001:8020/spark-streaming&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;hadoop001&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    lines.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">      .updateStateByKey[<span class="type">Int</span>](updateFunction _)   <span class="comment">//updateStateByKey 算子</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 累计求和</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param currentValues 当前的数据</span></span><br><span class="line"><span class="comment">    * @param preValues     之前的数据</span></span><br><span class="line"><span class="comment">    * @return 相加后的数据</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(currentValues: <span class="type">Seq</span>[<span class="type">Int</span>], preValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> current = currentValues.sum</span><br><span class="line">    <span class="keyword">val</span> pre = preValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="type">Some</span>(current + pre)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用 <code>updateStateByKey</code> 算子，你必须使用 <code>ssc.checkpoint()</code> 设置检查点，这样当使用 <code>updateStateByKey</code> 算子时，它会去检查点中取出上一次保存的信息，并使用自定义的 <code>updateFunction</code> 函数将上一次的数据和本次数据进行相加，然后返回。</p>
<h3 id="2-3-启动测试"><a href="#2-3-启动测试" class="headerlink" title="2.3 启动测试"></a>2.3 启动测试</h3><p>在监听端口输入如下测试数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]#  nc -lk 9999</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br></pre></td></tr></table></figure>

<p>此时控制台输出如下，所有输入都被进行了词频累计：</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-word-count-v2.png" alt="spark-streaming-word-count-v2.png"></p>
<p>同时在输出日志中还可以看到检查点操作的相关信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 保存检查点信息</span></span><br><span class="line">19/05/27 16:21:05 INFO CheckpointWriter: Saving checkpoint for time 1558945265000 ms </span><br><span class="line">to file &#x27;hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除已经无用的检查点信息</span></span><br><span class="line">19/05/27 16:21:30 INFO CheckpointWriter: </span><br><span class="line">Deleting hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000</span><br></pre></td></tr></table></figure>

<h2 id="三、输出操作"><a href="#三、输出操作" class="headerlink" title="三、输出操作"></a>三、输出操作</h2><h3 id="3-1-输出API"><a href="#3-1-输出API" class="headerlink" title="3.1 输出API"></a>3.1 输出API</h3><p>Spark Streaming 支持以下输出操作：</p>
<table>
<thead>
<tr>
<th align="left">Output Operation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>print</strong>()</td>
<td align="left">在运行流应用程序的 driver 节点上打印 DStream 中每个批次的前十个元素。用于开发调试。</td>
</tr>
<tr>
<td align="left"><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td align="left">将 DStream 的内容保存为文本文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。</td>
</tr>
<tr>
<td align="left"><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td align="left">将 DStream 的内容序列化为 Java 对象，并保存到 SequenceFiles。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。</td>
</tr>
<tr>
<td align="left"><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td align="left">将 DStream 的内容保存为 Hadoop 文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。</td>
</tr>
<tr>
<td align="left"><strong>foreachRDD</strong>(<em>func</em>)</td>
<td align="left">最通用的输出方式，它将函数 func 应用于从流生成的每个 RDD。此函数应将每个 RDD 中的数据推送到外部系统，例如将 RDD 保存到文件，或通过网络将其写入数据库。</td>
</tr>
</tbody></table>
<p>前面的四个 API 都是直接调用即可，下面主要讲解通用的输出方式 <code>foreachRDD(func)</code>，通过该 API 你可以将数据保存到任何你需要的数据源。</p>
<h3 id="3-1-foreachRDD"><a href="#3-1-foreachRDD" class="headerlink" title="3.1 foreachRDD"></a>3.1 foreachRDD</h3><p>这里我们使用 Redis 作为客户端，对文章开头示例程序进行改变，把每一次词频统计的结果写入到 Redis，并利用 Redis 的 <code>HINCRBY</code> 命令来进行词频统计。这里需要导入 Jedis 依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>具体实现代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCountToRedis</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;NetworkWordCountToRedis&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*创建文本输入流,并进行词频统计*/</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;hadoop001&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> pairs: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">     <span class="comment">/*保存数据到 Redis*/</span></span><br><span class="line">    pairs.foreachRDD &#123; rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">        <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          jedis = <span class="type">JedisPoolUtil</span>.getConnection</span><br><span class="line">          partitionOfRecords.foreach(record =&gt; jedis.hincrBy(<span class="string">&quot;wordCount&quot;</span>, record._1, record._2))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;</span><br><span class="line">            ex.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (jedis != <span class="literal">null</span>) jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中 <code>JedisPoolUtil</code> 的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis.clients.jedis.Jedis;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.JedisPool;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.JedisPoolConfig;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JedisPoolUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 声明为 volatile 防止指令重排序 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> JedisPool jedisPool = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HOST = <span class="string">&quot;localhost&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> PORT = <span class="number">6379</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 双重检查锁实现懒汉式单例 */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Jedis <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (jedisPool == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (JedisPoolUtil.class) &#123;</span><br><span class="line">                <span class="keyword">if</span> (jedisPool == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    JedisPoolConfig config = <span class="keyword">new</span> JedisPoolConfig();</span><br><span class="line">                    config.setMaxTotal(<span class="number">30</span>);</span><br><span class="line">                    config.setMaxIdle(<span class="number">10</span>);</span><br><span class="line">                    jedisPool = <span class="keyword">new</span> JedisPool(config, HOST, PORT);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> jedisPool.getResource();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-3-代码说明"><a href="#3-3-代码说明" class="headerlink" title="3.3 代码说明"></a>3.3 代码说明</h3><p>这里将上面保存到 Redis 的代码单独抽取出来，并去除异常判断的部分。精简后的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pairs.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> jedis = <span class="type">JedisPoolUtil</span>.getConnection</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; jedis.hincrBy(<span class="string">&quot;wordCount&quot;</span>, record._1, record._2))</span><br><span class="line">    jedis.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里可以看到一共使用了三次循环，分别是循环 RDD，循环分区，循环每条记录，上面我们的代码是在循环分区的时候获取连接，也就是为每一个分区获取一个连接。但是这里大家可能会有疑问：为什么不在循环 RDD 的时候，为每一个 RDD 获取一个连接，这样所需要的连接数会更少。实际上这是不可行的，如果按照这种情况进行改写，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pairs.foreachRDD &#123; rdd =&gt;</span><br><span class="line">    <span class="keyword">val</span> jedis = <span class="type">JedisPoolUtil</span>.getConnection</span><br><span class="line">    rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">        partitionOfRecords.foreach(record =&gt; jedis.hincrBy(<span class="string">&quot;wordCount&quot;</span>, record._1, record._2))</span><br><span class="line">    &#125;</span><br><span class="line">    jedis.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时在执行时候就会抛出 <code>Caused by: java.io.NotSerializableException: redis.clients.jedis.Jedis</code>，这是因为在实际计算时，Spark 会将对 RDD 操作分解为多个 Task，Task 运行在具体的 Worker Node 上。在执行之前，Spark 会对任务进行闭包，之后闭包被序列化并发送给每个 Executor，而 <code>Jedis</code> 显然是不能被序列化的，所以会抛出异常。</p>
<p>第二个需要注意的是 ConnectionPool 最好是一个静态，惰性初始化连接池 。这是因为 Spark 的转换操作本身就是惰性的，且没有数据流时不会触发写出操作，所以出于性能考虑，连接池应该是惰性的，因此上面 <code>JedisPool</code> 在初始化时采用了懒汉式单例进行惰性初始化。</p>
<h3 id="3-4-启动测试"><a href="#3-4-启动测试" class="headerlink" title="3.4 启动测试"></a>3.4 启动测试</h3><p>在监听端口输入如下测试数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]#  nc -lk 9999</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br></pre></td></tr></table></figure>

<p>使用 Redis Manager 查看写入结果 (如下图),可以看到与使用 <code>updateStateByKey</code> 算子得到的计算结果相同。</p>
<p><img src="/2021/04/23/Spark%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/spark-streaming-word-count-v3.png"><br><br/></p>
<blockquote>
<p>本片文章所有源码见本仓库：<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/code/spark/spark-streaming-basis">spark-streaming-basis</a></p>
</blockquote>
<h2 id="参考资料-4"><a href="#参考资料-4" class="headerlink" title="参考资料"></a>参考资料</h2><p>Spark 官方文档：<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark%E7%AE%80%E4%BB%8B.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark简介.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_RDD.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_RDD.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Transformation%E5%92%8CAction%E7%AE%97%E5%AD%90.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Transformation和Action算子.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark累加器与广播变量.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL_Dataset%E5%92%8CDataFrame%E7%AE%80%E4%BB%8B.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL_Dataset和DataFrame简介.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Structured_API%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Structured_API的基本使用.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL外部数据源.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL常用聚合函数.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL%E8%81%94%E7%BB%93%E6%93%8D%E4%BD%9C.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/SparkSQL联结操作.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Streaming%E4%B8%8E%E6%B5%81%E5%A4%84%E7%90%86.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Streaming与流处理.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Streaming%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.md">https://github.com/RealTommyHu/BigData-Notes/blob/master/notes/Spark_Streaming基本操作.md</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/22/%E9%80%9A%E8%BF%87GithubPages%E5%92%8C%E9%9D%99%E6%80%81%E7%AB%99%E7%82%B9%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" rel="prev" title="通过GithubPages和静态站点生成框架Hexo搭建博客">
      <i class="fa fa-chevron-left"></i> 通过GithubPages和静态站点生成框架Hexo搭建博客
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/04/25/Ubuntu%E6%9B%B4%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90%E6%8F%90%E9%AB%98%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6/" rel="next" title="Ubuntu更换国内源提高包管理器下载速度">
      Ubuntu更换国内源提高包管理器下载速度 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">Spark简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">一、简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">二、特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84"><span class="nav-number">1.3.</span> <span class="nav-text">三、集群架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">1.4.</span> <span class="nav-text">四、核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Spark-SQL"><span class="nav-number">1.4.1.</span> <span class="nav-text">1 Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Spark-Streaming"><span class="nav-number">1.4.2.</span> <span class="nav-text">2 Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-MLlib"><span class="nav-number">1.4.3.</span> <span class="nav-text">3 MLlib</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Graphx"><span class="nav-number">1.4.4.</span> <span class="nav-text">4 Graphx</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86RDDs"><span class="nav-number">2.</span> <span class="nav-text">弹性分布式数据集RDDs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81RDD%E7%AE%80%E4%BB%8B"><span class="nav-number">2.1.</span> <span class="nav-text">一、RDD简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%88%9B%E5%BB%BARDD"><span class="nav-number">2.2.</span> <span class="nav-text">二、创建RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%94%B1%E7%8E%B0%E6%9C%89%E9%9B%86%E5%90%88%E5%88%9B%E5%BB%BA"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.1 由现有集合创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%BA%94%E7%94%A8%E5%A4%96%E9%83%A8%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2 应用外部存储系统中的数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-textFile-amp-wholeTextFiles"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.3 textFile &amp; wholeTextFiles</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%93%8D%E4%BD%9CRDD"><span class="nav-number">2.3.</span> <span class="nav-text">三、操作RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E7%BC%93%E5%AD%98RDD"><span class="nav-number">2.4.</span> <span class="nav-text">四、缓存RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E7%BC%93%E5%AD%98%E7%BA%A7%E5%88%AB"><span class="nav-number">2.4.1.</span> <span class="nav-text">4.1 缓存级别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98"><span class="nav-number">2.4.2.</span> <span class="nav-text">4.2 使用缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E7%A7%BB%E9%99%A4%E7%BC%93%E5%AD%98"><span class="nav-number">2.4.3.</span> <span class="nav-text">4.3 移除缓存</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E7%90%86%E8%A7%A3shuffle"><span class="nav-number">2.5.</span> <span class="nav-text">五、理解shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-shuffle%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.5.1.</span> <span class="nav-text">5.1 shuffle介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Shuffle%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">2.5.2.</span> <span class="nav-text">5.2 Shuffle的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E5%AF%BC%E8%87%B4Shuffle%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">2.5.3.</span> <span class="nav-text">5.3 导致Shuffle的操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="nav-number">2.6.</span> <span class="nav-text">五、宽依赖和窄依赖</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81DAG%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">2.7.</span> <span class="nav-text">六、DAG的生成</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformation-%E5%92%8C-Action-%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="nav-number">3.</span> <span class="nav-text">Transformation 和 Action 常用算子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Transformation"><span class="nav-number">3.1.</span> <span class="nav-text">一、Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-map"><span class="nav-number">3.1.1.</span> <span class="nav-text">1.1 map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-filter"><span class="nav-number">3.1.2.</span> <span class="nav-text">1.2 filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-flatMap"><span class="nav-number">3.1.3.</span> <span class="nav-text">1.3 flatMap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-mapPartitions"><span class="nav-number">3.1.4.</span> <span class="nav-text">1.4 mapPartitions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-mapPartitionsWithIndex"><span class="nav-number">3.1.5.</span> <span class="nav-text">1.5 mapPartitionsWithIndex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-sample"><span class="nav-number">3.1.6.</span> <span class="nav-text">1.6 sample</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-union"><span class="nav-number">3.1.7.</span> <span class="nav-text">1.7 union</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-intersection"><span class="nav-number">3.1.8.</span> <span class="nav-text">1.8 intersection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-distinct"><span class="nav-number">3.1.9.</span> <span class="nav-text">1.9 distinct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-groupByKey"><span class="nav-number">3.1.10.</span> <span class="nav-text">1.10 groupByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-reduceByKey"><span class="nav-number">3.1.11.</span> <span class="nav-text">1.11 reduceByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-12-sortBy-amp-sortByKey"><span class="nav-number">3.1.12.</span> <span class="nav-text">1.12 sortBy &amp; sortByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-13-join"><span class="nav-number">3.1.13.</span> <span class="nav-text">1.13 join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-14-cogroup"><span class="nav-number">3.1.14.</span> <span class="nav-text">1.14 cogroup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-15-cartesian"><span class="nav-number">3.1.15.</span> <span class="nav-text">1.15 cartesian</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-16-aggregateByKey"><span class="nav-number">3.1.16.</span> <span class="nav-text">1.16 aggregateByKey</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Action"><span class="nav-number">3.2.</span> <span class="nav-text">二、Action</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-reduce"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1 reduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-takeOrdered"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2 takeOrdered</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-countByKey"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.3 countByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-saveAsTextFile"><span class="nav-number">3.2.4.</span> <span class="nav-text">2.4 saveAsTextFile</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">4.</span> <span class="nav-text">Spark累加器与广播变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B-1"><span class="nav-number">4.1.</span> <span class="nav-text">一、简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">4.2.</span> <span class="nav-text">二、累加器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%90%86%E8%A7%A3%E9%97%AD%E5%8C%85"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.1 理解闭包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E4%BD%BF%E7%94%A8%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2 使用累加器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">4.3.</span> <span class="nav-text">三、广播变量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL-Dataset%E5%92%8CDataFrame%E7%AE%80%E4%BB%8B"><span class="nav-number">5.</span> <span class="nav-text">SparkSQL_Dataset和DataFrame简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Spark-SQL%E7%AE%80%E4%BB%8B"><span class="nav-number">5.1.</span> <span class="nav-text">一、Spark SQL简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81DataFrame-amp-DataSet"><span class="nav-number">5.2.</span> <span class="nav-text">二、DataFrame &amp; DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-DataFrame"><span class="nav-number">5.2.1.</span> <span class="nav-text">2.1 DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-DataFrame-%E5%AF%B9%E6%AF%94-RDDs"><span class="nav-number">5.2.2.</span> <span class="nav-text">2.2 DataFrame 对比 RDDs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-DataSet"><span class="nav-number">5.2.3.</span> <span class="nav-text">2.3 DataSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E9%9D%99%E6%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BF%90%E8%A1%8C%E6%97%B6%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8"><span class="nav-number">5.2.4.</span> <span class="nav-text">2.4 静态类型与运行时类型安全</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Untyped-amp-Typed"><span class="nav-number">5.2.5.</span> <span class="nav-text">2.5 Untyped &amp; Typed</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81DataFrame-amp-DataSet-amp-RDDs-%E6%80%BB%E7%BB%93"><span class="nav-number">5.3.</span> <span class="nav-text">三、DataFrame &amp; DataSet  &amp; RDDs 总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Spark-SQL%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="nav-number">5.4.</span> <span class="nav-text">四、Spark SQL的运行原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92-Logical-Plan"><span class="nav-number">5.4.1.</span> <span class="nav-text">4.1 逻辑计划(Logical Plan)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92-Physical-Plan"><span class="nav-number">5.4.2.</span> <span class="nav-text">4.2 物理计划(Physical Plan)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E6%89%A7%E8%A1%8C"><span class="nav-number">5.4.3.</span> <span class="nav-text">4.3 执行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Structured-API%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="nav-number">6.</span> <span class="nav-text">Structured API基本使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%88%9B%E5%BB%BADataFrame%E5%92%8CDataset"><span class="nav-number">6.1.</span> <span class="nav-text">一、创建DataFrame和Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%88%9B%E5%BB%BADataFrame"><span class="nav-number">6.1.1.</span> <span class="nav-text">1.1 创建DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%88%9B%E5%BB%BADataset"><span class="nav-number">6.1.2.</span> <span class="nav-text">1.2 创建Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%94%B1%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9B%E5%BB%BA"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">1. 由外部数据集创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%94%B1%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9B%E5%BB%BA"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">2. 由内部数据集创建</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E7%94%B1RDD%E5%88%9B%E5%BB%BADataFrame"><span class="nav-number">6.1.3.</span> <span class="nav-text">1.3 由RDD创建DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BD%BF%E7%94%A8%E5%8F%8D%E5%B0%84%E6%8E%A8%E6%96%AD"><span class="nav-number">6.1.3.1.</span> <span class="nav-text">1. 使用反射推断</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BB%A5%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E6%8C%87%E5%AE%9ASchema"><span class="nav-number">6.1.3.2.</span> <span class="nav-text">2. 以编程方式指定Schema</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-DataFrames%E4%B8%8EDatasets%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="nav-number">6.1.4.</span> <span class="nav-text">1.4  DataFrames与Datasets互相转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Columns%E5%88%97%E6%93%8D%E4%BD%9C"><span class="nav-number">6.2.</span> <span class="nav-text">二、Columns列操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E5%BC%95%E7%94%A8%E5%88%97"><span class="nav-number">6.2.1.</span> <span class="nav-text">2.1 引用列</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E6%96%B0%E5%A2%9E%E5%88%97"><span class="nav-number">6.2.2.</span> <span class="nav-text">2.2 新增列</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%88%A0%E9%99%A4%E5%88%97"><span class="nav-number">6.2.3.</span> <span class="nav-text">2.3 删除列</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E9%87%8D%E5%91%BD%E5%90%8D%E5%88%97"><span class="nav-number">6.2.4.</span> <span class="nav-text">2.4 重命名列</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E4%BD%BF%E7%94%A8Structured-API%E8%BF%9B%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2"><span class="nav-number">6.3.</span> <span class="nav-text">三、使用Structured API进行基本查询</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E4%BD%BF%E7%94%A8Spark-SQL%E8%BF%9B%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2"><span class="nav-number">6.4.</span> <span class="nav-text">四、使用Spark SQL进行基本查询</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Spark-SQL%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="nav-number">6.4.1.</span> <span class="nav-text">4.1 Spark  SQL基本使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%85%A8%E5%B1%80%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE"><span class="nav-number">6.4.2.</span> <span class="nav-text">4.2 全局临时视图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">7.</span> <span class="nav-text">Spark SQL 外部数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B-2"><span class="nav-number">7.1.</span> <span class="nav-text">一、简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%94%AF%E6%8C%81"><span class="nav-number">7.1.1.</span> <span class="nav-text">1.1 多数据源支持</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">7.1.2.</span> <span class="nav-text">1.2 读数据格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%86%99%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">7.1.3.</span> <span class="nav-text">1.3 写数据格式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81CSV"><span class="nav-number">7.2.</span> <span class="nav-text">二、CSV</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E8%AF%BB%E5%8F%96CSV%E6%96%87%E4%BB%B6"><span class="nav-number">7.2.1.</span> <span class="nav-text">2.1 读取CSV文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%86%99%E5%85%A5CSV%E6%96%87%E4%BB%B6"><span class="nav-number">7.2.2.</span> <span class="nav-text">2.2 写入CSV文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="nav-number">7.2.3.</span> <span class="nav-text">2.3 可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81JSON"><span class="nav-number">7.3.</span> <span class="nav-text">三、JSON</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E8%AF%BB%E5%8F%96JSON%E6%96%87%E4%BB%B6"><span class="nav-number">7.3.1.</span> <span class="nav-text">3.1 读取JSON文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E5%86%99%E5%85%A5JSON%E6%96%87%E4%BB%B6"><span class="nav-number">7.3.2.</span> <span class="nav-text">3.2 写入JSON文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="nav-number">7.3.3.</span> <span class="nav-text">3.3 可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Parquet"><span class="nav-number">7.4.</span> <span class="nav-text">四、Parquet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E8%AF%BB%E5%8F%96Parquet%E6%96%87%E4%BB%B6"><span class="nav-number">7.4.1.</span> <span class="nav-text">4.1 读取Parquet文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%86%99%E5%85%A5Parquet%E6%96%87%E4%BB%B6"><span class="nav-number">7.4.2.</span> <span class="nav-text">2.2 写入Parquet文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE-1"><span class="nav-number">7.4.3.</span> <span class="nav-text">2.3 可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81ORC"><span class="nav-number">7.5.</span> <span class="nav-text">五、ORC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E8%AF%BB%E5%8F%96ORC%E6%96%87%E4%BB%B6"><span class="nav-number">7.5.1.</span> <span class="nav-text">5.1 读取ORC文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%86%99%E5%85%A5ORC%E6%96%87%E4%BB%B6"><span class="nav-number">7.5.2.</span> <span class="nav-text">4.2 写入ORC文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81SQL-Databases"><span class="nav-number">7.6.</span> <span class="nav-text">六、SQL Databases</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">7.6.1.</span> <span class="nav-text">6.1 读取数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">7.6.2.</span> <span class="nav-text">6.2 写入数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81Text"><span class="nav-number">7.7.</span> <span class="nav-text">七、Text</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E8%AF%BB%E5%8F%96Text%E6%95%B0%E6%8D%AE"><span class="nav-number">7.7.1.</span> <span class="nav-text">7.1 读取Text数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E5%86%99%E5%85%A5Text%E6%95%B0%E6%8D%AE"><span class="nav-number">7.7.2.</span> <span class="nav-text">7.2 写入Text数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7"><span class="nav-number">7.8.</span> <span class="nav-text">八、数据读写高级特性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E5%B9%B6%E8%A1%8C%E8%AF%BB"><span class="nav-number">7.8.1.</span> <span class="nav-text">8.1 并行读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E5%B9%B6%E8%A1%8C%E5%86%99"><span class="nav-number">7.8.2.</span> <span class="nav-text">8.2 并行写</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E5%88%86%E5%8C%BA%E5%86%99%E5%85%A5"><span class="nav-number">7.8.3.</span> <span class="nav-text">8.3 分区写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E5%88%86%E6%A1%B6%E5%86%99%E5%85%A5"><span class="nav-number">7.8.4.</span> <span class="nav-text">8.3 分桶写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E7%AE%A1%E7%90%86"><span class="nav-number">7.8.5.</span> <span class="nav-text">8.5 文件大小管理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%E9%99%84%E5%BD%95"><span class="nav-number">7.9.</span> <span class="nav-text">九、可选配置附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-CSV%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="nav-number">7.9.1.</span> <span class="nav-text">9.1 CSV读写可选配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-JSON%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="nav-number">7.9.2.</span> <span class="nav-text">9.2 JSON读写可选配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="nav-number">7.9.3.</span> <span class="nav-text">9.3 数据库读写可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">7.10.</span> <span class="nav-text">参考资料</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-1"><span class="nav-number">7.11.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">8.</span> <span class="nav-text">SparkSQL常用聚合函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E5%8D%95%E8%81%9A%E5%90%88"><span class="nav-number">8.1.</span> <span class="nav-text">一、简单聚合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">8.1.1.</span> <span class="nav-text">1.1 数据准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-count"><span class="nav-number">8.1.2.</span> <span class="nav-text">1.2 count</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-countDistinct"><span class="nav-number">8.1.3.</span> <span class="nav-text">1.3 countDistinct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-approx-count-distinct"><span class="nav-number">8.1.4.</span> <span class="nav-text">1.4 approx_count_distinct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-first-amp-last"><span class="nav-number">8.1.5.</span> <span class="nav-text">1.5 first &amp; last</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-min-amp-max"><span class="nav-number">8.1.6.</span> <span class="nav-text">1.6 min &amp; max</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-sum-amp-sumDistinct"><span class="nav-number">8.1.7.</span> <span class="nav-text">1.7 sum &amp; sumDistinct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-avg"><span class="nav-number">8.1.8.</span> <span class="nav-text">1.8 avg</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0"><span class="nav-number">8.1.9.</span> <span class="nav-text">1.9 数学函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE%E5%88%B0%E9%9B%86%E5%90%88"><span class="nav-number">8.1.10.</span> <span class="nav-text">1.10 聚合数据到集合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88"><span class="nav-number">8.2.</span> <span class="nav-text">二、分组聚合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%AE%80%E5%8D%95%E5%88%86%E7%BB%84"><span class="nav-number">8.2.1.</span> <span class="nav-text">2.1 简单分组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88"><span class="nav-number">8.2.2.</span> <span class="nav-text">2.2 分组聚合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">8.3.</span> <span class="nav-text">三、自定义聚合函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%9C%89%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="nav-number">8.3.1.</span> <span class="nav-text">3.1 有类型的自定义函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%97%A0%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">8.3.2.</span> <span class="nav-text">3.2 无类型的自定义聚合函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-2"><span class="nav-number">8.4.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL%E8%81%94%E7%BB%93%E6%93%8D%E4%BD%9C"><span class="nav-number">9.</span> <span class="nav-text">SparkSQL联结操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">9.1.</span> <span class="nav-text">一、 数据准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E8%BF%9E%E6%8E%A5%E7%B1%BB%E5%9E%8B"><span class="nav-number">9.2.</span> <span class="nav-text">二、连接类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-INNER-JOIN"><span class="nav-number">9.2.1.</span> <span class="nav-text">2.1 INNER JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-FULL-OUTER-JOIN"><span class="nav-number">9.2.2.</span> <span class="nav-text">2.2 FULL OUTER JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-LEFT-OUTER-JOIN"><span class="nav-number">9.2.3.</span> <span class="nav-text">2.3 LEFT OUTER JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-RIGHT-OUTER-JOIN"><span class="nav-number">9.2.4.</span> <span class="nav-text">2.4 RIGHT OUTER JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-LEFT-SEMI-JOIN"><span class="nav-number">9.2.5.</span> <span class="nav-text">2.5 LEFT SEMI JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-LEFT-ANTI-JOIN"><span class="nav-number">9.2.6.</span> <span class="nav-text">2.6 LEFT ANTI JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-CROSS-JOIN"><span class="nav-number">9.2.7.</span> <span class="nav-text">2.7 CROSS JOIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-NATURAL-JOIN"><span class="nav-number">9.2.8.</span> <span class="nav-text">2.8 NATURAL JOIN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%89%A7%E8%A1%8C"><span class="nav-number">9.3.</span> <span class="nav-text">三、连接的执行</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming%E4%B8%8E%E6%B5%81%E5%A4%84%E7%90%86"><span class="nav-number">10.</span> <span class="nav-text">Spark Streaming与流处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%B5%81%E5%A4%84%E7%90%86"><span class="nav-number">10.1.</span> <span class="nav-text">一、流处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E9%9D%99%E6%80%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">10.1.1.</span> <span class="nav-text">1.1 静态数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E6%B5%81%E5%A4%84%E7%90%86"><span class="nav-number">10.1.2.</span> <span class="nav-text">1.2 流处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Spark-Streaming"><span class="nav-number">10.2.</span> <span class="nav-text">二、Spark Streaming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%B5%81%E5%A4%84%E7%90%86-1"><span class="nav-number">10.3.</span> <span class="nav-text">一、流处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E9%9D%99%E6%80%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-1"><span class="nav-number">10.3.1.</span> <span class="nav-text">1.1 静态数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E6%B5%81%E5%A4%84%E7%90%86-1"><span class="nav-number">10.3.2.</span> <span class="nav-text">1.2 流处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%AE%80%E4%BB%8B"><span class="nav-number">10.3.3.</span> <span class="nav-text">2.1 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-DStream"><span class="nav-number">10.3.4.</span> <span class="nav-text">2.2 DStream</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Spark-amp-Storm-amp-Flink"><span class="nav-number">10.3.5.</span> <span class="nav-text">2.3 Spark &amp; Storm &amp; Flink</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-3"><span class="nav-number">10.4.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="nav-number">11.</span> <span class="nav-text">Spark Streaming 基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%A1%88%E4%BE%8B"><span class="nav-number">11.1.</span> <span class="nav-text">一、案例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-1"><span class="nav-number">12.</span> <span class="nav-text">Spark Streaming 基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%A1%88%E4%BE%8B%E5%BC%95%E5%85%A5"><span class="nav-number">12.1.</span> <span class="nav-text">一、案例引入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-StreamingContext"><span class="nav-number">12.1.1.</span> <span class="nav-text">3.1 StreamingContext</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">12.1.2.</span> <span class="nav-text">3.2 数据源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%81%9C%E6%AD%A2"><span class="nav-number">12.1.3.</span> <span class="nav-text">3.3 服务的启动与停止</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Transformation"><span class="nav-number">12.2.</span> <span class="nav-text">二、Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-DStream%E4%B8%8ERDDs"><span class="nav-number">12.2.1.</span> <span class="nav-text">2.1 DStream与RDDs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-updateStateByKey"><span class="nav-number">12.2.2.</span> <span class="nav-text">2.2 updateStateByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%90%AF%E5%8A%A8%E6%B5%8B%E8%AF%95"><span class="nav-number">12.2.3.</span> <span class="nav-text">2.3 启动测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E8%BE%93%E5%87%BA%E6%93%8D%E4%BD%9C"><span class="nav-number">12.3.</span> <span class="nav-text">三、输出操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E8%BE%93%E5%87%BAAPI"><span class="nav-number">12.3.1.</span> <span class="nav-text">3.1 输出API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-foreachRDD"><span class="nav-number">12.3.2.</span> <span class="nav-text">3.1 foreachRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E4%BB%A3%E7%A0%81%E8%AF%B4%E6%98%8E"><span class="nav-number">12.3.3.</span> <span class="nav-text">3.3 代码说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E5%90%AF%E5%8A%A8%E6%B5%8B%E8%AF%95"><span class="nav-number">12.3.4.</span> <span class="nav-text">3.4 启动测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-4"><span class="nav-number">12.4.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">13.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Tommy Hu</p>
  <div class="site-description" itemprop="description">写点东西</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tommy Hu</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'e26b1eec8da027069747',
      clientSecret: '48d14ef26caf77c5a7154a3c0cf08d66f6d21549',
      repo        : 'BlogComments',
      owner       : 'RealTommyHu',
      admin       : ['RealTommyHu'],
      id          : '86fd3d94f39bc0b481858767bd132476',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
